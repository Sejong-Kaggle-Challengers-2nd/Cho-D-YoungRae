{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5등코드공부.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "19xZK9TsIXcAM_dvzMijuKS4n1_kGOARz",
      "authorship_tag": "ABX9TyM7RhM6UQjq/se4OLv/V+pk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sejong-Kaggle-Challengers-2nd/Cho-D-YoungRae/blob/main/5/5%EB%93%B1%EC%BD%94%EB%93%9C%EA%B3%B5%EB%B6%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILZtmSihhh7s"
      },
      "source": [
        "# [컴퓨터 비전 학습 경진대회](https://dacon.io/competitions/official/235626/overview/description/)\n",
        "> [5등 코드](https://dacon.io/competitions/official/235626/codeshare/1675?page=2&dtype=recent) 공부\n",
        "\n",
        "6가지의 모델을 Ensemble하여 사용하였습니다.\n",
        "\n",
        "모델: thin_resnet, xception, inception, densenet, vggnet, resnet\n",
        "thin_resnet은 resnet기반으로 만든 조금 작은 모델입니다.\n",
        "나머지 모델들은 keras(tensorflow)의 application을 그대로 사용하였습니다.\n",
        "\n",
        "train dataset으로 학습을 시키고 test dataset을 predict한 결과 중 일정 점수 이상을 받은 데이터들을 train dataset에 추가하여 다시 학습하였습니다.\n",
        "\n",
        "Ensemble은 여러 모델의 각 fold로 예측한 값의 root sum을 사용하였고,\n",
        "Loss는 mse, optimizer는 adam을 사용하였습니다.\n",
        "\n",
        "Data Augmentation은 keras의 RandomRotation과 직접 구현한 RandomRoll을 사용하였습니다. keras의 RandomTranslation은 성능이 좋지 않아 사용하지 않았습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKgtU8sOpC8O",
        "outputId": "ac0a4b0f-e642-4e90-aa2a-3ab9463aac33"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Apr  9 00:57:12 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuGtW5IOqF3n"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import glob\n",
        "import shutil\n",
        "import gc\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from albumentations import Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip, Rotate, RandomCrop, RandomBrightnessContrast\n",
        "\n",
        "import tensorflow as tf\n",
        "# import tensorflow_addons as tfa # 사용 안 되는 듯\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, EarlyStopping\n",
        "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D, Permute\n",
        "from tensorflow.keras.layers import MaxPooling1D, Dropout, Reshape, Multiply, Conv2D, MaxPool2D, LSTM, Add, Lambda, AveragePooling2D\n",
        "from tensorflow.keras.layers import GlobalMaxPooling2D, LeakyReLU, PReLU, GlobalAveragePooling2D, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling, RandomTranslation, RandomRotation, RandomZoom\n",
        "# from tensorflow_addons.layers.netvlad import NetVLAD # 사용 안 되는 듯\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier \n",
        "from tensorflow.keras.applications import ResNet50, ResNet152\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.applications import EfficientNetB7, EfficientNetB4, EfficientNetB2, EfficientNetB1\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.applications import ResNet101V2\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "import subprocess as sp\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxJlvKEWrFOg"
      },
      "source": [
        "file_dir = \"/content/drive/MyDrive/kaggle-study/5/data/\"\n",
        "\n",
        "train_csv = pd.read_csv(file_dir + 'train.csv')\n",
        "test_csv = pd.read_csv(file_dir + 'test.csv')\n",
        "submission_csv = pd.read_csv(file_dir + 'submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "3Lbvh3thsnx-",
        "outputId": "20b53196-2e45-4363-bcd2-480b4932f595"
      },
      "source": [
        "train_csv.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>digit</th>\n",
              "      <th>letter</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>L</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>L</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>D</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>A</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 787 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  digit letter  0  1  2  3  4  ...  776  777  778  779  780  781  782  783\n",
              "0   1      5      L  1  1  1  4  3  ...    0    1    2    4    4    4    3    4\n",
              "1   2      0      B  0  4  0  0  4  ...    0    1    4    1    4    2    1    2\n",
              "2   3      4      L  1  1  2  2  1  ...    3    0    2    0    3    0    2    2\n",
              "3   4      9      D  1  2  0  2  0  ...    2    0    1    4    0    0    1    1\n",
              "4   5      6      A  3  0  2  4  0  ...    3    2    1    3    4    3    1    2\n",
              "\n",
              "[5 rows x 787 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVE4ePv1svpb"
      },
      "source": [
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# TRAIN\n",
        "train_letters_char = train_csv['letter'].values\n",
        "train_letters_int = label_encoder.fit_transform(train_letters_char)\n",
        "train_letters = keras.utils.to_categorical(train_letters_int)\n",
        "\n",
        "train_pixels = train_csv.loc[:, '0':'783'].values\n",
        "train_pixels = train_pixels.astype(np.float32)\n",
        "train_pixels = train_pixels.reshape((-1, 28, 28, 1))\n",
        "\n",
        "train_digits_int = train_csv['digit'].values\n",
        "train_digits = keras.utils.to_categorical(train_digits_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdEQDLWot5MP"
      },
      "source": [
        "위 전처리 어떤 과정으로 되는건지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0XsdPCSunVP",
        "outputId": "78a0e31f-49ea-4514-e836-9112953ce53c"
      },
      "source": [
        "train_csv['letter'].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['L', 'B', 'L', ..., 'A', 'Z', 'Z'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6t9ooJIutcx",
        "outputId": "42d70971-3512-4244-e4ee-79e33d46e48d"
      },
      "source": [
        "label_encoder.transform(train_letters_char)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11,  1, 11, ...,  0, 25, 25])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm8chwUhvtsI",
        "outputId": "62b36d85-bdd4-484d-de45-5383c6249bcd"
      },
      "source": [
        "keras.utils.to_categorical(train_letters_int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAPGGTR-vyDJ"
      },
      "source": [
        "`keras.utils.to_categorical()`은 정수형만 원핫 인코딩 시켜주는 듯\n",
        "\n",
        "```python\n",
        "keras.utils.to_categorical(train_letters_char)\n",
        "```\n",
        "```\n",
        "---------------------------------------------------------------------------\n",
        "ValueError                                Traceback (most recent call last)\n",
        "<ipython-input-16-6314712126be> in <module>()\n",
        "----> 1 keras.utils.to_categorical(train_letters_char)\n",
        "\n",
        "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/np_utils.py in to_categorical(y, num_classes, dtype)\n",
        "     67 \n",
        "     68   \"\"\"\n",
        "---> 69   y = np.array(y, dtype='int')\n",
        "     70   input_shape = y.shape\n",
        "     71   if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
        "\n",
        "ValueError: invalid literal for int() with base 10: 'L'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYqOiC2-v733",
        "outputId": "8be19989-53c6-4e10-f99c-cd32f379b55c"
      },
      "source": [
        "# TEST\n",
        "test_letters_char = test_csv['letter'].values\n",
        "test_letters_int = label_encoder.fit_transform(test_letters_char)\n",
        "test_letters = keras.utils.to_categorical(test_letters_int)\n",
        "\n",
        "test_pixels = test_csv.loc[:, '0':'783'].values\n",
        "test_pixels = test_pixels.astype(np.float32)\n",
        "test_pixels = test_pixels.reshape((-1, 28, 28, 1))\n",
        "\n",
        "print(train_letters.shape, train_letters.dtype)\n",
        "print(train_digits.shape, train_digits.dtype)\n",
        "print(train_pixels.shape, train_pixels.dtype)\n",
        "\n",
        "print(test_letters.shape, test_letters.dtype)\n",
        "print(test_pixels.shape, test_pixels.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2048, 26) float32\n",
            "(2048, 10) float32\n",
            "(2048, 28, 28, 1) float32\n",
            "(20480, 26) float32\n",
            "(20480, 28, 28, 1) float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "91jRuDaDyIis",
        "outputId": "7a24d887-89f5-4168-abd7-ef57ef5ebf83"
      },
      "source": [
        "# train 이미지 확인\n",
        "\n",
        "plt.rcParams.update({'font.size': 17})\n",
        "fig, axs = plt.subplots(4, 4)\n",
        "fig.suptitle('image samples')\n",
        "fig.set_size_inches(15, 15)\n",
        "\n",
        "# train_pixels[train_pixels < 0.6] = 0\n",
        "\n",
        "for i in range(len(axs)):\n",
        "    for j in range(len(axs[i])):\n",
        "        img = train_pixels[i * 4 + j].reshape((28, 28))\n",
        "        # img[img < 150] = 0.0\n",
        "        # img[img >= 150] = 1.0\n",
        "        axs[i][j].imshow(img, cmap='Greys')\n",
        "        axs[i][j].axis('off')\n",
        "        axs[i][j].title.set_text(f'{train_digits_int[i * 4 + j]}  {train_letters_char[i * 4 + j]}')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAOpCAYAAAAt4Xe7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdebhcVZnv8d9L5jlkTpiSQAIhJMigDBpmUAQVLt3idFFbbbnCbdH2tj1xpYdr223T7YSCjeJFW64TirQiIBAZRAhTGEMCZCAhDAmZ52HdP3ad9vRhvXuzV1aq6px8P8+Tp+Ct2mvt2rV37XpPVf3KQggCAAAAAPj2avUKAAAAAEC7o3ECAAAAgAo0TgAAAABQgcYJAAAAACrQOAEAAABABRonAAAAAKhA4wQANZjZRDMLZja71euC5jKzRWbGb3gAwB6KxgkAAAAAKvRu9QoAQDezTNI0SRtbvSIAAKB5aJwAoIYQwjZJ81q9HgAAoLn4qB4A1OB9x8nMLmvUP2Rmp5nZbDNbZ2YrzewHZjahcbtJZvbvZvaSmW0ys3vNbFZknjFm9ldmdqeZvWBmW83sZTP7mZkdW7J+7zaz+xtjrzSz681sqpl9p7F+J0WWmWRmV5nZQjPbYmavmtkvzezEmttmhpl9z8yeM7PNjfkfb4w9utPtBpvZp8zsFjN7vtOct5jZ25yxFzXWv4+Z/bWZLWjM8ZyZ/a9OtzuvsU3XN+a/xsyGl4zX18z+1syebazHYjP7RzMbVPO+n2BmP208rlvNbFlj7kmR2442sy+Y2VON9VxrZs809pM31pkXANA8vOMEAHm9U9LZkv5D0lWSjpP0bkkzzOxcSfeoeMfqe5L2l3SepFvMbEoIYWmncd4k6VJJd0j6maR1kg6S9A5JZ5nZO0IIv+o8sZl9StK/NG77fUkvS3qzpN9JejS2so1G6ueSBkq6SdKPJY2WdK6k283sj0II/7fqTpvZDEn3SwqN+/5cY8xJkj4g6RuSXmncfKKkL0q6szHnq5L2k/QuSaeb2cdCCFc7U10n6VhJv5S0VdIfSPonM+srab2kz0u6QdJvJZ0m6UOSRqnYbjE/lHSMpJ9I2qTisfszSbPM7KQQwtbXcd//tHF/1km6UdILkqZKukDSuxrjPNq47YDGuh0k6bbG/dgpaV9JJ0m6V9KcqjkBAC0QQuAf//jHP/69zn8qXvQHSbO71C9r1LdLOrFT3ST9qnHdq5L+qstyf9m47l+61EdKGhaZ/3AVDcJTkfXaquLF+5Qu1/1jY44g6aRO9WEqmpk1ko7ossw+Kr7PtVHSmNexXS5vjH9O5LpBkgZ0+v/BsTFVNA8vNLbTgC7XLWqMP0fS8Mj9Xt+4L9M6XddX0uON5WY64z0jaWSnem8VzVyQ9NnYMl1qb1HR+DwsaVSX606VtEPSnE61dzTG/lLk/u8laUSr93H+8Y9//ONf/B8f1QOAvK4LIfym439CCEHSvzf+d42kL3S5/fcal0d0LoYQVoYQ1nQdPIQwV9Ltkg4xs/07XfV+SX0kXRlCWNBlsb+TtDqyrheoeDfm8yGEh7vMs0zSP0kaoOJdsddrU2SdN4QQNnX6//UhhJcjt1uq4h2vvSV5H1n78xDC6k7LLJJ0l4rm7BshhKc6XbdVxTtKUpft28nfhRBWdlpmu6SOj/79kbNMZ59S0RxfGEJY0eX+3Kbi3byjzWxal+Vi22lnCOHV1zEnAKAF+KgeAOT1QKS2vHH5SAhhR5frXmhc7tt1ITM7S9KFko5W0eB0fc7eR9KSxn8f2bi8q+s4IYT1ZvaIio+CdfbmxuU0M7ssst5TGpeHRq7r6jpJfyLpZ2Z2vYqPod3buZHpzMzeLOmTKj7KOEbFu0Od7ePMU7Z9H4xc527fhtldCyGEx81spaSpZjY4hLDeWVYqtmFQ8fHJMyPXj2lcHirpKUm/kfS8pM+a2dEqPqr3W0kPhSJ4BADQpmicACCv17xLpOLje9HrQgjbzUwq3i36T2b2SUlfUvERtFskLZS0QY2Pw6n4yF6/TosMa1y+5KxXrD6ycflBZ5kOgyuuVwjhgUYz9Jcqvqv0PkkysyUqPob45Y7bNr7r9WMV2+VWSfNV3M+dKpq7E/Vf71vneWpt307X9YlcJ0kvltRHqtiuZY3TSBXvOF1achupsQ1DCGsb4R6fU/F9uNMa168xs2tVfJRzXcVYAIAWoHECgDZjZr1VfGdqk6SjQgjzu1x/XGSxjqZhrDNsrN7xkbc3hxB+m7Cq/0UI4X5J55hZHxUfjTtd0kWSvmRm20MIVzRu+ncqvs9zWgjhv7xDZmZXqWicmmWcpMVOXYo3Y52tljRUUv/GxzIrhRBekPRxM7tQRYjESZL+WNL/lDRe0h++nnEAAM3Fd5wAoP2MkjRcRQBE16ZpkH7/sbzOOr6jFIs2HyzpDZFl7m1cnpC+qq8VQtgWQrg/hPB/JL2nUe78PamDJL0aaZr20u8/PtgsJ3UtmNlhKt5Jml/xMT2p2IZ9VSTz1RIKT4cQrlLxuK2X9M5G4wwAaDM0TgDQfl5WkWY3xczGdxQbL6j/VUVj1dW/S9om6UIzm9LluktVNGJdXaMiwe7PY7/v1JjzSDMbGbuuy+2Ob0Rtd9Xxzk3nMIRFkvZuNCid/ZWk6VVzZXZp5/vX2MZfbPzvt1/H8h1pglea2cSuV5pZbzM7udP/TzezcV1vpyIQo5+KhMCdr3vtAQBNw1+1AKDNhBB2mtlXJP25pIfN7McqXpyfrOI3lu5o/HfnZRaa2V+pSMJ70Mx+qKIBe4ukw1SEEpyoTi/KQwirGt83ukHSHWZ2l4rfe9qkIkzhSBUfJTtC0kqV+zNJpzbGWChprX7/u1Nb9ftmRCqavysl3d1Yzw2Sjpc0Q8XvIHm/ubQ7PC7p8cY23izpLEnTVLyT9K9VC4cQfmNmn1bRQD1tZjepiDjvq+K3qd7c+O+OxvV0Sf9sZveq+G7XSyo+RvlOFd/DuiyEQOMEAG2IxgkA2tOlKn6X6KOSPqLiuza3SvprSX8TWyCE8EUze17SZ1TEk29S0TAdp+KFvdTlOzshhDsbP177aUlva8y1U0VS3eMqfgPq6dexvl9X0Vwd25ivn4pEu/8n6fIQwmOd5rzKzLZIuqTTet7VuK/nqbmN07tVbNMPqEjye0lFk/e34XX8+K0khRC+ZGb3qEgJPEHSmSreMVyuohG8vtPNb1bRUM1S8WO7w1U0uHMkXRFC+EWG+wQA2A3sdX6XFQDQTTU+fvacpAmShoYQNrZ4lVrOzBZJOiCEYK1eFwBA98B3nACghzCzUWbWt0vNJP1vFe9y/JKmCQCANHxUDwB6jrNVfH/m1yp+GHeoio/NzVTxMbpPt3DdAADo1micAKDneFDS7SqCFt4pqZekZSqCGP4hhLCkhesGAEC3xnecAAAAAKAC33ECAAAAgAo0TgAAAABQgcYJAAAAACrQOAEAAABABRonAAAAAKhA4wQAAAAAFWicAAAAAKACjRMAAAAAVKBxAgAAAIAKNE4AAAAAUIHGCQAAAAAq0DgBAAAAQAUaJwAAAACoQOMEAAAAABVonAAAAACgAo0TAAAAAFSgcQIAAACACjROAAAAAFCBxgkAAAAAKtA4AQAAAEAFGicAAAAAqEDjBAAAAAAVaJwAAAAAoAKNEwAAAABUoHECAAAAgAo0TgAAAABQgcYJAAAAACrQOAEAAABABRonAAAAAKhA4wQAAAAAFWicAAAAAKACjRMAAAAAVKBxAgAAAIAKNE4AAAAAUIHGCQAAAAAq0DgBAAAAQAUaJwAAAACoQOMEAAAAABVonAAAAACgAo0TAAAAAFSgcQIAAACACjROAAAAAFCBxgkAAAAAKtA4AQAAAEAFGicAAAAAqEDjBAAAAAAVaJwAAAAAoAKNEwAAAABUoHECAAAAgAo0TgAAAABQgcYJAAAAACrQOAEAAABABRonAAAAAKhA4wQAAAAAFWicAAAAAKACjRMAAAAAVKBxAgAAAIAKNE4AAAAAUIHGCQAAAAAq0DgBAAAAQAUaJwAAAACoQOMEAAAAABVonAAAAACgAo0TAAAAAFSgcQIAAACACjROAAAAAFCBxgkAAAAAKtA4AQAAAEAFGqddYGazzSyU/Dsk0zzBzEKOsYDuxszGm9lVZva8mW1pXH7TzMZnnCN2/G41s6Vm9kMze1OuuYDuwsxOM7OdjePhS5nGnNgYb1GO8YDuxMwuMLO7zGytmW0ysyfN7DIzG5hp/ImRc9kWM3vFzB4ys2+b2TvMrHeO+fZEbLg8vixpdaS+otkrAvQkZjZR0m8ljZf0C0mPSZoh6WOSzjaz40MIizJO+Ted/nuIpDdI+gNJ55rZO0IIv8o4F9C2zGyEpO9I2iBpcGvXBuj+zOwaSR9S8drwR5LWSjpR0ucknWVmJ4cQ1meabo2kjj929JI0XNI0Se+T9GFJT5jZB0IIj2Sab49B45THlzK/eANQ+LqKpukzIYTLO4pm9hlJX5R0haSzck0WQrisa63TXH8uicYJe4orJfWX9PnGPwCJzOydKpqmRZLeFEJ4pVHfS9JVkj6q4g93f5ppytXO+WykpH+S9EeSfm1mx4QQns005x6Bj+oBaEtmNknSmZIWS/rXLlf/q6Qlkt7eeFdqd7q5cTl6N88DtAUz+6CkP5T0CUnLW7w6QE/wB43LyzuaJkkKIeyU9BeN//2YmQ3YnSsRQlgZQviIpB9I6miiUAONUx5nmtlnzewzZnaOmQ1t9QoBPcApjctbGieX/xRC2CHpli63213OaFzev5vnAVqu8YeIr0i6LoTww9auDdBjjGtcPtf1ihDCCknrVHw8/Jgmrc//bly+08yGNGnOHoGP6uXx9S7/v87M/iKEcEVL1gboGQ5uXM53rp/f5Xa7zMwu6/S/gyXNlHSqpLv1+78KAj1S42ND35W0XtJFLV4doCfp+M77pK5XmNkoFU2TJB0iafbuXpkQwnwzWyppX0lHNWPOnoLGadf8XNLlkh5WcVDsJ+k8SX8p6Wtmti2E8M0Wrh/QnQ1rXMaCVzrXh2ec83OR2mJJ35f0UsZ5gHb0F5LeIunMEMKqVq8M0IPcKOm9kv7UzH7QeJdJZmaS/r7T7fZu4jotU9E4jWninN0eH9XbBSGEfwkh3BhCWBpC2BxCWBBC+IKkcxs3+T9m1quV6wjg9QshWMc/SQNVpOo9qOJd5e+0ct2A3cnMjlLxh4MrSY8EsvuBpP9Q8Y7TU2Z2tZn9i6QHVIRGzGvcbmd88d3CGpf83E0NNE67QQjhNhUfIxol6dAWrw7QXa1pXHrvKHXUvXekdkkIYVMIYa6KvxIuknSBmR27O+YCWqnxmy7/riJw5TMtXh2gx2l8T/dcSZ+WtFTS+1X8rMZKFZHkHcl2zfxkwz6Ny5ebOGe3R+O0+7zauBzU0rUAuq+nG5dTneundrndbhFC2Crpocb/NuuLu0AzDVbxXcEDJa3v/OOZkq5p3OaTjdrsVq0k0J2FELaHEP41hHBECGFACGFICOGMEMJ9Kr5PKzUphMjMDlbROG1T8akKvE58x2k3MLPBkg5T8fbnwhavDtBd3d64PMPM9uqcrNf4EntH2t0dTViXEY1L/tiEnmiLpG85102RdIKkJyT9Trv5DxXAnsbMTlHxHfmHQghPNmnav21c3pDxR3f3CDROiRq/MbM2hLCyS32IpKtV/AXv5hACXygHEoQQFprZTSp+y+lTKoJYOnxa0v6Sbgoh7NY/TjQ+nveWxv/O3p1zAa0QQtik4gc4X8PMPqSicfp1COGSZq4X0JOY2bAQwpoutQNVvGbcqSZ8TLbxA7j/KOndKj4m+Oe7e86ehsYp3YmSrjKzu1Xk8q9QkU5yhoqEkmfknIhSmdl3Sq7+QghhXsn1QHf0CUm/lfTPZnaypMckzZB0lqQXG9dn0yWOfICKjwOereK58ishhIdzzgfswUZVnNM+05E8BvQQt5jZFhXnsTUq3s09W1IfSReGEHJ+emJ4p/NZLxUptYeq+CNgP0mPS/rvIYRn44vDYyEQppHCzGZI+lMV+fcTJA2VtEFFMsrPJH0t19ufjc+ZVzk5hDA7x3xAOzGzCZIuU9EsjZb0iqRfSLoshPBCpjlix9gOFd9VfEjSv4UQfpJjLqA7abzjdI2kL+d4x6nxA7uv513iSSGERbs6H9AuzOwzks5X8V3CwSqCIO6Q9MUQwmOZ5pio1x5fW1X8wO4SSY+oeI36yxDC9hxz7mlonAAAAACgAl90BgAAAIAKNE4AAAAAUIHGCQAAAAAq0DgBAAAAQIXSOPKQMTkiZai99qrX1+3cubP6Rhnmzz1PLnW3l1T/ccn9OHrb0sxq1cvG2muvvfyF2kd0w5btazmPj7LtWpe3j5TN4V2XcqzV3S5l+3Td/T1lX0/hzZMz7KdsrJLjs62PtZzntGYo25wpdyXl2GzlHDmfS7yxUrZjM7ZX2Tw98Tir+7glvhaodftUdV+/5Dw3p2yXlLHq3pfcu2zO55mSZaJX8I4TAAAAAFSgcQIAAACACjROAAAAAFCBxgkAAAAAKtA4AQAAAECF0lS9uiknZdflTElLGSslgaNuMEyzxkp5XDzeMjlTu5q1THeWkurTyuMjd9pXzrSfnOqmMKUkZKVISS7KmVy4J6m73cqSFesmu5XtT814js59Tq07VkpSrCfnejUria/Nw/Nqa1YSXs5j1lM2/44dO6L1LVu2ZJu/X79+0XrOfbNZr/U9zTo31b0vvOMEAAAAABVonAAAAACgAo0TAAAAAFSgcQIAAACACjROAAAAAFCBxgkAAAAAKpTGkeeMKG1GvHRKdGuZutGVZfNv3bo1Wp83b160PmLECHesUaNGResDBgxwl/G0OqK4bgxmT40pT4kDrRv7mTMq29ufJWnZsmXR+j777OMu06dPn1rzl0mJ6q47Vk45f14gJe7XU/acmXMbN1POCG1P2XZu9c88eHJGPqdEi+eMac/5WKYcmzkf45zx2c3UjHNQmZzHufcYbN++3V3mlltuidYXLVoUrW/atMkdy5tn+PDh0XrZMeudgydOnBitz5w50x0r52s3N/Y74RyU83mROHIAAAAASETjBAAAAAAVaJwAAAAAoAKNEwAAAABUoHECAAAAgAqlqXrNSHsqSzmpm4CSkvZXtkzdpKGf/OQn7lg33nhjtP7d7343Wj/77LPdsc4444xo/aKLLorWm5WOlbKNm5Xe1+6alepVl7deZYlCt912W7ReltBz+OGHR+v9+vWL1nOmDaYkh+WUM0Gy1felu0o5dzRjrNxamer3xBNPuNdNmjQpWl++fHm0vmPHDncsb/v36tUrWl+6dKk71qxZs9zr6mr1Y99MOc9BudNFc/H2J0l64xvfGK2vWLEiWl+4cKE71u233x6te+dgb46y67xtXLb/v/3tb4/WR44cGa0feuih7lhvetObaq1XipTXwe5Yu7oyAAAAANDT0TgBAAAAQAUaJwAAAACoQOMEAAAAABVonAAAAACgQmmqnpc0UZZOkZJc5amb0taMZKAyS5Ysca9bu3ZttH700UdH6x/84AfdsYYNGxatp2yXuukzzUr6atcknd0l5VjLqe5xW7atvbSfsvSq6dOnR+t9+vSJ1su2i7fvpNyXZiSqpSSOtvq5rrsea+2arJiyD3jKHudmnFOvu+66aP3YY491l1m5cmW0/tRTT0XrW7dudcfyEve8RLFBgwa5Y51wwgnResp2qZvSK/W84ywlYbcZrxFyvkaSpDFjxkTrF1xwQe05vvrVr0brXqqfdyxJ0vXXXx+tP/roo9H67Nmz3bHmzJkTrXtJuGVJ0V6qXs4kvJxJj7zjBAAAAAAVaJwAAAAAoAKNEwAAAABUoHECAAAAgAo0TgAAAABQgcYJAAAAACqUxpF7UuJOU9SNgcwZBVx2nTfW8OHD3bGOO+64aL137/hDcMYZZ7hjDR06NFpPiVusex9zb+Nc65V7/naRcn+9CM+ysXJGdXq8mOAUOZ9nUqRs45yxup6UmPacx3q7a9cYd+/4yx2T7EmJ6r/qqqui9UmTJkXry5Ytc8davnx5tL5q1apo3fvJA8mPVvbu41FHHeWOlfOxb8ZPsrSLnHHtObdByn7uSXmuTZnn4osvrjV/2bafMmVKtO4df9/73vfcsbyfClizZk207v0kj9Sc14gpj5eHd5wAAAAAoAKNEwAAAABUoHECAAAAgAo0TgAAAABQgcYJAAAAACokperllJK+kjOBo0zdFJyPfOQj7nVe0sh1110Xrb/44ovuWIMGDYrWm5G006yUp5RklGYkw+0urX7s6irb1l56Xlmq3pYtW6J1b19v13TFnGlSOVO4yq6rm/RWpt3Tvlqtuz1HpTyeI0eOjNZfeOEFdxnv/Lht27Zo/bnnnnPHuueee6L10047LVofMGCAO5Yn52uQnphe2YzntJTXAs16rq07T0q6c8p6nX/++dH6unXrovVx48a5Y3mvXdevXx+tz5kzxx3rkksuida//OUvu8vkPJ/V3ca84wQAAAAAFWicAAAAAKACjRMAAAAAVKBxAgAAAIAKNE4AAAAAUKE0Vc9LmsiZ9JKSKJMziS9nMkpZatiGDRtqLVM2ViuTq8qSbFr9eHVnOe9vzjS0lPm9JKyHHnrIXWbYsGHR+tve9rba6+Xtoyn7Z87t0t3S61KO9Z4oZ6pVzrStFHUTysruS58+faJ1L1Vv4cKF7lhl57uY559/3r2ub9++0fqIESOidW99y6Q8Ljmff9pdShKeJ+U5uO4yuV/X1H1+bPU+sGTJkmh9xYoV7jLea9reveNtxfve9z53rMWLF0frd9xxh7vMySefHK0349zEO04AAAAAUIHGCQAAAAAq0DgBAAAAQAUaJwAAAACoQOMEAAAAABVonAAAAACgQmkceatjpOtGt5ZFSjYr7tWzdu3aaD1n1GWvXr2yjdXqOPA9Ke5Yas7xUbav1328y9bXiyldsGCBu8z+++9fa56U+Nicx3rKeqXEjuecP+dj3F1jklPWu9U/FeDJ+Rzprdd3vvOd2mPNnTs3Wl+5cqW7jHdfvDjkZ5991h3Ley4ZPXp0tH7WWWfVXq9mHRvNeC7bHZrx+iFn7H/O82/ZeCmx/3X3gbJ9wzue7r777mi9LPb/yCOPdK+L8V4XSNIb3/jGaP2II46oNUez8I4TAAAAAFSgcQIAAACACjROAAAAAFCBxgkAAAAAKtA4AQAAAECF0lS9nMktKQkgddNEyhKLUlJeUtbZ483Tv3//aH3vvfeuvV45E3ialWpXd93Kbp8zsarZcqa0NSPRaP369e51jzzySLT+u9/9zl3mDW94wy6vU4e6SVjNSlpLSVTKOX/OsVKSTdtByj6Q83mlGcdmznPqiBEj3LG8lLrly5fXmluStm7dGq0//fTT0frq1avdsaZOnRqtT5s2LVrPmTZaJuX1RHc9p6U8P9TdpinPNc1KI2xlsmvZPjNnzpxo/eGHH47WhwwZ4o51+eWXR+vf+MY3onXveUGS+vXrF62XJXEOHTrUva6uuo9Le5/lAAAAAKAN0DgBAAAAQAUaJwAAAACoQOMEAAAAABVonAAAAACgAo0TAAAAAFRIiiNvVkRm3YjAsjjLlPjQusts2rTJve6pp56K1gcOHBit54xaTIlczz1PrmVSomO7Ay9aNeVYy/k4eNv0oYcecse68847o/UVK1a4y8yfP79k7V4rZbvk/EmCVsdap8xfd79IiVbvrnLuAymaEVVf5rrrrovWZ86c6S7jRYV7x3nZfXnppZei9SeeeCJaP+SQQ9yxDj300Gj9xBNPdJfx1P1pAynvz5h013NazufanHL+NEvZ83nd4znlOF+7dm20/t3vftdd5lvf+la0Pnz48Gj9vPPOc8fy7v+FF14YrZed4++4445ovewnTPbbb79ovW/fvtF6zp/X6FlnPwAAAADYDWicAAAAAKACjRMAAAAAVKBxAgAAAIAKNE4AAAAAUKE0VS9nClRKmok3f850mpSUF2+9br/9dnesJUuWROsjR46M1lNSq1LuY85tnJIM1YzkxO6QTJTyONR9vHOmgC1cuNC9zkv7KXs+2bFjR7Sesk/fdddd0frRRx8drQ8YMMAdK+c+lTM5rdXqphD1RClJmHWPzZwpYJJ09dVXR+tHHnlktD5ixAh3rA0bNkTr3nZZt26dO9bs2bOj9VdeeSVaP/DAA92xxo8f714Xkzu5MGcacXc4d8XkTP1M2Z45X1ekzNGM4/zv//7vo3XvGJf8Y/Occ86J1i+++GJ3rLr789ixY2uv17Zt22rNIeV9TevZc85yAAAAAJCIxgkAAAAAKtA4AQAAAEAFGicAAAAAqEDjBAAAAAAVSlP1UnjJFTkTLXInDdVdxpv/oYcecsfyEveOP/74aN1LGSuTMwWxWUlf3TkJL6eUxLWcj1Hd+VeuXOmOtX379mjdS5CUpEMPPTRaX7VqlbuMZ9iwYdH6smXLovUJEya4Yw0cOLDW3Dn325Q0yrJlUlLg6s7f7lJS/+pun5Q5cqZ6XXPNNe4yBx10ULTer1+/aP3ZZ591x9q4cWO07m0vL4VPkhYtWhStT5w4MVo/4IAD3LG89MycybopUubvaefHlCS6dn1d06zXSF/72tei9SuuuCJa37x5szvWRRddFK1/7nOfi9Zzvm7u06dP7bGGDh3qXterV69aY2V9vZRtJAAAAADooWicAAAAAKACjRMAAAAAVKBxAgAAAIAKNE4AAAAAUIHGCQAAAAAqJMWRNytutRlzlMVjetdt27YtWh8xYoQ71gMPPBCtP//889H6BRdc4I518MEHu9fFtHNEsseLIE15vLqDnJHQ3vYuO27rbu+yKFAvQvScc85xl/Fiv3/84x9H694xKNWP8B08eLA7lhdtfMghh0TrKc+NOaXE/abElHfXmORmRBvnfI4s255f/epXo/WyY3POnDnRev/+/aP1su21ZcuWaN3bn7zIcck/nr3jrOwceOCBB0brzfoZk7pjpRyze5KcrytyjlX2XF93vCuvvNK9bu7cudH6pk2bovXzzjvPHWvq1LoMI1kAACAASURBVKnRejNef6xbt84da+vWrdG69zMlKetVpu7jxTtOAAAAAFCBxgkAAAAAKtA4AQAAAEAFGicAAAAAqEDjBAAAAAAVklL1cqaZpKR55E7P83jr5qWpnHrqqe5YXkrebbfdFq17aWKSdOmll0brKUl0OVN7Uub3NGuZduGte8pjl5IcVHcZL4VLkiZNmhSt77fffu4yvXvHn4q85K6y5DBvnb1lNm7c6I71u9/9LlofPnx4tD5u3Dh3rLr7Z84URImELiktQbCVzytlj9mOHTui9bL0uquvvjpa37x5c7S+fft2dywvCdNbrzIjR46M1idPnhyt9+3b1x1ryJAh0XrO/T/n83JPTIptdRJe3XlSHoOU167e88/MmTPdZW644YZofdasWdH6W97yFnesiy66qGTtXivnvjl//vxat5fKXzPk3Mfq4h0nAAAAAKhA4wQAAAAAFWicAAAAAKACjRMAAAAAVKBxAgAAAIAKSal6KUkvdZNmUuRMuilbJmX+gw8+OFr3Urj2339/d6xWpteV3d5br5xpbim6QzJRzuSeZiQalaXqeUk4XnJWGe/4mDJlirvM+PHjo3UvIawshWzhwoXR+l133RWtv/Wtb3XHGjx4cLTupf3lTgeqm+rVE1P46ialSnnPA3XnKBvroIMOitbLUu1GjRoVrb/yyiu1x9qwYYN7XYy3/0vS6NGjo3UvPc9LAczN2y9Sjg1vmZSxyvbXdpCyP+dMHa27TZuV1Oztt88884y7jHcO9M5zl1xyiTtW3deOKa/dVqxYEa0//fTT7lje64my1xmk6gEAAABAG6NxAgAAAIAKNE4AAAAAUIHGCQAAAAAq0DgBAAAAQAUaJwAAAACokBRH3mrNio6sO1ZZdLQXnTl58uRo/cwzz6w9f86IVE9ZBGhKdKt3Xc6xukMceTNix3NG73uR45K0zz771BpLkkaMGBGtz5o1K1r3YpWl+rHb3tySdMABB0Trd955Z7T+61//2h3Li489/vjjo/Xcx3PO/aU7HFN1pGw3T9mxnHO7nXDCCdF6WVS/dz+92H2vLkn33XdftL569epo3ftJDkk69thjo/UJEyZE6x/4wAfcseoe/2WP1572sxi5pJy/6/5UQMo50xsr508IlF334IMPRuv33HOPO5Z3PF1zzTW11yvnPuht/yeffDJa37p1qzvW6aefHq3njN3Ped95xwkAAAAAKtA4AQAAAEAFGicAAAAAqEDjBAAAAAAVaJwAAAAAoEJpql5KokXOZLO6qSm500Ryprns2LEjWu/Vq1e0PnDgwNpz5EzbS0kgSpmfpK9CM7ZdzrH23ntvd6whQ4ZE63379nWXOeyww6L1MWPGuMvU5d2X/v37u8uMGzcuWp82bVq0/qtf/cod6/bbb4/WveSysuSwnHImSnXnY9BTN+2rTN3EN++8IfkpVevXr3eX8Y7N6dOnR+tl6ZmLFy+O1tetWxet77vvvu5Yb3zjG6P1Cy64IFrPma6akjbaE/fzZmhG8q9U//FJ2Z/KzJs3L1r3kvC85DxJOvXUU2vNnfN18Pbt293rrr/++mh95cqV0fopp5zijuUlgZatb87+wOPNzztOAAAAAFCBxgkAAAAAKtA4AQAAAEAFGicAAAAAqEDjBAAAAAAVSlP1UtRNrsiZTpMzBbDquhgvIa9ZUpLZvG2WM00o5zLNSNjpLprxGH3961+P1vv06eOO1a9fv9rze8l2rU618o4PL52sLIXopz/9abS+Zs2aaL1sG59//vnRes7Uqj0pbSxlu+VM20uZ30uv8tLuJD+lz7svmzZtcsdatWpVtD5q1Kho3UvOk6TTTjstWs+ZEOrJnfRWd/6U5LB2P85SkodzypnW6/HSIyXp2muvjdbnzp0brZ933nnuWBdffHG0njN52DtvPfzww+5Y3vPPjBkzovWpU6e6Y3lSjk1PzkRm3nECAAAAgAo0TgAAAABQgcYJAAAAACrQOAEAAABABRonAAAAAKhA4wQAAAAAFbLHkXtRrDnjKetGaJdJiSj0Il3L5u/dO76pBw4cWHu9PCnRpXWXyRkPKdV/LHtqHLl3v8qijZsRVbt58+Zo/aGHHnKX8dZrxIgR7jJehHnKfakb05ry3DR58uRofe+993aXOfLII6N1L/L58MMPd8dqRhx4zsjldpFy7si5rXNGmOfkxREvXbrUXca7L/vtt1+0ftRRR7ljjRw5MlrP+dMKzXgNUjZPynq1er9IlfL6oXYkdMm2qXvMlq3Xtm3bovXLL7/cXebnP/95tP7e9743Wv/sZz/rjuVJuS/evvbggw9G63PmzHHH8o7Zo48+OlrPfc5oxrnOm6N7HpUAAAAA0EQ0TgAAAABQgcYJAAAAACrQOAEAAABABRonAAAAAKhQmqqXkmCWM3WtbgpNSspKzkSblPt+0EEHRet9+/atPVYz0gbLkkxy7i/NSA1rJzlTDFO2nXccbNy4MVp/9NFH3bG8JL6hQ4e6yyxatCha95L4hgwZ4o5VV8o+nZL6OGHChGj9t7/9bbS+Zs0ad6yU49OTOymznbU6lbPuOaXs/OTtHxs2bKg1hyQ999xz0fqNN97oLtOrV69ofcqUKdG6d66T/PNdSnJoM84dZY9L3WMz5Xm53dP2Wp2GmjOt94EHHojWb7jhBncZL121LHXVU3e7eOdfSXr44Yej9blz50brY8eOdcc65ZRTonUvKTolUTElCbMZ+157H30AAAAA0AZonAAAAACgAo0TAAAAAFSgcQIAAACACjROAAAAAFChNFUvJekrZ9pT3USNlPUqWyZnOs+OHTuyjeVJSSCpK2V7paQd5kwg6g5JfDmPtZz77bBhw6L1kSNHusvcfPPN0fr8+fPdZX7+859H6+eff360vs8++7hj9e4df1q78MILo/Wvf/3r7lif+MQnonVvWx5wwAHuWMcee2y0vv/++0frZcmBzUgvbdZzYztoxvlJqv+4ld3ee17t06ePu8zWrVuj9WeffTZaX7VqlTvWvvvuG61Pnz49Wi87Zr374m3jlO2S8/yQc39p94S8FCnbum4SXs6E32XLlrnXffGLX4zWy46NCy64IFq/6KKLaq1XmeXLl0frXkpr2TJe4uuZZ57pjjVgwICStXut3OmtdY/BstvXHavnHbEAAAAAkBmNEwAAAABUoHECAAAAgAo0TgAAAABQgcYJAAAAACrQOAEAAABAhdI4ci8+MHfst6duPGVKfG6ZuhGFZXN48ZybN2+O1jds2OCONXjw4FrrVabu45USA5uyv6TE0HbXKOQyzYr+95bx4sjrRpFK0vbt293rFi9eHK1feeWV0frEiRPdsQ466KBofeHChdG691MBkvSFL3whWp8xY0a07m0vSVq5cmW0PmLEiGi9V69e7lgpj3Hd58Cccb/trixavO5xlvOYLYuq9o7Bssdt48aN0fojjzwSrZfdFy9efNq0adG69zMBZXK+nmh17Hjdc53UfaPK60bCl0l5rL1lFixYEK3/8pe/dMd6+OGHo/W3v/3t7jLz5s2L1t///vdH62Wx395PXDzzzDPR+pYtW9yxRo8eHa1796V///7uWDnPAc04n+R8ju+eRyUAAAAANBGNEwAAAABUoHECAAAAgAo0TgAAAABQgcYJAAAAACrUj7mp4KVjpCSANSNpI2V+L5lo1apV7liPP/54tH7//fdH6yeccII71oUXXhit50zaSkl5SknM8eRMrOoO6iYxSfUfu5T5P/jBD0brZamPd911V7Re9th5iVveMeUlCknSkiVLovUHHnggWh87dqw71tFHHx2te8dBv3793LH69OkTrXvJRX379nXHynl8pOxH3TXty9Os5466yWplyYqDBg2K1seMGeMus3r16mh93Lhx0XpZeuapp54are+7777Rurf/l8mZqJYiZ7Jwzvnb/VzXjMfNSySWpBtvvDFaf/XVV6P1NWvWuGN55zov8VXyk49POeWUaN07Z1WtW4x3LEv10/Na/XqrbI6UlEpP7dfBtWcAAAAAgD0MjRMAAAAAVKBxAgAAAIAKNE4AAAAAUIHGCQAAAAAq0DgBAAAAQIXSOPJmxGrmjM8tkzO+94YbbojWH3vsMXesn/70p9H6jh07ovXp06e7Y3lRwDnjwD1lc+R8vDxlMcjdNbpVSlvHupHQZY9D3X3qPe95jzvW5MmTo3UvolXyo4pvvfXWaH3Lli3uWF60uRfvXRb5PHDgwGg95fHy5p85c2a0Pn78eHesVkf/e9f1tJhyKe99rfuzA2WPwYgRI2ovM3z48Gh9+fLl0frzzz/vjjVt2rRo/YADDqi9XnXPESnngWb8vEnuebrDuSumGefim266yb3u5Zdfjta9c0NZ7P5+++0Xra9fv95dxnv9dtxxx0XrI0eOdMfyjtlZs2ZF62U/R+DJGR+f8hjn/AmVlOfl2s8/tWcAAAAAgD0MjRMAAAAAVKBxAgAAAIAKNE4AAAAAUIHGCQAAAAAqWEXSTbZ4mJxJeK2co2yeJ554wl1m8eLF0bqX8uKlr0jSkCFDStbutVISiFKkjFU3za3s8Sq5ru2jibxjrVn7dN2xcq+X93hv2rQpWi9LwvOSKteuXRutb9y40R1r+/btteYvSzTykgP79evnLuPxtmXKNs6ZQmRtHgO2c+fO6Io3K6Wt7uOWkkSXkhS7atWqaH316tXuWAceeGC29cqp7rYse+xTzkPNsCcdZ5758+e71915553Rundu8OqStGjRomj9hRdecJc56qijovWzzz47WveS+yQ/jTXz83btseoeZ61O0E48ZKIL8Y4TAAAAAFSgcQIAAACACjROAAAAAFCBxgkAAAAAKtA4AQAAAECF0lQ9LxklaaKMqRkpaSJeOk5OZakdXmqLd1/KxsqZTpQzgciTkgyVomTd2jqBSPKPtZz7QTOSc1Ln8TQj1arsuSFnClrKsVNXyvNczmTRvfbaq92PteiKp+wDzUhpy32c5Xxer3tfciY+lj1edZdJSXpLeV7OOVa7H2c5U2I9Zdtty5Yt0bo3f8rzZtl98RJUvYS8VqfH5Tw2cu7nrQ6P9NIreccJAAAAACrQOAEAAABABRonAAAAAKhA4wQAAAAAFWicAAAAAKACjRMAAAAAVEiKI88ZEdisiMKUuMW6Y5Wtl3dds+KDc82REpuZM4I0RbtHt0p+fGvmOWovkzPyOEUz5s8ZH50zcjklxr/Vka9efGu7yPkTGymPWzPOAymR+DnPaXVjisvWK2WsunLHkec8zj3tfpzlPJ814/VDznNAmcyR9LVuXzZPzp/rSTk35nzOqDtHGeLIAQAAACARjRMAAAAAVKBxAgAAAIAKNE4AAAAAUIHGCQAAAAAqlKbqAQAAAAB4xwkAAAAAKtE4AQAAAEAFGicAAAAAqEDjBAAAAAAVaJwAAAAAoAKNEwAAAABUoHECAAAAgAo0TgAAAABQgcYJAAAAACrQOAEAAABABRonAAAAAKhA4wQAAAAAFWicAAAAAKACjRMAAAAAVKBxAgAAAIAKNE67yMxmmdmPzOwFM9tiZi+a2Wwzu2A3zHWZmYXGv8Nzjw+0IzM708xuMrPFZra5cflLMztrN8w108y+bmaPm9kaM9vaOKZvNrOLzWxo7jmBVjOz88zsK2Z2p5mtbpxjZu/G+d5iZt81s4VmtsnM1prZY2Z2uZntv7vmBVrFzD5vZrea2RIz29A4v3Ts8/tlmmNip9eIW8xsVMltb+1020tyzL+nsBBCq9eh2zKzv5V0qaSVkn4haZmkEZIOk7Q0hPCejHP1krRY0gRJJukbIYRP5BofaEdm9nlJfyFptaSfSXpR0n6SzpU0UNI/hBD+MsM8vSV9UdIlkoKkeyQ9JGm9pLGSZkmaKmllCME9GQHdkZk9IulwSRskLZI0XdJvQggnZZ6nj6QrJH1M0lZJt0h6UlIfScdIOl7SZkkfCyF8L+fcQCuZ2UZJjzf+vSypv6QjVZxb1kk6PYRw3y7OMVHSQknbJfWW9L9CCP8cud2BkhZI2tG43adCCF/albn3JDROiczsQ5KuUfHE/wchhHVdru8TQtiWcb5zJP20MeeZKl407hNCWJ9rDqCdmNlYSS9IWiNpZghhaafrDlHR2PSRNCaEsGoX5/qapIskzZP07hDCY5HbnCzpKyGEGbsyF9BuGvv2MknPSDpU0mPaPY3TlZI+LukJSeeEEJ7pcv3pkn4kaaikd4UQbsw5P9AqZtY/hLA5Uv+4pCsl3RlCOHEX55ioonF6VNJgSdtCCIdEbvePkv5MxWvKc0XjVAsf1UtgZn0lfUHSRknv69o0SVLOpqnh443Lb0q6VsWJ5b2Z5wDayUQVz1EPd26aJCmEME9Fk9Nbxbu8yczsGBVN02oVf/V7TdPUmPMOSUfvylxAOwoh3BFCmB9C2Lm75jCzY1Wcx9ZIelvXpqmxHrdKukDFpyquMrN+u2t9gGaKNU0N/69xOSXndJKulnSwmf2XZqzx+vXDku5T0WChJhqnNKep+PjOLZJeNbOTzewzZvanZnaqmWXdrmY2SdIZkp4MIfxO0rcbV33cXwro9hZI2iLpCDOb0PkKM5sq6ZDGbRbu4jwXNi6/2bVB6yqEsGUX5wL2VB3H2b+VHWchhJ9LelDSeEnnNGPFgBZ6V+NybuZxr1Hxkb2PdamfK2m0ij/CI0HvVq9AN3VM4/JlSbMlndDl+sfM7L/F/qKW6I9VNLnflqQQwtNmdo+kN5vZ0SGEBzLNA7SNEMKrZvYZSV+W9LiZdf6O0zmSnpb0/gx/JZ/VuLx1F8cB4Os4zm55Hbe9VdJRkk6U9IPdtkZAkzWCGIZLGiJppqRTJC2R9Kmc84QQXjSzn0s6z8z+JITwauOqj0taq+Kdrj/LOeeegnec0oxpXH5ExceJzpI0TMWXx78naYakXzTeEt0ljS/TfljSNknf7XRVx7tOF75mIaCHCCF8TdI7Vfzl7MMqgiI+oOJL7Fer+Ljerup4N6v03SYAu2R84/L513HbJY3LfXfTugCtcomkz0n6tIpPLz0g6dTGx89z+6aKEIr/LklmNkXSSZL+PYSwcTfMt0egcUrTsd16SXpPCOGXIYS1IYQFKj6f/YCKJuq8DHOdq+Jjgb8IIbzcqf5DFYlf7yEiGT2VmX1a0g0q/jo2RUUoymEq3un9mn7/+XAAPU//Vq8AkFMIYWIIwVR8XO5tKr7P95CZvWM3THeLipTMjo/r/XFjPj6mtwtonNKsbly+GEK4t/MVoYgpvKHxv2/KMFfHO0rf7lxspOn9UNIgFX+BB3qUxpdaL5f0HyGEPwkhPBNC2BRCeELS+1Sk6v2hmZ20i1O90Ljkr9vA7rO8cfl6frOm4zYvl94K6KZCCCtCCDer+P76RknXmtmwzHN0hERMbyRnfkjSnBDCIznn2dPQOKXpeEt1tXN9RzTygF2ZpPEF+JMb//vzTj9WFswsSPqjxnWERKAn6vgL3G1dr2h8r+k3jf89ahfnuatxefoujgPA13GcnfE6bttxG76/ix4thLBa0r0qvvd0xG6Y4tsqPup+raRR4t2mXUbjlOY2FXGPE81sUOT6wxqXu5r21dEQ3SvpW86/pZJmmtlxuzgX0G46viM42rm+47uGW3dxnqsalx8zs33Kbkg8MpCs4zj7aNeUzM7M7CwVfwzZKunHzVgxoMU6Pu2Q+2dsFEJYLuk/GnOsk3Rd7jn2NDROCUIISyT9TMXnr//ezKzjOjOboeLt0O0qfsgviZn1l/TBxv9+MITw0dg/Sf/SuA0hEehpOv5C/XEz27/zFWZ2lIrvEAZF3pGqoxHxf4WkvSXdbGbTY7czs1mS7t+VuYA9VeNj7d9U8Zf1m8xsctfbmNkp+n0I0j9U/TwA0B2Y2VTvY3hm9gkVvw/4kqQ5u2kVPq3i+/JvCyFs2E1z7DGII093sYq3VS+RdFwjHnyspP8mqZ+k/xlCeG4Xxv9DSSMlzW6ETnj+r6TPS3q3mV0SQlhVclugO/mJpJslvVXSk2b2UxXfR5qsIo68t6TLQwhPZpjrEhV/7btE0qON4/khFel9oyW9RdI0Sa9kmAtoK2Z2jn7/m0nDG5eHmNl3Om4TQvhQhqkuVvEH249KesrMbpb0pKQ+Kr4T/JbG7a6V9DcZ5gPawdsl/YOZ/VbScyrOI6MlHSdpuorzzPtDCLv66YmoEMJC7fonoNBgxXfHkMLMRkr6axUnnAkqdv77JP1zCGGX/gpuZndLerOKg+n7Fbf9roqAiE+FEL60K/MC7cTMekv6H5Leq+IEM0jSGhVNzTdDCMnv6jrzHa7iI7InqviC+gBJr6r4hfUbJH2nEcwC9BhmdpmKiGRXIwks13yzVBxnb5E0TsUfG6Xi+8EfDSFcn2suoNXM7DAV+/ubVZxXhkvaJOlZSb+W9JUQwuuJ6a+aZ6KKBmluCOENr+P2l6k47nntWAONEwAAaJnGd4V/I+lISX8cQri6xasEAFE0TgAAoKXMbKyKIKQDJH04hHBti1cJAF6D7zgBAICWCiG8ZGZvU/EbbQeY2bAQwppWrxcAdMY7TgAAAABQgThyAAAAAKhQ+lG9nTt3Rt+O2msvv9/y3sHy6mVj7dy5s9Yy3u0lqdNPLb2u9SpbxtOssTzedikbqye94+htY6u78VvAO9bKpBwH3U3KcdvKscrUfQ5MOW5zPjeX6cbHWu076x1PKftTzmO2Gcd/zv28TN37UrZeOY+zlMfYuy7nodHux1lwNkLZaufcb+s+1in7QMp9acbx1OrX1Cna9XzuHWe84wQAAAAAFWicAAAAAKACjRMAAAAAVKBxAgAAAIAKpeEQKV8YS/kCZt2xPClfVkv5kmnKlwXr3v+UsXJ+IdGT8z42a/42/x5tqbrHQKq62yjlccgZntLq/TDnF1DrfmG3bP5WB120+7GW8gX0us/3OUM9ysZqxn1pVtCMN8+2bdtqj9WnT59dXZ3/lPPY9LT7MZMi5/6Usn2asd82KwQm5fWmp+565TwHla1vzvvYjOcy3nECAAAAgAo0TgAAAABQgcYJAAAAACrQOAEAAABABRonAAAAAKhA4wQAAAAAFUrjyFMiCutGYpdFBKbEvXpS4l7rzp8St5hT7jj2ulIiOJsVld7ucsaX5nwcmhWfmjUqtOax3qw47pwx6TmfG+vOUXXdniJln21GjHzKWCn3ZePGjdH6qlWrat1ekubNm1drrLL977TTTovWx48fX3uslNcNOZ/L68bXt4tm/DRKymu3nNHmzYqkrztWyr6ZMlbO9cq5X6Tse3Xnb++jDwAAAADaAI0TAAAAAFSgcQIAAACACjROAAAAAFCBxgkAAAAAKlhZ0sXOnTtrx/PkTKKrO0dKalhKmklKase2bduidS/lpCz9pO42a1bSlyclASglfarkMW77CLCQ8EDUXSQlDbMZaX9S3oSeuvPnTA5MWSZlX895X3Laa6+92vpYSzmneZrxtJJ7H/DG27JlS7S+aNEid6wvfelLteY46aST3LFmzJgRrffv3z9aX7hwoTvWiy++GK2/973vjdZ79y4NF47KmcSZMk83OKdF72zZvpkzKbDuc2ru10h170vO7dKsJNycrx2b8bikjOWdz3jHCQAAAAAq0DgBAAAAQAUaJwAAAACoQOMEAAAAABVonAAAAACgQmmcTN0UqKrrcklJtfNUpApG614C0Y4dO9yxXn311Wi9X79+0frgwYPdsbx5hgwZEq0nJtFF6ynJhSn7izdWSjJc+wcQ5U2drDtH2Tw5025yPm+k7AeenPclZ9pezrFSlmn18/zukDM9KmW75eTNs3HjRneZ2bNnR+vLly+P1lesWOGOtWDBgmj95JNPjtbf9a53uWMNGjQoWveO5bJtvHr16mi9V69e7jJ1tXPaWDtI2QZ1n2tTUlq9ZZqVXukpezxzJqXW3W+asS/nXqYZxwbvOAEAAABABRonAAAAAKhA4wQAAAAAFWicAAAAAKACjRMAAAAAVKBxAgAAAIAKpXHkKTGQOaOAc8YwehHi27Ztc5dZtmxZtP7MM89E65s2bXLHWrt2ba1lXnzxRXes/v37R+tjxoyJ1j/2sY+5Y3lyxlA2K+653SNaU7Q6Qrzu3CnrVTVeTNlzw7e+9a1aY33kIx9xr6t7X3LexxTNisLursdays8seHI+R6U8buvXr4/Wf/CDH7jLePfzuOOOi9b32Wcfd6xjjz02Wj/xxBPdZTyrVq2K1p999tlo3ft5D0maOXNmtJ7yExfe45Lysxy5fy6lnTXj9UOztk3Kz5zUfW7I+bom53NZmbrrlfvnNepu47LtUvYcEL19rVsDAAAAwB6IxgkAAAAAKtA4AQAAAEAFGicAAAAAqEDjBAAAAAAVSlP1UtIp6iagpCRdeGNt3rzZHevRRx+N1l955RV3GS/ZzpunLOnnpZdeitbvvffeaH3BggXuWAcddFC0fv7550frrU5myzl/2Vgp69zucibR5EypzJ0e5433zW9+M1ofNGiQO9bAgQOj9d6940933rEpSePGjYvWU9KRvO1fN9FHSks8rZuc1IykpXaRst1yPkel3P6OO+6I1suSYt/znvdE68OHD689/0knnRStX3vttdH64MGD3bG8VD3vXDty5Eh3LC/VL+fzYs7nuLKxUp4b2kHOZNWUVLu62y1n4mzZeM14TZ0ziS7nfU+5j81KCKyrex6VAAAAANBENE4AAAAAUIHGCQAAAAAq0DgBAAAAQAUaJwAAAACoUJqql5JOkTPZrG56npecJ0lz586N1nfs2FF7/jVr1kTr8+fPd8fatGlTtO6lhni3l6QlS5ZE61u3bo3WH3vsMXesGTNmROspqWHNSLXLnX7TE6Uk5HgpRDkTp/7t3/7NXebll1+O1seOHRutT5482R1r3bp10XqfPn2i9bIEJu849BL6WjRwRQAAIABJREFUvLqUNzXKk/MYyJnQ1y5StnXd57Wcz1FlY3n7c9kxu3jx4lrr9dxzz7ljXXHFFdH6rbfeGq2PHj3aHctL+5swYUK0ftppp7lj9e3bN1pv1vNi3f0iJYWxu6btlal7rsmZsJs7Jbbuc33O19plmvFarBnPsWXL5Hwt4+l5Rx8AAAAAZEbjBAAAAAAVaJwAAAAAoAKNEwAAAABUoHECAAAAgAo0TgAAAABQoTSOPGd0oSclBnL9+vXR+sKFC92xtm/fXqteNp4XnTxy5Eh3rH79+kXrAwcOjNafeeYZd6ypU6dG6yeddFK0Pn36dHesurGmZZGO3lg5YzO7awxyqpTjI2e0p1d/9tln3bGuv/76aH3lypXuMnfffXe0ftZZZ0XrZcett496++crr7zijuXFnnsx/l4Ucpm6275M2fFcNya2Jx5rOaOF68bxpsxf9hicfPLJ0br3cxmSdM8999Sql/1cx5YtW6J1L5J/6dKltcd685vfHK2PGjXKHcuTsj/njFbekzTjp0lSpMS7p8RYe8ts27YtWn/hhRfcsbyf3/FeU44ZM8Yda/DgwdF6yuPl3cdrrrkmWvfuuyS99a1vjdbL7kv//v1rrVdOvOMEAAAAABVonAAAAACgAo0TAAAAAFSgcQIAAACACjROAAAAAFChNFXPU5YakzNZbePGjdH6008/Ha2vXbvWHWvdunXR+hNPPOEu46WA7LPPPtH6pEmT3LE8XhLfqlWr3GWmTJkSrXtpg2VpJnUTo3KrO09Kql934K17SkJMzlQnL+2qbL0WLFgQrT///PPuMt51XqJQWdqXty2942DDhg2112vYsGHRundslklJtcu5TIp2Tc1KlXLMpGyDnAmGXqrUueee6y7jHc9e4mVZep2XeLfffvtF69/+9rfdsSZMmBCtH3DAAdF6s16DNGOsnCmM7S7lcWtGSlrZ+cRLvCt7vbl8+fJo/bnnnovWt27d6o7lvXb1lilLdv3whz8crY8dOzZav+WWW9yxvCTcefPmRetDhw51x/K2pZc6LUmnnHJKtO6dg3v16uWOVfe5vPu+0gQAAACAJqFxAgAAAIAKNE4AAAAAUIHGCQAAAAAq0DgBAAAAQAUaJwAAAACokBRHXhb77MX6eXUvBlWSFi5cGK0//vjj0XpZhPeSJUui9VdeecVdxotb9WIVy6JDJ0+eHK2PHz8+Wj/ssMPcsRYtWhSte/HtKVodj93TYlh3h7rbteyx86K6H3300Wjd+0kASXrmmWeidS+mVPIjV+fOnRute/HFkrT33ntH6972WrZsmTuWN7/3vDF16lR3LE/KTwJ4j2XZc7O3TKujzdtB2f2pe5yV3b4ZMe59+vRxr/OiigcPHhytl+1P9913X7R+2223Retl2+Xwww+vNX/KNs75Mw1l6v4sRs5jtjvLGTvubR8vdvzll192x/Iiucv2Jy/23zvPLl261B3r9ttvj9a9n8pIed7evn17tP7AAw+4Y3kR4mPGjInWy86NZbHjnieffDJa934OwXuOS8E7TgAAAABQgcYJAAAAACrQOAEAAABABRonAAAAAKhA4wQAAAAAFZJS9VJ4aR5lqXpeEp6XguWlbEh+qt0JJ5zgLjNkyJBo3Uvt6t+/vztW797xTe2licycOdMda82aNdG6lxhTllZTN1ErJQEop5REx+6cQJSS9lU3OUfyj8PFixdH614KniRNmTKl9jJeqpF3rJWl6k2aNCla37x5c7RelhDoped5Y6VIOW5S9umUhDLUTz3M+XiWPTY5E9e8ZbzkLEn66le/Gq17x1PZOW3atGkla1dP3W2Z+/xQ93HpzucnT877lPPY8J6358yZ4461adOmaL3sfOolW44bNy5anzhxojvWggULonUv8dZ7HShJ1157bbTubeOyc+MxxxwTrZ911lnRuve6QJKeeOKJaL0sKdp7XLxEQ1L1AAAAAKCJaJwAAAAAoAKNEwAAAABUoHECAAAAgAo0TgAAAABQoTRVLyXNxLvOq5elUy1cuDBa//73vx+tl6WJzJgxI1ofMGCAu8zFF18crS9atChaf+mll9yxvCRAr37IIYe4Y/Xr1y9a91KzyhJutm7dGq17CYEpCVwpCTsp83TnVL2U++s93t4x9fDDD7tjvfDCC9H6ihUronVvH5Sks88+O1p/05veVHv+UaNGReuDBg1yx/J4CT1l+8fUqVOj9eOOOy5ab1ZCXc7n5pS0ve56rKWkhdY9p6UkYeZMMc05/7333uuO5SVOemN5aVuSny6bsp97cp6HUrbxniQlcbLu41M21quvvhqtP/XUU9H60qVLa8/Tt29fdxkvJe/444+P1r39XypPa47Ztm2be523zt4+O3LkSHes6dOnR+vea9ey5EDvtb6XnFc2z9ChQ91lPHWPc95xAgAAAIAKNE4AAAAAUIHGCQAAAAAq0DgBAAAAQAUaJwAAAACoQOMEAAAAABVK48hzRiRv2LAhWp89e7Y71vXXXx+tv/jii9H6aaed5o7lRRR+8pOfdJfx7r8XkfzII4+4Yy1fvjxaX7t2bbQ+efJkd6yxY8dG6wsWLIjW58+f7461bt26aH3WrFnuMp5mxI6XxfN2Zzljb9evXx+te5Hfkh/f6m3vvffe2x3ryCOPjNbLYlWfffbZaP2YY46J1lO2y5VXXhmtT5gwwR3Lm9/7eYMydaOVy+5jShS1J+fzfLvLGZOcMpYnZ7x7Soy897MUjz32mDuW95ME73rXu6J173khRcqxkTJWM6LFe2J8ec7Yf+/xfPnll92xfvOb30TrK1eurDWH5Ed4H3HEEe4yXlS39/M3ZfOPHz8+Wj/99NNrj+U59NBDo/WymPQHHnggWvci35csWeKO5f20Qe/efoty8MEH11om53HWPc9+AAAAANBENE4AAAAAUIHGCQAAAAAq0DgBAAAAQAUaJwAAAACokJSqV5ZCs2PHjmh91apV0XpZ4tsTTzwRrQ8fPjxaP+yww9yxPvWpT7nXeeomw3j3vYyXZuSlnEnSmDFjovU5c+ZE63fffXft9fJS9cqSSeqmhpUt4+mJCURlUradt+94iUKSvx96CZIHHXSQO9a4ceOi9bJEpdGjR0frKfuU5xOf+ES0npKollPOdLaUeXJu43aXcl+961qduOYdTynrNXfu3GjdS8OVpLe+9a3Rupcodu6557pjeffFOwZypjqmbK9mndOa8fyzO+Rcb+/cdNNNN7nLbNq0KVpPeU49++yzo3XvPCfVT3YrS5y7//77a43lpS5L0llnnRWte8mBffr0ccc67rjjovVLL700Wr/hhhvcsaZNmxatl6U7e+vWlHTn2jMAAAAAwB6GxgkAAAAAKtA4AQAAAEAFGicAAAAAqEDjBAAAAAAVSlP1UtIpvNSS++67L1r/xS9+4Y7lJfocf/zx0frkyZMr1u61Uu5jyjJ10wZ/9KMfuWNNmDAhWh84cGC0fuGFF1as3Wt5j2Pu7VU3uTElAak7JBOl3F/vMapbL5vfSzSaN29e7fUaMGCAu8zmzZujdS/ta/369e5Y/5+9O4/Tq6rzff9bVKpSlco8J2SoECiSkAQIkwxmQEVGRVQasWn0tAwK6ctF+9zbfa+t957uc26fbtrWtgVpcFaUo43aIqIMCcpwI2EIY0hCQobKQCpzZaxknz+eh35x6N9vLdfKyq7nqXzer5evkt9+9lr72XuvvffKU/V9brrpJnOZ5s477zSX3XDDDVFtpaSzpZyfsSlkvn5SxnoZaWdHQk9fP2KTWnMn93V3d6v1V155Ra37juepp56q1ufPn6/WUxLNUhInU8ZGGWJTLUXqN9ky5fpgPSOtWrVKrVvJeT5W/xMmTDDXsRJffe8lNomztbXVbMtKkbbOG+v52NdPyvPH2rVr1bqVAvjiiy+abVmuu+46c5mVqpfzPmup7bscAAAAANQAJk4AAAAAEMDECQAAAAACmDgBAAAAQAATJwAAAAAIYOIEAAAAAAHeOHKLL6Jw3759av3BBx9U62vWrInuf9iwYWr9Xe96l7lOzljPAwcOqPWVK1ea6zz55JNq3Yppb2hoMNv64Ac/qNZnzpyp1lNiWHPG45a1Tm+UEt/ap48+rFPaWrFihVr/4Q9/aLY1YMAAtT5mzBhznf79+6v1trY2tT5jxgyzrQceeECtv/zyy2q9o6PDbOtLX/qSWr/88svV+ujRo822LFZMqu947d69W637xo21j61YV198az1E/Gvq7VqUsp9927tkyRK1bkU7T5s2zWzLug9ayordjo3X9/Wd82sxYiOqRWo/3j9WyrOj9dUXvrase+DUqVPV+qxZs8y2mpqa1HrK1zVYhg4dai4788wzo7bLik8XyfuVFNYzg3Ucre0Vsb/2xPderLGRc8yafWdrCQAAAAB6KSZOAAAAABDAxAkAAAAAApg4AQAAAEAAEycAAAAACPCm6qWkUOzYsUOt/+53v4vuo1+/fmp9/Pjxaj0l0cpKBhGxk6u+853vqPWvfOUrZltr166N2i4rFcbHSl9JSWZKSeKz5Ezgqtc0ryPBSpVpbW1V675UGysJZ/369Wp93bp1ZlvWmFq+fLm5zqBBg9T6wIED1bqV3Cdip8Rt3rxZrS9evNhsa/Xq1Wp98ODBat2X7Gmdu9ZYt65/IiKbNm1S675xe/zxx6t1a3+lXP974/iMTRhNSY9Lud5a/XR3d5vrLF26NKqtc845x2wr9n6bkl6Xooxz0Jd2V0ZCYL2m7fn2zZYtW9S6da3zaW9vV+vTp09X675rbRmpmr7jOW7cOLVunec50/58yZlWQqd1z/T1bT27DhkyxLN1Out45Ryz9Tn6AAAAAKBETJwAAAAAIICJEwAAAAAEMHECAAAAgAAmTgAAAAAQEB/dJv50iu3bt6v1ESNGqPUVK1aYbQ0bNkytW4laVnKfiMjEiRPV+rZt28x17r33XrX+r//6r2rdl/5i7TMr6WzKlClmWy0tLWrdei9PPPGE2dYLL7yg1q+//npznVg507l6a9JXyvu11rGSaHzn1EsvvaTWn376abVuJU6K2Nu8d+9ecx3rnB45cqRa9401Kz1v5cqVav3JJ58027LeS0NDg1q30glF7PQ66xpgpSOKiPTv31+tW+mEIvGJbikpRLU+1mIT8kTik6jKSOHy8SW47ty5U61PmzZNrVvnmUj8NSvl3Mi5Ts4UxJTtytl/rUvZP7HJqlbiq4j9jGidzynXupR7c8rxTLlmxbK215eqt2bNGrW+f/9+tT5r1iyzrUsvvVStT5gwwVwndjzl3F984gQAAAAAAUycAAAAACCAiRMAAAAABDBxAgAAAIAAJk4AAAAAEMDECQAAAAACkuLIfayY3jlz5qj1rVu3mm1Z61jxlAsWLDDbWrZsmVp/9tlnzXWsWGMrbtEXaXnGGWeo9bPPPlutW/tRROS1115T6w888IBaP/fcc822Bg8erNZvv/12tX7ssceabc2cOVOtjx492lzHYsVT99Y48pT3Za3Tp48+rK1obxE7ln7Lli1q3RqDIiLNzc1qfcyYMeY65513nlq3ttkXEd3V1aXWrSjmgwcPmm1ZUeF9+/ZV60OHDjXbsuLIrbr1dQwi9tc7WONZRKRfv35q3XfdstTDmNKUEe9cRoS2j++rAqxjfcopp6h1K9bZJ+e5kRIFnasPkbTjUq8R4jmlRK9b11Traxmse5OI/dU0p59+ulpPOQdyRtL3NGu7rHuTiMikSZPU+jXXXKPWfc+OV155ZdR2icQfF+LIAQAAAKBETJwAAAAAIICJEwAAAAAEMHECAAAAgAAmTgAAAAAQ4E3VS0mnaG9vV+tWOtZZZ51ltnXyySerdSuJ75VXXjHbeuSRR9T6gQMHzHUsVuKd9d5F7KQRa1/+4Ac/MNuykgC7u7vVui+h76STTlLrEydOVOu+92ilqa1bt85cx0otGzt2rFr3pbzUs5xpgVZyli/xzUpEtM5bX3qbdY74+reSAK1Ur+3bt5ttdXR0qHXrnPrABz5gtmVt8/XXX6/Wp0+fbrYVmwLkG7cp48Dqx5dQmKuPWlFGqlVKEpR1DMran9b4Szk3rOtPShJdGe8/dzpa7HNTSlu1LmW7rWcBa//4UtpuuOGGqLZynwOx15mcSZw51/HdZ6x0aeu43HzzzWZbZaRk5hxLfOIEAAAAAAFMnAAAAAAggIkTAAAAAAQwcQIAAACAACZOAAAAABDgTdWzpKTAWKldvuSotrY2tb5+/Xq1vm/fPrOtYcOGqXXfe7FS6saNG6fWZ82aZbZlsd7/7NmzzXV27typ1pcvX67W16xZY7ZlpaN1dXWp9cWLF5ttWQlMvuNiJZ2NGjVKrVvpTyL2sbRSnuqBb9ut/W2tM2jQILMtK93yzDPPVOsLFy402zrllFPUum+sW+mWnZ2dUa8Xsc+d4cOHR2/X3Llz1frxxx8f3ZYlJVEoZwpjyuvLSKc7Eno6pS12v6Xca610MpH4lDzfe+nJxDffdTFnqlbOxMl6Sxs8HCmJzNayyy67LLqtnMmGKcct5f1bYs+BnOeGb3svv/zyqHVyp0fmTImNfXas3ydKAAAAACgJEycAAAAACGDiBAAAAAABTJwAAAAAIICJEwAAAAAEMHECAAAAgAAXiIeMzk7MGRFotfXVr35Vre/evdtsy4rEPnjwoLmOFX3d3Nys1vv27Wu2dfPNN5vLNHfccYe5bOPGjWrdOlwjR44027L2sfUeW1tbzbYaGxvVektLi7nOqaeeqtatSOmUqN9jjjmmtrNbJW2sedpS6ymx7DnHre/YLVmyRK1Pnz49W/85I7Rz9lFGW6ntxar1sXbo0KFs97RcrxfJGzv96quvmsseffRRtX7ttdeqdd+1u4yo7FoeT7HrpFxLPdfsmh5nIqLubN8+iN3XKfHWZZ0b1vss49qQM3Y/5X6Scx/n/DqWxPGnvhk+cQIAAACAACZOAAAAABDAxAkAAAAAApg4AQAAAEAAEycAAAAACEhK1Ssr6cZKx7DWSUlZ8YlNJwvsy6i2fAkgmzZtUuuDBg2K3i4rVXDLli1qfejQoWZbDQ0Nat2375uamqLa8u2XOk4gKiXtyyfn+ZkiZ6JSbB8p6Ugp+6UnU6NE4t+LjydRqabHWso4yzkGYu+DKalSq1atMtf55S9/qdbnzp2r1k866SSzrZzpWbHr5Ey7SxlnOY+9T70mxVrjrKyUtpzPjjmf93KeAzmVMZ5yHnuRvM8gnvspqXoAAAAAkIKJEwAAAAAEMHECAAAAgAAmTgAAAAAQwMQJAAAAAAKYOAEAAABAQB/fwpwRhVZ0YE/HM+aMTk2JNU2JoR05cmSWPnwaGxvVuhUf7pMz7rXG046TpcRL5xwHscchJSY5p5zRpikR4jnjiFPaKuMrGcqKXC5TSrR4GbG3Ods69thjzXUGDBig1p999lm1vnPnTrOtrq4utf7e975XrZcRq+yTMmYsOa/Lub9GpZblfEbKGVWdO/Y69n6e8xzIuY9T+8nVVllffRR7jvGJEwAAAAAEMHECAAAAgAAmTgAAAAAQwMQJAAAAAAKYOAEAAABAgAskTagLy0q6yJlEZ0lJ1Mr1ep+U7cqZJpJTznTGxPSZmo8myjnWUpLDcp7rKUl8OZOoLGUkVJWV9FTGWE8ct7U+1qLHWUqqV6yUPlLGQEdHh1p/8MEH1bqVrioicuaZZ6r1yZMnq/U+fewQ35zJYbFy7+PYY+nrw3PNrOlxdujQoegd15PPNbnTnWPfS87kvrLujbH9506PzHld9swp1Mb4xAkAAAAAApg4AQAAAEAAEycAAAAACGDiBAAAAAABTJwAAAAAIMCbqidGAlHOdK4UKX3kTC0pI+WlrASynGlGmZPwovXGVL3AOlGvz50cFKunz+ky0oZSErJS0oGs61lKQljsdgXaqumxljO90pJyDHImrqXcn/ft2xfdVt++faO2q6evPz0t87W0LsdZipxJdClSxlmsnMmOvvts7DbnvGenyHk/zZleySdOAAAAABDAxAkAAAAAApg4AQAAAEAAEycAAAAACGDiBAAAAAABTJwAAAAAIMAbR37o0CF1YRmR4z4p8ZQ9vc0pkcNHuu8UPR0pm9K/6+mD/wfIGd+aM3a7rLZio//L+HoBkbRo01hlxN2K5I1Wt9RrTHLO6Pec51POaGGRvOM8Zx89GfmccuxzSvwKgZoeZynPjjnPzdjxlPLsmPOaUdbXa8S+/5TjlXPM9PRXmFjPjnziBAAAAAABTJwAAAAAIICJEwAAAAAEMHECAAAAgAAmTgAAAAAQ4E3VAwAAAADwiRMAAAAABDFxAgAAAIAAJk4AAAAAEMDECQAAAAACmDgBAAAAQAATJwAAAAAIYOIEAAAAAAFMnAAAAAAggIkTAAAAAAQwcQIAAACAACZOAAAAABDAxAkAAAAAApg4AQAAAEAAEycAAAAACGDiBAAAAAABTJwAAAAAIICJUwbOuXc75/6Hc67DObfPObfBObfAOfcnGdpuc84Vgf/dkuN9APXAOXeRc+4B59wbzrm91Z+/dM5dkqn9t8bcqhztAfWgVsZV9d5ZOOfm5ugXqAXOuaHOuf9UfVZc6pzrcs7tcs4945z7v5xzrZn6efsz44vOuQbjdU9XX9OWo9+jSZ+e3oB655z7f0Xk8yLSKSL3i8g6ERkqItNF5GIR+U6mrraLyD8ay57K1AdQ05xz/1VE/kJEtonIT0Vkg4iMF5EPichFzrn/VhTFX/bgJgJ1h3EFHHFXisjtIvKmiCwQkftEZLCIXCQify0iVzvnZhdF0Zmxz5NE5Ppqv8jEFUXR09tQt5xznxCRb4rIr0XkI0VR7HzH8saiKA4cZh9tIrJSRN4oiqLtcNoC6plzbpSIdEjlHxFmFkWx9m3LpojIMyLSKCIji6LYehj9tAljDkeJWhtXzrkFIjJHROYVRbEgtT+gljjnzheRgSLyi6Iout9Wb5bKP1a8X0S+UhTF/3aY/bRJZZx1SGVi1iUiJxRFsf0dr3taRE4TkUlFUaw6nD6PNvyqXiLnXJOI/H8isltErn7npElE5HAnTQD+F21SuWY9+/aHOxGRoiheFZFXpfIp+tDyNw2oW23CuAKOqKIoHimK4qdvnzRV63tF5L9U//P8jF2+KSJ/JyIjROT/ztjuUY+JU7r3isgoqXzatMU5N8859znn3Gedc+9xzrFvgbyWicg+ETnVOTf27Qucc+0iMqX6mpU9sG1AvWJcAT1rf/Vnt/dV8f67VP585M+cc5Mzt33U4m+c0p1V/blJKr+vOvsdy19wzl1RFMXyTP0Nds59UamvKoriW5n6AGpWURRbnHOfE5Evi8iLzrm3/y3G5SKyVEQ+XhTFoR7cTKCu9MC4su5lb2nL1A9QL66v/nwgZ6NFUex2zv2liHxbKpOoD+ds/2jFxCndyOrPP5XKjP4SEfmdVD6F+isR+WMRud85N6Moiv16E1EGicgXlPpCEflWhvaBmlcUxVedcyul8reFn3zboo0icpdUfq0IQISSx5V1LwOOOs65j0rlOXK1VCY3uX1XRP5MRK6ohk88dgT6OKrw62Tp3tp3DSJyVVEUvyyKYkdRFMtE5E9E5GkRaZd8M/w3iqJwyv/mZmofqHnOuVtF5Gci8kMROUFE+kklwXKBiHy1WgcQoeRxZd3LXFEUTir/GAj0es6590tlYrNdRC4vimJb7j6KSgLcrdX//JJzzuXu42jDxCndWyf4hqIonnz7guqJ+rPqf55Z6lYBvZRzbo6I3CaVVKI/K4pieVEUe4qieElErpZK+tdH+f4X4A/HuALK55z7gFSeE3eKyPlFUTx7pPqqfsr0ExGZJSLXHql+jhZMnNK99asL1r8QvBXb2lLCtgBHg8uqPx9+54Lq31+89S/Vp5W2RUD9Y1wBJXLOXSWVicwWEZl7JCdNb/OfpRJC8Te5vmz3aMXEKd3DIlKISJtxEk6v/iSJCMijqfpzhLH8rb87zPE3hcDRgnEFlMQ5d72IfF8q37M0u/rJ7hFXFMXrIvIVERkrIv9HGX32VkycEhVFsVoqX1rWLCJ//fbfG3XOzRCRT0glWvJ/9MgGAr3Pb6s/b3DOTXj7AufcaVL5e8JClH85B2BiXAElcM59VkS+LpV/UJ+dMXX5D/XXIrJZRD4nlSAzJCBV7/DcLCKnisgtInK2c+5xqZyMV4hIXxGZX53lAzh8PxGRB6XyDesvO+fuk8q/2h0nldjkPiJyW1EUL/fcJgJ1h3EFHGHOuT8Rkb+v/ufDIvJJJadhW1EU/3iktqEoiu3Oub8Ska+JyLgj1U9vx8TpMBRF0eGcO10q38p8uVQmUl1S+Z3wvy+Kgn+hAzIpiuKQc+5SEfm0iHxMRD4gIq1SSSRaICJ3FkWR4xPehupPfjUJvV6J4wo4mh33tv9/vfGaN0TkiE2cqu4UkZtE5KQj3E+v5SoBcAAAERHn3Nki8oSIPF4UxXk9vT0AAKA28DdOAPC/+lD155PeVwEAgKMKnzgBOOo550ZL5dvVTxaRi6Xy3Rozi6JY1ZPbBQAAagcTJwBHPefcKVL5os+tIvI7Efmroiie79mtAgAAtYSJEwAAAAAEhFL11FmVb7KlxCuKiMihQ4fU+jHH2H9mlXNSF7tdIva25dwuqy3ffoltK4XVlrUfU/u32ks5XpZjjjnG3ugacejQoWwHL+XcsaTsb+vY5Tw/fe8x9lpT1j8exY71nNdZn5Tj5Rm3NT3Wco6zMt5qSh9l3AdE8r7/lPtNrrZyv8cyrid1cE9Td0JPP2+lKOM5NPFam62tMt5jCt/5EvteUrbLup8RDgEAAAAAAUycAAAAACCAiRMAAAAABDBxAgAAAIAAb6qe9Ye0KQEBsX/g5mP9wVjO7fLJGZxw4MABtX7w4MHo7WpsbFTrffrYGSDJW1aCAAAgAElEQVQ590uK2D9KTAw+qPU/pBVJ+GPaWGUd05Q/QI09D3p6rJch5T2mhO2U8ce0taJIOKA5r1Gxx6CnpWxXzj/yt9rKGYKSsk7KtSzlGcjTVk2Ps5QQlp4MLqjlEJQy7o2WlBCmlLZS5Hw2slghLHziBAAAAAABTJwAAAAAIICJEwAAAAAEMHECAAAAgAAmTgAAAAAQwMQJAAAAAALsnOpEsVGkvkhBK1YxJaI0ZxSx1f+ePXvMtpYsWaLWX3rpJbW+a9euwNb9R83NzWp99uzZ5jpTpkxR6ymxuWUcl5S2ajy51StnHGjOCG9f32VE3OeMjy0j7ta3LGdEte+4rFq1Sq1PmjRJrdfzuLGkxLhbUsZGGfs0ZZxb29zZ2Wm21dXVpdbb2trsjTPE7v+yrmWxffj6KSNuu1bkfK/1eB3qyW3OGUdeRuS3SNo9sKznCQ2fOAEAAABAABMnAAAAAAhg4gQAAAAAAUycAAAAACCAiRMAAAAABHhT9XIm0eVMjsmZjJSSgLRhwwa1/otf/MJsa+fOnWq9oaFBrZ966qlmW0OGDFHra9asUesPP/yw2dawYcOi+rC21ydnMlxvlZJElzM9LjZxqqyxlrJfcl5rYt+nr29r2be//W21vnz5crOtpUuXqvX9+/eb63R0dKj1r33ta2p91qxZZlvWfqnHBCwR/7kZew74Xl/GuZmSfLpp0ya1/uijj5ptTZw4Ua1bKY0pYyPl9bHXkpRrmU/OZ6B6HU8p7zVnGmtsWyl95Dw2OfvPOTbKmgP0dOJk7PvkEycAAAAACGDiBAAAAAABTJwAAAAAIICJEwAAAAAEMHECAAAAgABvqp4lJR0jZzqHlYKTktDmS9Tp7u5W67/97W/V+p49e8y2Ro0apdYnT56s1k855RSzrcbGRrU+duxYtb5582azLStpy+pj4MCBZluxyWwpUtLU6iGZqIw0ypT+c7bV08lBFl9bseeulVAnYo/D1atXq/Wf/exnZlu7d+9W6/v27TPXscbu008/rdZnzJhhttXc3Gwuq0cpSXg5U9LKSpU6ePCgWn/ttdfUupUGKyIydOhQtZ7yXmLv9Sn3gZ5OerPUw/0pVkriY+xxS0mczTlmc57PZSXOxurp/VXW815sW3ziBAAAAAABTJwAAAAAIICJEwAAAAAEMHECAAAAgAAmTgAAAAAQwMQJAAAAAAKS4sh9YuMLfdGBvojYWClR2YsXL1brGzZsUOunn3662dYZZ5yh1q336Hvv1jYPGTJErb/rXe8y2/rVr36l1tesWaPW//Zv/zZ6u1KOY0qEeT3HupYRsZ8SeWrJGe3r699a5/bbbzfbsiKXu7q6ouoiIitWrFDrLS0tav3AgQNmW88995xaX7lypVpvb2832xowYIBat7ZXROSGG25Q69dee61ab2pqMtuq5+h/Tc6x4RN7nqfcB3zrWOf6Cy+8oNaHDx9utmXdb8r4aoOyvqahjGspKnJ+lY0lZZyVEcmdM3a7p2P3U/pPia+3lPG1OHziBAAAAAABTJwAAAAAIICJEwAAAAAEMHECAAAAgAAmTgAAAAAQkD1VL2cCipWCkTN9Zffu3eayp59+Wq03NDSo9ZNPPtlsy0oNSdnm2KSfSZMmmW2tXbtWrd9///1q/YorrjDbOuuss6K2SyT+WKYks9WDnCmCVqpcyv6x2tq/f7+5jnWMfIlvX/7yl9X66tWr1fqSJUvMtvr00S9r1vv3JeGdffbZan3KlClqfeDAgWZbY8aMUevWsfddT/bs2aPWW1tbzXWuvvpqc9nRImdKVMo1KjYlKuXauWvXLnOdF198Ua3v3btXrfvOQStVL+X+bL3PMhLNelrO91jPcqahWW3lTEnMmRKbkuqXMwkvZ6pvyv7KmXhXBj5xAgAAAIAAJk4AAAAAEMDECQAAAAACmDgBAAAAQAATJwAAAAAI8Kbq1WpyS0oyiZXa8dxzz5nr7Nu3T61/+MMfVustLS1mW5aUJJnYhL4NGzaYbT3++ONqffjw4Wr9lFNOMdvKmaZURvpVLbGOqZVqJyLS1dWl1js6OqL7t46DlTi3dOlSsy3rvVgplSIi9913n1rfunWrWp88ebLZ1pw5c9T6uHHj1Lrv/LDG9NixY9V6c3Oz2dbUqVOj1nn99dfNtj71qU+Zy2KljJt6HWspiU+++0psWzn32wsvvKDWH3jgAXOd733ve1H9Dxs2zGzrxBNPVOsp77GMFLKcabw+KdtsqfXxFKunEwSt+1zKs2OKnImPKfslZ3Jh7DaX9R5zXjMsfOIEAAAAAAFMnAAAAAAggIkTAAAAAAQwcQIAAACAACZOAAAAABDAxAkAAAAAArxx5Dmjoi0pMbA54yFXr15tLuvTR989I0eOVOsp8b0prLas+rHHHmu2ddxxx6n10aNHq3Xfvk95j7HHOGekZC2x3u/evXvNdX7961+r9Y0bN0b3393drdZjo+9F7PfS1NRkrtPY2BjVz5lnnmm2df7556v1hoYGtb5t2zazLSv23Orf6kNEZMGCBWp99uzZat2KVfdJicK29LYoZJG8ccQ544BTXm99lcSTTz5prvPKK6+o9bPOOkutd3Z2erYuTs44Yt99KHYf5xwzPjkjn1OizWtdzn2d8xzo6dhvS87Y7bvvvlutX3LJJWZb1tf1WF9hknJ8recCEfs5vF+/ftH9xB7j3jf6AAAAACAzJk4AAAAAEMDECQAAAAACmDgBAAAAQAATJwAAAAAI8KbqWVISSKx1ykosyZlOY/VTVqJVbFu+17/vfe9T6xdddJFa9yWjWcpKYaxn1nnY0tJirnPhhReq9bVr12bZJhGRzZs3q/UNGzaY64wYMUKtr1mzxlznlltuUesHDx5U65dddpnZ1vTp09W6de4sXrzYbOsb3/iGWv+Hf/gHtT537lyzrT//8z83l2lyplSKxCc6pSRA1XoSX8q9I/YeUVZ63I4dO9S6b/xb4+m8885T662trWZbZdzT6vHekfMZqF7vdynvNXa/pTxvWWM5d6pebFspx9lKtfvud79rrrNixQq1vnLlSrW+c+dOsy3r2mRdM6zkXhF7v/ieNydNmqTWrWtZc3Oz2VYsPnECAAAAgAAmTgAAAAAQwMQJAAAAAAKYOAEAAABAABMnAAAAAAhg4gQAAAAAAd448pToxJxR2Vb/sXURO4bV178VXZkzQj1n3GhKHx//+Mej2srdf0/GtNeSlG23Yj+PP/54tZ6y79ra2tS6L1q0sbFRrVsx5SIiV199ddR25Yw2fv7558227rnnHrVuxbSOHz/ebKuzs1OtDx8+XK373qMVBeuLr855Dar12PGcrH2ach+K7cPHOgetyGERkUGDBql1K9r3xhtvNNuytjkl8tlSqxHetfoMUCtyjoGUtqxzMGWcWcfHdw9saGhQ6ynvZf/+/Wp93bp1av2+++4z21q4cKFa37NnT/R2zZ49W62PGTNGrQ8cONBsy7pm7dq1y1yno6NDrW/dujVqu1LwiRMAAAAABDBxAgAAAIAAJk4AAAAAEMDECQAAAAACmDgBAAAAQIA3VS9n2ktKyklsok5KopSPLzVFkzOhLyX9JWfST0r/lpTkRGudlIS+ekgAy5keZSX6+I6pdX727dtXrTc1NUVv19SpU6PXSbluxKadWYmbIiIzZ85U6+vXr1frvuRA6z1a78U6jqlizzHSvtLaynmN8rX10ksvqfUdO3aY67z//e+PqudMwku5D8T2IRKf6pfzPaaoh/tTrNjERZH4/VBWsuHu3bvV+ssvv2yuM2HCBLVuJcv5xuyyZcvU+lNPPaXWFy1aZLZlOeuss9T6NddcY64zZ84ctW49G/jusy0tLWp98eLF5jrWPrPqo0ePNtuyWOcLnzgBAAAAQAATJwAAAAAIYOIEAAAAAAFMnAAAAAAggIkTAAAAAAR4U/Vi02l8rJSVlNSw2AQuX/++dWL53ktsSl3OpJ2yUp5ySum/N6YT5RwfvrZi0xVT0pFSkvBS+o8dazfeeKO5bN68eWp906ZNav2ZZ54x23r22WfV+umnn67WhwwZYrZlyXm++NRzgqUmZR+UcY3++te/bq6Tkp5lJUtZ51rK9banr90pzxqWlGeg2OeT3pheaSkrjTD2Wmddz0Xs9LqOjg5zHSt11ep/y5YtZlvWOn366I/vl156qdnW8OHD1fqJJ56o1q3kPBGRAQMGqHUrPW/z5s1mW6+//rq5zNLa2qrWreTCnPjECQAAAAACmDgBAAAAQAATJwAAAAAIYOIEAAAAAAFMnAAAAAAggIkTAAAAAAR448hzRgGnRJTGxnfGxhCHNDQ0ZGsrZ3Rrzhjq2D5yxkCn9N9b5YzwTnl9zrj+nOeB1X9KHynjYMqUKWq9ra1Nrfsiaq3I1ebmZrV+7rnnmm2lXJvqNSq8DLUa1T9mzBizrR07dqj1pqYmc50JEyao9UGDBpnrxEq5D/385z9X62vWrFHr27ZtM9tqbGxU6/3791frn/70p822LCnR6r0twj9Fznj7lHvAgQMH1Pprr71mrmNd0/fv32+us3btWrVunYPt7e1mW6NGjVLrO3fuVOvW+S8ictNNN5nLNL7jZcWOb9y4Ua0vXLjQbMsaz76xcdJJJ6l1K3LdJ3Zs8okTAAAAAAQwcQIAAACAACZOAAAAABDAxAkAAAAAApg4AQAAAECAN1UvJfHNkpIokzM9Luc25+wjZ1tlpPOkbFdKMltK/70xtSjntudM6MuZjiQSn8aZM90xZb/07dtXrZ9yyilmW4888ohaf+mll9T66NGjzbZOOOEEtZ77GmjJOaZ7m5RrlMVKdRSxkxV9qVqx/aeMWcsdd9xhLtuwYYNa/9a3vqXWfYlm06ZNU+sXXnihWl+0aJHZ1hlnnGEus+S8D+VMO611OZ83d+/erdYfe+wxtb5ixQqzLav/YcOGmetY542VamndT0Tij/WsWbPMZdZ7+d3vfqfWTzvtNLOtlStXqvUFCxaodSuFT8ROwps7d665zsiRI9W6dV3MeS3rfaMPAAAAADJj4gQAAAAAAUycAAAAACCAiRMAAAAABDBxAgAAAIAAb6qepazEt9h+YpMxQn34UkA0vvSTnEk7OdP+YhNbfNvb0ymM9ZyelyJ2H6Uk0Vl9pJzrKedBypjuyXRJK+lHRGTQoEFqvbOzU60vW7bMbMtK1fOJHZ8pCX21PgZj0xt961hjICWlcceOHWp93bp1ZluWd73rXeayIUOGRLXley9WSt7TTz+t1h988EGzrb1796r18847T62fc845Zls7d+5U64MHD1brU6dONdtKuafFjoGcaae1ImVsWMus+pYtW8y2XnzxRbVuJZha54yIfd74zkHrPtDc3GyuY7HOgZR747/8y7+odeu9LFmyxGzLWmZtV1tbm9mWlQQ4atQoc52c97NYfOIEAAAAAAFMnAAAAAAggIkTAAAAAAQwcQIAAACAACZOAAAAABDAxAkAAAAAArxx5CnxwZacUdX79+9X6z/96U/NdRoaGtS6L4aysbFRrcfGZuYWG6uYErWbEoFprVNG5LpvnXqNdBXJG/+eM0I3t9hY/JT3kvN6Zhk6dKi57IILLlDrjzzyiFpfv3692dbGjRvVui8O3boGlrFfaoV1nvX0+bR69Wq1bp0bIiJdXV1qvaWlxVzHuqdZ7+X2228321qxYoVat+7DvnvHRz7yEbV+3XXXqfX29nazreXLl6t1a3utcSGS9xgfTbH/Oe/fmzZtUusPPfSQ2ZZ17Vy1alX0dk2ZMkWtHzhwwFyntbVVradcf2J9+ctfNpdZ+8V6Du7Xr5/ZlvV1PePHj1frc+fONduy+ol9LhCx92XO53M+cQIAAACAACZOAAAAABDAxAkAAAAAApg4AQAAAEAAEycAAAAACPCm6llypqTlTF/ZsWOHuY6VxOdLRrHe549//GO1biWpiIj8/ve/V+vHH3+8Wr/mmmvMtqxtfvHFF9X6k08+abY1duxYtf5Hf/RHaj3l2OdM4ktJIKoHZSQu+faPtSwlBSi2LZH4dEef2LZynlO+c33YsGFq/cILL1Tr99xzj9nWY489ptY/8IEPmOs0Nzeby2LVeqpXGVKuN9Z+s47nsmXLzLb69u2r1gcNGmSuM2DAALV+9913q/VXX33VbOuBBx5Q69Y4v+WWW8y2br75ZrU+ZMgQtW4leonYyYFNTU1qPeVczpl2mtJWrUvZ7jfffFOtL1y4UK0//vjjZlvWs1BHR4dav+KKK8y2rARTX7JjzsTb2OvMnj17zGXWPps4caJanzZtmtmWlZ535plnqnXf83HuMajJ+XzIJ04AAAAAEMDECQAAAAACmDgBAAAAQAATJwAAAAAIYOIEAAAAAAHeVL2UdK7YtlJYaULz5s0z19m9e7dat9KMROz0us2bN6v1H/3oR2ZbCxYsUOtWopZvfz3//PNq3UqlsdKEREQ+9rGPmctit8uSMzHG9/p6TSASyZuEl9KWlYSVknZnLfMlzqWk9+Vqq6yELGsda3z6Ugj37dun1rds2WKuM2bMGLWeM7nQt821wHcOWnKem5auri61vnz5cnOdM844Q61biagiIqNGjVLrt912m1r33dOshD4rEXbWrFlmW4MHD1br1rlppeSK2PvMOo6+8z/lfE65NpTRVplSxpn1/GI9O/3mN78x29q6datav/zyy9X6zJkzzbauvfZatZ41pS0hcdZiXUtE7LTBxYsXq/VVq1aZbc2fP1+tW6meDQ0NZls576dlzE9q+y4HAAAAADWAiRMAAAAABDBxAgAAAIAAJk4AAAAAEMDECQAAAAACmDgBAAAAQIA3jtzii06MjQJMiRu0+p88eXJU3yIiS5YsMZd1dnaqdSsKePXq1WZb1jovvfSSWv/pT39qtjVp0iS1/r73vU+tt7e3m23t2LFDrY8ePVqtlxVFX68xrLmVFXlq9WNF8lt1X1v9+/ePXifl9TnbsuSMXE9hjduFCxea6/hiqjW5Y5prQWzsvk/O69revXvV+oYNG8y2rBh739dPWPebX/3qV9FtWV//Yd1vLr74YrMtS8rXIVgOHjwYvU4Z0eJl9V+mlOtjd3e3Wreisq1roIjItGnT1Ppll12m1j/0oQ+ZbaXEXscen5z3jQkTJpjLzj77bLVuxfsfd9xxZltWfHxLS4taT7nG+uT8OhbiyAEAAAAgMyZOAAAAABDAxAkAAAAAApg4AQAAAEAAEycAAAAACPCm6uVMgckpZwrWySefbK7z0EMPqfUZM2ao9c9+9rNmW6tWrVLrVjqZLyFwyJAhan3dunVqvbW11Wxr6NChUdvV2NhotpXCSowpK7WoVqSkJ6UkF1nuvfdetW6dt74EyalTp6r1jRs3musMHjxYrY8fP16tX3nllWZbljLOD9/xso7Lnj17ol7v40uaspKTmpub1XqtJ3elSElpy5lsFnsOWMdMRGTZsmVq/Rvf+Ia5ztNPP63WrSSsT33qU2ZbEydOVOvXX3+9Wk/Zxymvt5IAt23bFlUXsdNlfWPTep9lpH3VOl9CnJXg9olPfEKt++4n/fr1U+tW4lzOc9O3Ts5rhrUvreQ8EZGGhga1bo0B3/bOnz/fXKZJGTM+0Ul4nnMvdh/ziRMAAAAABDBxAgAAAIAAJk4AAAAAEMDECQAAAAACmDgBAAAAQIA3VS+n2KSZnH34jBgxwlxmJW38+Mc/Vuu+hL4vfvGLUdvley/79u1T63v37lXrVmqWb1lKolcZiTFlbVfZcr5fi28/WCl11113nVq3UvhERMaMGaPW33jjDXOd008/Xa3feuutan3Xrl1mW3379lXrViJkzrQ93/HasGGDWn/mmWfUui9RrU8f/dJtpUaJ2GljOd9/rY+1nGmdOfebddxOPPFEc53169er9Z07d5rrDBw4UK1b4/+9732v2da5555rLjvSrHNZRKS9vV2tP/bYY2p98+bNZltWql7OFEZf2le9JsWmpMoNGDBArd90001ZtknE3i7fdds6PjnTEHOm51qpz75lKccr9tkkZcykJOHF9hHqR3191KsBAAAA4CjExAkAAAAAApg4AQAAAEAAEycAAAAACGDiBAAAAAABTJwAAAAAIMAbR27FB6bEMKbEapYRkzt48ODodTo7O9X61q1bzbZSIi0tVtyyVU+R8zim9GNFTdZ63HGqlDhQS851Zs+erdaPPfZYs61ly5ap9e3bt5vrdHR0qPUXX3xRrVvR3iJ2rK0VYeyL658yZYpat6L/ly5darZlvX8rdty3XZdeeqlaHzVqlLlO7FdC5IzbrRUpsbex17yUtqyYYN9XXEycOFGtT5o0yVzn4osvVutz5sxR61a0t0/KPSL26xh8+9h3bdKsXLnSXDZ16lS13tDQYK4Te6/37a+c94VakBJJnev1Inmfw3J+tUFsHLav/5TzKaX/WDkj10XyjrNYfOIEAAAAAAFMnAAAAAAggIkTAAAAAAQwcQIAAACAACZOAAAAABDgTdUrI9UuZZ2cSTMpqR1W0pAv1a63pMTlTMURsfdxzsSYetjHKdteRoLlNddcE71dVlvf/OY3zXUmT57s2br/6LXXXjOX7dixI6ot33tZs2aNWo89b0XstD8rCe8973mP2VZra6u5zBJ7XqSce7UuNr1NJH6cpbRlrTNt2jSzrUGDBqn1888/31xn5MiRat1KiUtJVkxJLotNz/O1ZaVnDhw4UK2/+eabZlvd3d1R2yVSXiJtLUtJScv5vBebrJb7uSa2n5wJprmfgy1lpEfmlHKMzWtcli0CAAAAgF6MiRMAAAAABDBxAgAAAIAAJk4AAAAAEMDECQAAAAACmDgBAAAAQIA3jjwl1jBn5G3OSEcrOtEXK9rc3KzWhwwZotZ37twZ3X8ZUZc+KTGUljLOFx/fsax1Oc8PS0pbOeNTP/nJT5rr7N+/P2q7rAhvEZFXXnlFrR88eDCq7mO9R+uaISJyxhlnqHXreuL7eoMU9RohnlPK9c4611OizS2bN29W675jdtppp6n14cOHm+tYseM5z42ckc+xMeUidhz5vHnz1HpXV5fZVp8++iNSzv3ley/1GmGeM5K9jPt67qjsnFHdscr6GqHYfnJGrqf07+sj9uuC6vdJEwAAAABKwsQJAAAAAAKYOAEAAABAABMnAAAAAAhg4gQAAAAAAS6QTKEuzJkEl1NKAohvu37zm9+o9RUrVqj1GTNmmG3Nnj1brcemefjkTEBL2V85k+Gs7fIlVnnSd2o+TqwwdlLuczqXnGmYoWUa33kQm4LmkzM5yOq/jHGbIjFNqqbH2qFDh6LHmSXneWal6nV2dprrtLW1qXVfGmPOFFVLzrTBnAm6OfvPeV1Oaas3jrOevG+lXGt7OnGujPS8nEl0PrWaXGiNMz5xAgAAAIAAJk4AAAAAEMDECQAAAAACmDgBAAAAQAATJwAAAAAISErV86XTlJEAkpLeljMJsLu7W603NjZG91FGak+KspJRYs+XlMQq6aWpegl9JC3T+I5DSnpUbEJPWSlEOfe/pYxxI1JO6qXU+Fiz0r5yns8p98ecCVW+95KzH0vO9LicaV8p53/KMY5t62hKr8wp53me+94Q+/xSp+dgtrZyXktTxKZX8okTAAAAAAQwcQIAAACAACZOAAAAABDAxAkAAAAAApg4AQAAAEAAEycAAAAACEiKI6/V2OucEaW+fsqIbswZ3drT8fG+9xLbf2+MbhWx41tzngdlRXjH9pG7n5T+yxDbf0p8fEpEryVlf7me3skBVux/zrFR1i4oY8z6lBEhXsbzQcp25dQbx1kZ9zOfMu5bPX0/LUPiuanWc8an517Hc+4RRw4AAAAAKZg4AQAAAEAAEycAAAAACGDiBAAAAAABTJwAAAAAICCUqgcAAAAARz0+cQIAAACAACZOAAAAABDAxAkAAAAAApg4AQAAAEAAEycAAAAACGDiBAAAAAABTJwAAAAAIICJEwAAAAAEMHECAAAAgAAmTgAAAAAQwMQJAAAAAAKYOAEAAABAABMnAAAAAAhg4gQAAAAAAUycAAAAACCAiVMi59x/dc79xjm32jnX5Zzb7px7wTl3m3Nu/BHob6Zz7mvOuRerfe13zm1wzj3onLvZOTcwd59ATytjnDnnPu+cK5xz93pe893qa/4mR59ArXHO/Ylz7rfOuR3OuT3OuZedc190zvXL1H5bdQytCrxuQfV1c3P0C9Q659wY59zXnXNrnHP7qj/vdM6NydhH4ZwrcrV3NHNFwX5M4ZzbLSIvVv+3SUSaRWSWiLxbRHaKyPuKovj/M/TTR0T+TkRuEZFCRB4XkWdEZJeIjKr21y4inUVRDD/c/oBaUsY4c841iMhjInKOiPynoii++Y7lHxORH4jI70XknKIoug+nP6DWOOe+KSKfEJHNIvJzEdkhInNE5FQReVpE5hVFsesw+2gTkZUi8kZRFG2e1y2o9j2vKIoFh9MnUOuq4+IJERkjIveLyAsiMkNELhGR9VK556zK0E8hIlIUhTvcto52fXp6A+rY0KIo9r6z6Jy7QUTuEJH/LpWL/+H6RxG5SUReFZEri6J4Qelznoh8JUNfQK054uOsKIqDzrk/FpHnReQrzrnHiqJYUe1noojcLiJdIvJxJk3obZxzH5DKpGmViJxZFMWb1foxIvJ1EfmUiPw/IvLZHtpEoDf7mlQmTZ8riuK2t4rOuc9J5R/N/1kqkyjUCH5VL5H2MFf1w+rPEw63D+fcWVKZNG2Tyr+s/4dJU3VbHhWR0w+3P6DWlDHOqv2sFJGbRaS/iHzfOden+uD4XREZJCK3FEWxLEdfQI35SPXnbW9NmkREiqI4JCJ/Uf3P65xzLaVvGdCLOecmichFIvKGiHzpHYu/JCKrReTi6qdSqBFMnPL7YPXn8xnaurH6886iKNb6XlgUxb4M/WwC36YAACAASURBVAH1Iuc4ExGRoii+IyI/EpGzROQLIvKXUvmVwPuKorgrVz9AjRld/fn6OxcURbFZKr8SO0Aq4wJAPudXf/66+g8V/64oioMi8ut3vA41gF/VO0zOuVtEZLBUbiwzpXKCrxaR/z1D8++u/vxNhraAunWEx9nb3SiVv3X6C6n8TWGHiFyXuQ+glmyu/pz0zgXOueFSGXMiIlNEZEGG/gY7577oWd6WoQ+gHpxY/fmasfy1d7wONYCJ0+G7RUQmvu2/F0nlbyGWZ2h7bPWn99Mm4ChwJMfZvyuKYptz7r+IyJ3V0s1FUXTm7AOoMf8mIh8Tkc86535U/ZRJnHNORP76ba8bkqm/QVL5RBc42g2q/txmLH+rPriEbcEfiF/VO0xFUbRVU0pGiMiFIuJE5Bnn3GU9u2VA71HWOHPO9ReR//y20pU52wdq0I9E5BdS+cTpFefcXc65f5BKmt4npBJMJCJySF892htFUTjrfyKyMFM/AJAdE6dMiqLYXBTFgyJygYjsFpHvOOcGBVYL6aj+HHeY7QC9whEaZ2/3FRE5Xip/mPuMiFzlnLs6Y/tATan+bcWHRORWqfx2w8el8uupnVJJrFxRfenGHtlAoPfaXv1pfaL0Vt36RAo9gIlTZkVRbBORJ6Vywp96mM39tvrzfYfZDtCrZB5nIiLinPuwiHxSRJ4Vkf9TRP5YRPaIyD8fiS+1BmpFURTdRVF8qSiKU4uiaCmKYkBRFBdUvyNtZvVli3pyG4FeaGn1Z7uxvP0dr0MNYOJ0ZLz1CdGBw2zn69Wf1znnjvW90DnX9zD7AupNrnEm1fF1p1QmSh8vimJ/URSvSOXX9gaLyLerf/MBHDWcc+eLyHgReaYoipd7enuAXuaR6s8Lql9/8e+q/31B9T8fLXWr4MXEKYFzrt369SDn3Gek8p1KG0Xk94fTT1EUT0nly8+GiMiDzrmTjD7fLfxrIHqZssZZdUL0bREZKiJ/Xp0wveWfReRBEZknlV9lAnodbZw55yaLyF1S+dumz5W+UUAvV/3+wAekEnz0zoTYW0Vkgog8UH0dagSpemkuFpH/5px7QirfffGmVP5o/WwROUlEuqT6r9YZ+rpFKv+ifouILHHOPS6Vv73oqvZ5nohMrW4D0JuUNc4+KyLvkcoN6p/fvqAoisI590kReUFE/sY592vri6iBOvZr59w+qZzn26XyxdKXikijiNxY/ZJ1APl9RkSeEJG/d87Nk8oYnCEil4jIhupy1BBXFEVPb0Pdcc5NF5EbRORcqfwaw2Cp/IrPChF5SES+UhTFmsx9nlztc061zxYR2SIiS0TkZyLyraIoduXsE+hJZYyz6rhaJJWHxRlFUah/AO+cu0JEfiKVm9oZfOE0ehPn3OdE5I9EZLKI9JfKJ7mPisjf5fqHAudcm4islEqqXpvndQukcp+bVxTFghx9A7XMOTdWRL4olcnSCKn8I+H9IvLFoig6PKvG9FGIiFSTK3EYmDgBAAAAQAB/4wQAAAAAAUycAAAAACCAiRMAAAAABDBxAgAAAIAAJk4AAAAAEOD9HqfCiNxLSeKrfMfkH14XETl06FDUOr7t8vVThthtTtne2P2VW8pxiT2XfO/FauuYY46p+fjNQ4cOqRufcuxyJmWW1X/K9cFijYPYvnPLOdbLSEM95hj739WsfVzrY62Mcebbb7HHLedY8i2LHTO+tlL6KOP+aMm9j1POi9i2XE8/0ASkPDvGngMp4yzlfCrjeKask/OalfLsFvv+c773FDnvZ3ziBAAAAAABTJwAAAAAIICJEwAAAAAEMHECAAAAgABvOEQK6w+wrD/y8v3BqNVWWSEIZQQXxNZ9y3L+sWLsvhfJG9qR8484e6ucxy5lnZxy/jFvznPK80ejUX3k3q6cf8ybcg2q8b9Nj5YzpChlnJURNOGTEs4S23/KeykjzCLlj8ZzjjOfMsIxjoSc1zRLznGW+9mhjPtmGfu4p8Pbcl5/ch4TPnECAAAAgAAmTgAAAAAQwMQJAAAAAAKYOAEAAABAABMnAAAAAAhg4gQAAAAAAS4Q6Red0Rgb35uirEjJlFhFi7VfduzYodY3bdpktvXMM8+o9ZEjR6r1rVu3mm195CMfMZflkjPWOCUGVkRqO7tVRIqEkyr2/KzHWPicYzBnW2XEt6dEDqeskzMm+ZhjjqnpsZZznOWMxE6R0lbsNuc8n1Jiv9etW6fWu7q6zLaamprU+tixY9V63759zbYsKTHNOdX6ODt06FC2m0BPR6/n7D/lq3Ry3rdy9mEt6+7uVut9+tjffpQzwj9nHLoYz4584gQAAAAAAUycAAAAACCAiRMAAAAABDBxAgAAAIAAJk4AAAAAEGDHXEg5iRa1nPRlrWMlo6xZs8Zs66677lLrP/rRj9S6Lwlv9+7dav20005T65deeqnZVux+yZ1wk3Isy2irbCnJXbHvNyXVKmcaZoqePj9ir1u+/RXbVs7tSuknd0ppb2Ptt5TEtTLaSukn5f6c0tbdd9+t1hctWqTWp0yZYrY1ePBgtX7OOeeo9fb2drOtnOmZOZPDjiY5E99yHs+U63NK/7HjLKWte+65R623traabVkpzlYfvnE2aNAgc5mljNRr63jxiRMAAAAABDBxAgAAAIAAJk4AAAAAEMDECQAAAAACmDgBAAAAQIA3Vc+SkvSTM50rJYEoZ2rZN7/5TbX+ta99zWzr1VdfVev9+/dX6xdffLHZ1ogRI6LqF1xwgdnWhg0b1Pro0aPNdSwpSWMpaTax/ddDql4Z6XEpCVkpyV379u1T63379o3u35IziSplv1jKStuylu3du9dcp7GxUa03NDSo9XoYN7HKuEaUlRSbc5tTxnnsue5r67rrrlPrjz/+uFrv6uoy22publbrK1euVOsTJ04022ppaTGXxcp5Xa5XOZNCffst9nkzZbt6emxafO/9W9/6llrftm2bWv/2t79ttrV//361ftVVV6l13zPlwIED1bp1bxLJe/2JxSdOAAAAABDAxAkAAAAAApg4AQAAAEAAEycAAAAACGDiBAAAAAABTJwAAAAAIMAbR54S+VtGrHBKtLkVXeiLNPzVr36l1j/96U+rdSsOXETk7rvvVuuXXnqpWm9tbTXb2rhxo1r/+c9/rtb79etntjVq1Ci1Hhub65MzcrysaPOy5YxJTjl2sZGvu3fvNtt65pln1PoTTzxhrmPFkX7mM58x17H0ZBRsSt/f/e531fp5551nrtPd3a3Wly9fbq5jjfUZM2ao9T59kr6toqalRL/HngNWHL+PFdWfM9o8tCxX/ymvt/oZMGCAWl+3bp3ZlhVhvHPnzujtsuT86pOyYq3LVEa8fxlfcZMq59dlWFLe/5tvvqnWt2zZotate4aIyEMPPaTWf/jDH6r1D37wg2Zb48aNU+u5r3+58IkTAAAAAAQwcQIAAACAACZOAAAAABDAxAkAAAAAApg4AQAAAEBAUmxSSjpOztSwnMlumzZtMtf52c9+ptanTZum1j//+c+bbVmJIo2NjWrdt18GDx5sLotlJbNY/acce5+c50U9S0n7ik2VSdmn+/fvV+tPPfWUuY41blauXGmuM2fOHLWekkSVso4ldp177rnHXGalYW7dulWtr1q1ymyrqalJrVspZCIi/fv3V+spaZT1Oj5znhtWEtWiRYui2zr//PPVunWcRdLGecp1Jmf/Fmudj370o2r9mmuuMdvavHmzWr/qqqvUunXtERFpaWlR674xE5sCl7OtWtHT9/XY51CfnM+oOe/z1nnjOzesfqx7k/WsK2In6y5ZskStW9dLkXL2cU61PfoAAAAAoAYwcQIAAACAACZOAAAAABDAxAkAAAAAApg4AQAAAECAN1UvJQXGEpsAlsLXlpWe98QTT5jrtLe3q/Vbb71VrZ9wwglmWw0NDeYyje+9bN++PWqdgwcPmm2tW7dOrR977LHR25Ui9rzI3X+tK+v9xh6Hzs5Os63m5ma13rdvX3OdnEk4sclFvn0cu12jRo0yl7322mtq/YUXXlDr/fr1M9uaMGFCdP8jRoxQ63366LcB336p17SvFNb9bsOGDWrdlx5p7Z9du3apdV9Koi9xzxKbKpgyNqw+UpLDFi5cqNZ91x8r7aurq0utW/s+t5SkuXpNrywjwTTlObSMVMmUflLSiq0+fvGLX5htnXfeeWrdSne2xoyIyKmnnqrWhw4dqtbfeOMNs62U609s2mDOsdT77nIAAAAAkBkTJwAAAAAIYOIEAAAAAAFMnAAAAAAggIkTAAAAAAQwcQIAAACAAG8ceU/HPVptWfHaW7ZsMduyYsetSFkRkUmTJqn1trY2te6LHI+NTvTt+/Xr16v1zZs3q/X77rvPbGv48OFq/frrr1frPR2PmhLbWQ9SIjRzRrlby6zt8p3rM2bMUOvnnnuuuc706dPVehlfY+CL0Lbe/759+9T6c889Z7b1wAMPqPVFixap9Xe/+91mW9b+es973mOuY0WV98YIcUvKV2x0d3er9TVr1kS3ZbHigH2R47H3FN86KWKjlX1j9v7771fr1thoaWmJ3i7rvu2L8E+Jj87ZVs7+a0HO/eaT8zk0ZZyVcX+y+jjjjDPMtt58882ovn3XH+s+bz1TWl+jI5L29RY5v1rBYrV19NwxAQAAACAREycAAAAACGDiBAAAAAABTJwAAAAAIICJEwAAAAAEeFP1LL5klNhEIV9qhpWCsWfPHrX++OOPm21t3bpVrfsSSKzUEF+ijyU2HefOO+802/qnf/ontX7eeeep9VNPPdVsK2d6Xs7UkhT1mjTkk5KEZ+2HlPQmK+1r9OjRZlvr1q1T6yeeeKK5jpVgmZKQkzPRaMeOHWp94cKFav22224z27ISjUaOHKnWL730UrOta6+9Vq2PGDHCXCd2fPTGBMuU7bbW6erqUuu+xEmrrY0bN6r1gQMHmm1Z99qUlMQy0uN82zVv3jy1/vLLL6v1p556ymzLSpxsb29X677j1dPqNVUvZ4JgyphNSba0lPFck3I8rbRPqy4i0tnZGdWHb8wOHjxYrVv3s7Vr10b3k3Icy3h25RMnAAAAAAhg4gQAAAAAAUycAAAAACCAiRMAAAAABDBxAgAAAIAAJk4AAAAAEOCNI0+JCIyND06JFX7ppZfUekdHh9mWFZE4depUc53m5uao7coZ0bt58+boZePHj1frN954o9lW7DFOiY9PiSBNidS1trnWo1tFen4brf779NEvEccff7zZlhVHvmHDBnOdUaNGqXUr8jRlf1nnhxUFLSLyve99T63/4Ac/UOu+uNexY8eq9fnz56v1P/3TPzXbGjRokLksl5xR+L1Ra2urWj948KC5jrV/9u3bp9Z991orRtu3Tux11XcOxLbl2y7rXmvdt5uamsy2LrnkErWecrxSxH4dRM7o7FqX8uyY8znU0tNfpZJzv2zatMls64033lDr1vvv37+/2Zb1VT79+vVT68OHDzfbSnkO7MlxwydOAAAAABDAxAkAAAAAApg4AQAAAEAAEycAAAAACGDiBAAAAAAB3lS9nEkjKUl03d3dan3p0qVq3UoAExGZOHGiWrfSfETi30vOxLk5c+aYbS1atEitW2l79957r9nWVVddZS4rw9GUwuWTM5Ex9rz1sc5pK+1ORGTcuHFqfdWqVeY6VqqelR7nO2927dql1p977jm1ftddd5ltPfroo2p92LBhav2aa64x27KuQVYK2JAhQ8y2cp4v8J9PVoKblWK6cuVKsy0rCcpax5de2dLSYi6L7d/i2y85U62s68yECRPUupVQKSIyYMAAtW5dF3wpZFYSX04pqbP1yvdeazV1MOczSsrxtFIfrZTaJUuWmG1t2bIlart896ARI0aodWt7U64lvv3Vk+cLnzgBAAAAQAATJwAAAAAIYOIEAAAAAAFMnAAAAAAggIkTAAAAAAR4U/VS0rliE+d8bVkJILt371bro0ePNtuaNm2aWm9sbDTXiU3CS9kvVlvnnHOO2dbnP/95tf7II4+odV9qUCxfYknKfrHOC6ufoy2Fz5fUaEnZd7HHzjdurMS5tWvXmutYiZBtbW1qfd++fWZbX/jCF9T697//fbVupQCJiMydO1etn3766Wp94MCBZlvz589X6ylJSynJnrFpQynnXq2LvQ77llnnuZUEKSKyfft2tb5161a1vm3bNrMtKxE2Z7przoSqlPS41157Ta2ff/75Zlt9+/ZV69Y4P3DggNlWzoTSlHtXb7vf5XxGynndzJ32F3vcfGPWutdZ6cq+5z2rH6s+c+ZMsy3r+pPzeKXI+Rxqvj7q1QAAAABwFGLiBAAAAAABTJwAAAAAIICJEwAAAAAEMHECAAAAgAAmTgAAAAAQ4I0jzxkJbUUB+qJAX3/9dbVuxYpOmjTJbMuKTvTpyajshoYGc9mMGTPU+uTJk9X6ww8/bLYVGxFZVjxq7L73rVPPUiJPU6L/c8YOW3zH5+WXX1br1vVh2bJlZls/+clP1Lr1/j/0oQ+ZbZ122mlq/TOf+YxazxkH7pMz8jXlfKlXOa8RVux4nz72bdU6B6zIYSu+XERk5MiRaj0ljjzneZNynsdeyyZMmGC2ZfXve9aIbSvlPMo5nuo1pjzl+pgzdjzl9SkR5rF8/Xd2dqp16ys8UrZr/PjxUXVfP2VF9ccel5SvQzCvS4FtAwAAAICjHhMnAAAAAAhg4gQAAAAAAUycAAAAACCAiRMAAAAABHhT9cpIDfEl3axZs0att7S0qPUxY8aYbaUkR+VMZolNIPHteysh0KpfeeWVZls5U/VS0gbLSC2q1wQikbQkmJTXx+4jX1tWqpgv2dJKyrTS87Zu3Wq21draqtatJK4pU6aYbZ188slqPSVRKHYfpyRIHm2pk7FyXm8aGxvVupVuKmKn5Fn9NzU1mW3lTLWydHd3Ry+ztsuXELhq1Sq1/m//9m9q/ZxzzjHbstIOLU888YS5zEr2HThwoLmOlXbY1tam1q3z6GgTm4aW89kt5T6bcq236tb9T0Rk27Ztat1K4vSxrn/Dhw9X677rT6zcz2Gx50XO+x93UgAAAAAIYOIEAAAAAAFMnAAAAAAggIkTAAAAAAQwcQIAAACAAG+qXhl8ySBWak+/fv3Uev/+/c22UpI2UtJUYtuylJFomLuflKSvnMlQ9Zyel5L6mLK/Lda+S0khsxJ6Ro8eba5jpWpZqZvjxo0z27rrrrvUupWE5UvjtBKyLLlTJ3OKPZYp516ty5muaiVhdXR0mG1Z97QhQ4aodWsspbLut1binZUqJ+J/n5pdu3aZy55//nm1vnjxYrU+a9Yssy0rVc86jnv37jXbWrlypVpvaGgw17GuJ+PHj1frVgqpSH3f0zQ5n51ypkf65DwG+/fvV+u+lNhnn31WrVv3Rt/5ZCXOnnDCCdFtWVL2VxnPgSnXeOt+UZ93PwAAAAAoERMnAAAAAAhg4gQAAAAAAUycAAAAACCAiRMAAAAABDBxAgAAAIAAb9ZgShRgbERyV1eX2daePXvU+uDBg6O3q4yIwpTY7ZxSonYt1jq+95GyTqzeFs/6lpRjFBs7nhKVbW2XNTZFRF5++WW1vmbNGnMdK9rZikP1RYifeeaZar25uVmtp1wDLL59XMZXDKSMz5T4+nqV8l5jr/dWTLCIfXysuu/rOiwbNmwwly1dulStd3Z2qvXdu3ebbVlj1orq3rlzp9nWsmXL1LoVO25FjovE72OfYcOGqfXGxkZznQkTJqh16zzqjfe0lPcUe61N+UqEnNHmKdfajRs3qnUr9l5EZNOmTWrduv4MHTrUbKu9vV2tW+d5irIixC05zwuzj+geAAAAAOAow8QJAAAAAAKYOAEAAABAABMnAAAAAAhg4gQAAAAAAd5UvRSxqUVbtmwxl33/+99X6/Pnz1frVgKXT840s1pNj/O9x9h+cqYQhtqLfX09pxNZ2+4bTynpjhZrHSsF6KmnnjLbevPNN6O3a8CAAVHrrFu3zmxr69atan3UqFFqvax0ppRrTaycffj2i3VcUvZlLUh5rymJj9bY2LFjh1p/+OGHzbasa4bVlojI/v371XrKeWOl5zU1Nan1fv36mW11dHSo9YsuukitWwmZIvHvpX///uYyq39fqp51XuR8Pqn1e11PPwvE7reU/em7n1nPtb/97W+jXu9j7cuTTz7ZXOe4445T63379lXrZT3T5nymzp2QqKnPuxwAAAAAlIiJEwAAAAAEMHECAAAAgAAmTgAAAAAQwMQJAAAAAAKSUvV8CRQ5k1FWrFih1g8cOBC9XdaynAkcOROtykjgEolPE0k59r70mdh9mdJ/rScQiaQd79h1Dh48aC7btGmTWr/33nvV+uuvv262ZaX6HH/88eY67e3tan3btm1qfeHChWZbVhLgkCFD1LovocuSkh4Xm5yY+1yPXae3JlhqUpIorVS5SZMmmetY9zTrPPelbVnb7DtuVrLb8OHD1bqVtiUi0tbWptZbWlrUemtra/R2WQmFvjG7evVqtW49N/hY6Xkp14yy7um1ICXxNfY65NufORNn9+zZo9at8S8isnbtWrVu3Zu6u7vNtqxzcOjQoWr92GOPNduyxma9Jb6mtJcyP7DwiRMAAAAABDBxAgAAAIAAJk4AAAAAEMDECQAAAAACmDgBAAAAQAATJwAAAAAI8MaRp0TOWjGQVnTg+PHjzbas+NI77rhDrc+ZM8dsa+TIkWo9Z4R5yv4qI0K7rPh4a5kvujn2faZEkNaDMs6D5cuXm8vmz5+v1p999lm1PnDgQLOtc889V62fdNJJ5jr9+vVT69YxtaLFRUQWL16s1q3I15kzZ5ptNTU1qfWyIsRj9fRXItR6TLm1fb7tto61dW4OHjzYbMu6p+3cuVOt+75CwLqnTZ8+3Vxn2LBhan3EiBHmOpaUfWm58cYb1bp1Du7YscNsa/fu3Wq9o6NDre/atcts6/HHH1frxx13nLnOuHHj1LoVK13rYyZFSux3T36diK8PK5J/8+bN5jrr169X6ynPSFbsufX1HgMG/M/27uZlq2qNA/Cq3kqNk2VqZOpEzT6ghBBSwWgiBk4axOugkQ50JiLNatRf0CD6I4JmUoRITmtiEmIkZn4mqCmG5ec5kzNc91qsxTr7fepc1/De77738+y91t578cLv+Vdzr5E/STFFr1q/EX9fOv7f900TAABgIhZOAAAAFRZOAAAAFRZOAAAAFRZOAAAAFcVUvZ4EkNZeUdJMSnFqyLFjx7L1X3/9NewVpQlFKSMptaeDlM7LQiZq9XyuSJRMllJKN27cyNajZLKUyulsOSPTr2bJyFSZO3fuZOtRQlRKcXpedE7feeedsFeUeHX06NFwn/n5+Wz9yJEj2frcXHzrunbtWrb+7bffZuuLFi0Ke61duzZbj5KLelIfe8ZtNC560qxaj/F3Fl2fqdI6W69P6fn0yiuvZOsbN24M9wlTogY+03tStVqfQ1EKZ0opvfTSS9n6lStXsvXSNfn555+z9Sg1LaWUdu7cma1H7yALncT5v7DQn691np04cSLctm7dumy99Dw9e/Zs0/FL52vVqlXZepQG2/PcmGLOlvSM8ylS/SKz/0YJAACwwCycAAAAKiycAAAAKiycAAAAKiycAAAAKoqpepGe5KYo6aOUaPXBBx9k61999VW2/sknn4S9Pv/882w9SixJKf6eIxNIRiaARPXPPvss7LV169ZsPUomOn/+fNjrl19+ydY3b94c7hOZIjVsloxMT7p37162fuvWrXCfp556KluP5seaNWvCXlHy4vvvvx/uE13XS5cuZeultLGLFy9m6xcuXMjWr169GvbasWNHtv7qq69m6ytXrgx7RXrGek8KY2uvkfezWdHzXSNTJPSVUmejtNKeVK2ev2/9/j3nODp+Kd11+fLl2Xp0j/vjjz/CXtFx/vrrr3CfKNUzStUbmU4260bOjZ5e0T5REmxK8fX8/fffw31a0ytL6cIbNmzI1qMU49WrV4e9WsfaFGmbpX2mui9HwuvY3AkAAOD/jIUTAABAhYUTAABAhYUTAABAhYUTAABAhYUTAABARVcceSmisDXyrxQrvH379mw9ikL+5ptvwl5ffPFFtl6KSI6ihefm8qet57xEkcOXL18Oe33//ffZ+o8//tj8uaII8XPnzmXrN2/eDHtt2bIlW3/mmWfCfSLR+Sp9l39iVHnpOz148CBbP3XqVPNx9u/fn61v2rQpWy+Ng/n5+Wy9Jz5237592XopYj+KCj569Gi2/uWXX4a9jhw5kq1H3/HQoUNhr8hCx4H3xLfOeux4pOdct/4sRSn2P9oW9VqyZEnYK7qv9sT+jozE79Haq/TeEG2Lot1Lx47uS6XrsmzZsnBb6/H/aVHlIyOpe3p9+umn2frJkyfDXmvXrs3WSxHmd+/ezdZv376drZd+FieyZ8+e5n1a30NL53jk/I/m2ch3up53R3HkAAAAnSycAAAAKiycAAAAKiycAAAAKiycAAAAKh6ppLZkN45MW+pJ7fnpp5+y9V27doW9rl69mq2//fbb4T47duzI1p977rls/d69e83Hj5IAjx8/HvaKEluefvrpbP3gwYNhr9WrV2fr69evz9ZfeOGFsNfy5cuz9anSuQq9Zj4C7OHDh9mTVDo/0Xi7cOFCth6Nm5RSWrx4cba+YsWKbH3RokVhr57kspGpOlFy0pkzZ7L18+fPNx9j48aN2Xppfhw4cKDpGD3zpieFKzr3Pal+jz766EzPtZ551npOr127Fm77+uuvs/UbN25k66W0rXfffTdbL83N1ms9cjyVtM7z0vWK0kb//PPPbP3ixYvNvZ5//vlwn9ZUvZLoez4y+7GWze+OrUm6Pe+O0fvWhx9+GPY6e/Zsth4l5KUUj5uXX345W9+2bVvY66233srW9+7dkKcGZAAAA9FJREFUm62PHBojU+167rE9SdUjn43RPPMfJwAAgAoLJwAAgAoLJwAAgAoLJwAAgAoLJwAAgAoLJwAAgIpiHPm/O/L7WqMQe+Ipo4jE06dPh70++uijbP27774L94ninnsiGq9fv56t379/P1t/8cUXw17bt2/P1vft25etlyJto/jkKJ66J2p2qjjywjFmPbo1pYHR/63zJqW+6xrpif2MFOJ4m3udOnUqW49iilNK6dy5c9l6FEW7e/fu5s810sjxUhoT0Via9Tjy6Jk2Mnb7zp074T4//PBDtn7ixIls/fXXXw97vfHGG9n6k08+Ge4zRex4zzFax2ZPtHG0TxQdXdqnJya5x981jrwn9j/a1vO+FV2f6D3s448/DntFPyFQmuePP/54tj4/P5+tv/fee2GvzZs3h9tyJvqJl+Z53vM8KWl9NxBHDgAAMCELJwAAgAoLJwAAgAoLJwAAgAoLJwAAgIquVL2p0nFGpqxECSiXL18O9zl8+HC2fvz48Wy9lBry2muvZevPPvtstv7mm2+GvVauXJmtL126NFufm5sLez322GPhtpzOZJLmfXoUxthMJxCl1Jdg2ap0GqI5NTKhpsfIS9dzDxrZa4q0rakSLAvJSTM910YmxbbOmZTiBMfffvstWy8loj7xxBPhtkhrEtXItKspkvtKRn7HkugzjzzOrM+znlS9ViPvp1FKakrl1NVWS5YsydZLSZjRO9oUz6CF7jVVemXr88x/nAAAACosnAAAACosnAAAACosnAAAACosnAAAAComS9UbmabSk2bUI0rii45fOi9RakrP+RqZXBgZmYJYGWNNn6uUshJdl1lPIEqpb66NNDK9aqET5yZKtcrWe+6NPfeznhSi1u/fc9+Y9QTLaJ6NTJysHL95nylMkdI4VUJX6zgvzZmeY/T0az3OrD/TRj7Pet6RouOMvG9OdasbmQbZ+tya6l1/5LgY+c4iVQ8AAKCThRMAAECFhRMAAECFhRMAAECFhRMAAECFhRMAAEBFMY784cOHzdGtkZExrD0xhD0RxSPjDkfGPS9kHPnI757S2IjkwraZjm79r+yJ7RmfPX/fer1HR7G2xsSOPC8j7xsjx3pJZ1R4tj4y1jbN/lxrnmeR0THWOTOe7t5kqsjhKWKSp7ouhePP9MCI4sgHHyPcNjJ2u8fIZ1DreB4cx928z0LP85E/hyKOHAAAoJOFEwAAQIWFEwAAQIWFEwAAQIWFEwAAQEUxVQ8AAAD/cQIAAKiycAIAAKiwcAIAAKiwcAIAAKiwcAIAAKiwcAIAAKj4DwaM9XGn2KXjAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x1080 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "OZgAjcq9zMcz",
        "outputId": "a418888c-9650-4139-ca3b-d2e78e6be177"
      },
      "source": [
        "# test 이미지 확인\n",
        "\n",
        "plt.rcParams.update({'font.size': 17})\n",
        "fig, axs = plt.subplots(4, 4)\n",
        "fig.suptitle('image samples')\n",
        "fig.set_size_inches(15, 15)\n",
        "\n",
        "# train_pixels[train_pixels < 0.6] = 0\n",
        "\n",
        "for i in range(len(axs)):\n",
        "    for j in range(len(axs[i])):\n",
        "        img = test_pixels[i * 4 + j].reshape((28, 28))\n",
        "        # img[img < 150] = 0\n",
        "        # img[img >= 150] = 1\n",
        "        axs[i][j].imshow(img, cmap='Greys')\n",
        "        axs[i][j].axis('off')\n",
        "        axs[i][j].title.set_text(f'{test_letters_char[i * 4 + j]}')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x1080 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfjZnERIz3Ow",
        "outputId": "6146030b-ca22-4c2a-9579-5c0115926e3b"
      },
      "source": [
        "# 0번째는 n_samples(샘플의 수)\n",
        "\n",
        "input_shape_1 = train_letters.shape[1:]\n",
        "input_shape_2 = train_pixels.shape[1:]\n",
        "output_size = train_digits.shape[1]\n",
        "\n",
        "print(input_shape_1)\n",
        "print(input_shape_2)\n",
        "print(output_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(26,)\n",
            "(28, 28, 1)\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bytUZv97flO"
      },
      "source": [
        "### [tf.keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer?hl=ko)\n",
        "### [사용자 정의 층 구현](https://www.tensorflow.org/tutorials/customization/custom_layers?hl=ko)\n",
        "사용자 정의 층을 구현하는 가장 좋은 방법은 tf.keras.Layer 클래스를 상속하고 다음과 같이 구현하는 것입니다.\n",
        "\n",
        "- `__init__` 에서 층에 필요한 매개변수를 입력 받습니다.\n",
        "- `build`, 입력 텐서의 크기를 얻고 남은 초기화를 진행할 수 있습니다\n",
        "- `call`, 정방향 연산(forward computation)을 진행 할 수 있습니다.\n",
        "\n",
        "변수를 생성하기 위해 build가 호출되길 기다릴 필요가 없다는 것에 주목하세요. 또한 변수를 __init__에 생성할 수도 있습니다. 그러나 build에 변수를 생성하는 유리한 점은 층이 작동할 입력의 크기를 기준으로 나중에 변수를 만들 수 있다는 것입니다. 반면에, __init__에 변수를 생성하는 것은 변수 생성에 필요한 크기가 명시적으로 지정되어야 함을 의미합니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzah9-Ud0eII"
      },
      "source": [
        "# 픽셀을 가로, 세로로 민다.\n",
        "class RandomRollLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"Shift data\"\"\"\n",
        "\n",
        "    def __init__(self, roll_limit=0.1, u=0.5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.roll_limit = roll_limit\n",
        "        self.u = u\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.non_trainable_weights.append(self.roll_limit)\n",
        "        self.non_trainable_weights.append(self.u)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, pixels, training=None):\n",
        "        if training is None:\n",
        "            # 학습 단계를 나타내는 플래그 반환 : 0 = 테스트, 1 = 학습\n",
        "            training = K.learning_phase()\n",
        "\n",
        "        if not training:\n",
        "            return pixels\n",
        "\n",
        "        # 한 개의 [0, 1) 랜덤 값 반환\n",
        "        if tf.random.uniform(shape=[]) < self.u:\n",
        "            roll_limit = self.roll_limit * pixels.shape[1]\n",
        "            roll_limit = tf.cast(roll_limit, tf.int32)\n",
        "            roll = tf.random.uniform(shape=[], minval=-roll_limit, maxval=roll_limit, dtype=tf.int32)\n",
        "\n",
        "            pixels = tf.roll(pixels, shift=roll, axis=1)\n",
        "\n",
        "        if tf.random.uniform(shape=[]) < self.u:\n",
        "            roll_limit = self.roll_limit * pixels.shape[2]\n",
        "            roll_limit = tf.cast(roll_limit, tf.int32)\n",
        "            roll = tf.random.uniform(shape=[], minval=-roll_limit, maxval=roll_limit, dtype=tf.int32)\n",
        "\n",
        "            pixels = tf.roll(pixels, shift=roll, axis=2)\n",
        "            \n",
        "        return pixels\n",
        "\n",
        "    # Returns a dictionary containing the configuration used to initialize this layer. \n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'roll_limit': self.roll_limit,\n",
        "            'u': self.u,\n",
        "        }\n",
        "        # update 를 먼저 호출해야되지 않을까...?\n",
        "        config.update(super().get_config())\n",
        "\n",
        "        return config\n",
        "\n",
        "# 픽셀에 랜덤하게 특정값을 더해준다\n",
        "class RandomAddLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"Add data\"\"\"\n",
        "\n",
        "    def __init__(self, add_limit=0.1, u=0.5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.add_limit = add_limit\n",
        "        self.u = u\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.non_trainable_weights.append(self.add_limit)\n",
        "        self.non_trainable_weights.append(self.u)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, pixels, training=None):\n",
        "        if training is None:\n",
        "            training = K.learning_phase()\n",
        "\n",
        "        if not training:\n",
        "            return pixels\n",
        "\n",
        "        if tf.random.uniform(shape=[]) < self.u:\n",
        "            add = tf.random.uniform(shape=[], minval=-self.add_limit, maxval=self.add_limit, dtype=tf.float32)\n",
        "            pixels = pixels + add\n",
        "\n",
        "        return pixels\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'add_limit': self.add_limit,\n",
        "            'u': self.u,\n",
        "        }\n",
        "        config.update(super().get_config())\n",
        "\n",
        "        return config\n",
        "\n",
        "# 픽셀에 랜덤하게 값을 곱해준다\n",
        "class RandomMultipleLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"Multiple data\"\"\"\n",
        "\n",
        "    def __init__(self, multiple_limit=0.5, u=0.5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.multiple_limit = multiple_limit\n",
        "        self.u = u\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.non_trainable_weights.append(self.multiple_limit)\n",
        "        self.non_trainable_weights.append(self.u)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, pixels, training=None):\n",
        "        if training is None:\n",
        "            training = K.learning_phase()\n",
        "\n",
        "        if not training:\n",
        "            return pixels\n",
        "\n",
        "        if tf.random.uniform(shape=[]) < self.u:\n",
        "            multiple = tf.random.uniform(shape=[], minval=-self.multiple_limit, maxval=self.multiple_limit, dtype=tf.float32)\n",
        "            pixels = pixels * (1 + multiple)\n",
        "\n",
        "        return pixels\n",
        "    \n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'multiple_limit': self.multiple_limit,\n",
        "            'u': self.u,\n",
        "        }\n",
        "        config.update(super().get_config())\n",
        "\n",
        "        return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39XEDB1g0gOc"
      },
      "source": [
        "def thin_resnet_model(input_shape_1, input_shape_2, output_size=10, num_clusters=10):\n",
        "    input_1 = Input(shape=input_shape_1)  # letter\n",
        "    input_2 = Input(shape=input_shape_2)  # pixel\n",
        "\n",
        "    y = input_2\n",
        "    # 픽셀값 [-1, 1]범위로 rescaling\n",
        "    y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
        "    y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
        "    y = RandomRotation(factor=(-0.1, 0.1), fill_mode='constant')(y)\n",
        "\n",
        "    # y = RandomTranslation(0.1, 0.1)(y)\n",
        "    # y = RandomRotation(0.1)(y)\n",
        "    # y = RandomZoom(0.1)(y)\n",
        "\n",
        "    # CONV 1\n",
        "    y = Conv2D(64, (7, 7), padding='same')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = MaxPool2D((2, 2), strides=(2, 2))(y)\n",
        "\n",
        "    # CONV 2 - 1\n",
        "    y1 = Conv2D(48, (1, 1), padding='valid')(y)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(48, (3, 3), padding='same')(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(96, (1, 1), padding='valid')(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    \n",
        "    y2 = Conv2D(96, (1, 1), padding='valid')(y)\n",
        "    y2 = BatchNormalization()(y2)\n",
        "    y = Add()([y1, y2])\n",
        "    y = Activation('relu')(y)\n",
        "\n",
        "    # CONV 2 - 2\n",
        "    y1 = Conv2D(48, (1, 1), padding='valid')(y)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(48, (3, 3), padding='same')(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(96, (1, 1), padding='valid')(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    \n",
        "    y = Add()([y1, y])\n",
        "    y = Activation('relu')(y)\n",
        "\n",
        "\n",
        "    # CONV 3 - 1\n",
        "    y1 = Conv2D(96, (1, 1), padding='valid', strides=(2, 2))(y)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(96, (3, 3), padding='same')(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(128, (1, 1), padding='valid')(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "\n",
        "    y2 = AveragePooling2D((2, 2), strides=2, padding='same')(y)\n",
        "    y2 = Conv2D(128, (1, 1), padding='valid')(y2)\n",
        "    # y2 = Conv2D(128, (1, 1), padding='valid', strides=(2, 2))(y)\n",
        "    y2 = BatchNormalization()(y2)\n",
        "    y = Add()([y1, y2])\n",
        "    y = Activation('relu')(y)\n",
        "  \n",
        "    # CONV 3 - 2\n",
        "    y1 = Conv2D(96, (1, 1), padding='valid')(y)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(96, (3, 3), padding='same')(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(128, (1, 1), padding='valid')(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "\n",
        "    y = Add()([y1, y])\n",
        "    y = Activation('relu')(y)\n",
        "\n",
        "    # CONV 4 - 1\n",
        "    y1 = Conv2D(128, (1, 1), padding='valid', strides=(2, 2))(y)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(128, (3, 3), padding='same')(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(256, (1, 1), padding='valid')(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "\n",
        "    # y2 = Conv2D(256, (1, 1), padding='valid', strides=(2, 2))(y)\n",
        "    y2 = AveragePooling2D((2, 2), strides=2, padding='same')(y)\n",
        "    y2 = Conv2D(256, (1, 1), padding='valid')(y2)\n",
        "    y2 = BatchNormalization()(y2)\n",
        "    y = Add()([y1, y2])\n",
        "    y = Activation('relu')(y)\n",
        "\n",
        "    # CONV 4 - 2\n",
        "    y1 = Conv2D(128, (1, 1), padding='valid')(y)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(128, (3, 3), padding='same')(y1)\n",
        "    y1 = BatchNormalization(gamma_initializer='zeros')(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(256, (1, 1), padding='valid')(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "\n",
        "    y = Add()([y1, y])\n",
        "    y = Activation('relu')(y)\n",
        "\n",
        "    # CONV 5 - 1\n",
        "    y1 = Conv2D(256, (1, 1), padding='valid', strides=(2, 2))(y)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(256, (3, 3), padding='same')(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(512, (1, 1), padding='valid')(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "\n",
        "    # y2 = Conv2D(512, (1, 1), padding='valid', strides=(2, 2))(y)\n",
        "    y2 = AveragePooling2D((2, 2), strides=2, padding='same')(y)\n",
        "    y2 = Conv2D(512, (1, 1), padding='valid')(y2)\n",
        "    y2 = BatchNormalization()(y2)\n",
        "    y = Add()([y1, y2])\n",
        "    y = Activation('relu')(y)\n",
        "\n",
        "    # CONV 5 - 2\n",
        "    y1 = Conv2D(256, (1, 1), padding='valid')(y)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(256, (3, 3), padding='same')(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation('relu')(y1)\n",
        "    y1 = Conv2D(512, (1, 1), padding='valid')(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "\n",
        "    y = Add()([y1, y])\n",
        "    y = Activation('relu')(y)\n",
        "\n",
        "    # CONV 6\n",
        "    y = Conv2D(512, (2, 2), padding='valid')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "\n",
        "    # y = Reshape((-1, 512))(y)\n",
        "    # y = Flatten()(y)\n",
        "    y = GlobalAveragePooling2D()(y)\n",
        "    # y = NetVLAD(num_clusters=num_clusters)(y)\n",
        "    \n",
        "    # 여기서 letter와 합침\n",
        "    y = tf.keras.layers.Concatenate(axis=1)([y, input_1])\n",
        "    y = Dropout(0.3)(y)\n",
        "\n",
        "    y = Dense(output_size)(y)\n",
        "    y = Activation('softmax')(y)\n",
        "    output = y\n",
        "\n",
        "    model = Model(inputs=[input_1, input_2], outputs=output, name='thin_resnet_model')\n",
        "    # optimizer = tfa.optimizers.AdamW(learning_rate=0.05, weight_decay=0.0001)\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSTZkr602A_o"
      },
      "source": [
        "def vggnet_model(input_shape_1, input_shape_2, output_size=10):\n",
        "    input_1 = Input(shape=input_shape_1)  # letter\n",
        "    input_2 = Input(shape=input_shape_2)  # pixel\n",
        "\n",
        "    y = input_2\n",
        "\n",
        "    y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
        "    y = tf.keras.layers.experimental.preprocessing.Resizing(height=56, width=56)(y)\n",
        "    y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
        "   \n",
        "\n",
        "    model = VGG19(include_top=False, input_tensor=y, pooling='max', input_shape=y.shape[1:], weights=None)\n",
        "\n",
        "    y = model.output\n",
        "    y = Dense(1024)(y)\n",
        "    y = Dropout(0.25)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = tf.keras.layers.Concatenate(axis=1)([y, input_1])\n",
        "    y = Dense(256)(y)\n",
        "    y = Dropout(0.25)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = Dense(output_size, activation='softmax', name='softmax')(y)\n",
        "\n",
        "    output = y\n",
        "\n",
        "    model = Model(inputs=[input_1, input_2], outputs=output, name='vggnet_model')\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z471-B9k4H0q"
      },
      "source": [
        "def resnet_model(input_shape_1, input_shape_2, output_size=10):\n",
        "    input_1 = Input(shape=input_shape_1)  # letter\n",
        "    input_2 = Input(shape=input_shape_2)  # pixel\n",
        "\n",
        "    y = input_2\n",
        "\n",
        "    y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
        "    y = tf.keras.layers.experimental.preprocessing.Resizing(height=56, width=56)(y)\n",
        "    y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
        "   \n",
        "\n",
        "    model = ResNet101V2(include_top=False, input_tensor=y, pooling='max', input_shape=y.shape[1:], weights=None)\n",
        "\n",
        "    y = model.output\n",
        "    y = Dense(1024)(y)\n",
        "    y = Dropout(0.25)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = tf.keras.layers.Concatenate(axis=1)([y, input_1])\n",
        "    y = Dense(256)(y)\n",
        "    y = Dropout(0.25)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = Dense(output_size, activation='softmax', name='softmax')(y)\n",
        "\n",
        "    output = y\n",
        "\n",
        "    model = Model(inputs=[input_1, input_2], outputs=output, name='resnet_model')\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TpkwNeZ4UQU"
      },
      "source": [
        "def densenet_model(input_shape_1, input_shape_2, output_size=10):\n",
        "    input_1 = Input(shape=input_shape_1)  # letter\n",
        "    input_2 = Input(shape=input_shape_2)  # pixel\n",
        "\n",
        "    y = input_2\n",
        "\n",
        "    y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
        "    y = tf.keras.layers.experimental.preprocessing.Resizing(height=112, width=112)(y)\n",
        "    y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
        "    y = RandomRotation(factor=(-0.1, 0.1), fill_mode='constant')(y)\n",
        "\n",
        "    model = DenseNet121(include_top=False, input_tensor=y, input_shape=y.shape[1:], weights=None, pooling='max')\n",
        "\n",
        "    y = model.output\n",
        "    y = Dense(1024)(y)\n",
        "    y = Dropout(0.25)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = tf.keras.layers.Concatenate(axis=1)([y, input_1])\n",
        "    y = Dense(256)(y)\n",
        "    y = Dropout(0.25)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = Dense(output_size, activation='softmax', name='softmax')(y)\n",
        "    output = y\n",
        "\n",
        "    model = Model(inputs=[input_1, input_2], outputs=output, name='densenet_model')\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MheMaXfY4d8a"
      },
      "source": [
        "def xception_model(input_shape_1, input_shape_2, output_size=10):\n",
        "    input_1 = Input(shape=input_shape_1)  # letter\n",
        "    input_2 = Input(shape=input_shape_2)  # pixel\n",
        "\n",
        "    y = input_2\n",
        "\n",
        "    y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
        "    y = tf.keras.layers.experimental.preprocessing.Resizing(height=112, width=112)(y)\n",
        "    y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
        "    y = RandomRotation(factor=(-0.1, 0.1), fill_mode='constant')(y)\n",
        "\n",
        "    model = Xception(include_top=False, input_tensor=y, input_shape=y.shape[1:], weights=None, pooling='max')\n",
        "\n",
        "    y = model.output\n",
        "    y = Dense(1024)(y)\n",
        "    y = Dropout(0.25)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = tf.keras.layers.Concatenate(axis=1)([y, input_1])\n",
        "    y = Dense(256)(y)\n",
        "    y = Dropout(0.25)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = Dense(output_size, activation='softmax', name='softmax')(y)\n",
        "    output = y\n",
        "\n",
        "    model = Model(inputs=[input_1, input_2], outputs=output, name='xception_model')\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMBqPJE_4rHa"
      },
      "source": [
        "def inception_model(input_shape_1, input_shape_2, output_size=10):\n",
        "    input_1 = Input(shape=input_shape_1)  # letter\n",
        "    input_2 = Input(shape=input_shape_2)  # pixel\n",
        "\n",
        "    y = input_2\n",
        "\n",
        "    y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
        "    y = tf.keras.layers.experimental.preprocessing.Resizing(height=112, width=112)(y)\n",
        "    y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
        "    y = RandomRotation(factor=(-0.1, 0.1), fill_mode='constant')(y)\n",
        "\n",
        "    model = InceptionV3(include_top=False, input_tensor=y, input_shape=y.shape[1:], weights=None, pooling='max')\n",
        "\n",
        "    y = model.output\n",
        "    y = Dense(1024)(y)\n",
        "    y = Dropout(0.25)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = tf.keras.layers.Concatenate(axis=1)([y, input_1])\n",
        "    y = Dense(256)(y)\n",
        "    y = Dropout(0.25)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = Dense(output_size, activation='softmax', name='softmax')(y)\n",
        "    output = y\n",
        "\n",
        "    model = Model(inputs=[input_1, input_2], outputs=output, name='inception_model')\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqFUTOYP4v4C"
      },
      "source": [
        "# 필요 없을 듯\n",
        "# def k_fold_validation(model_fn, n_splits=5, verbose=1):\n",
        "#     kf = KFold(n_splits=n_splits)\n",
        "#     sum_accuracy = 0\n",
        "#     sum_epoch = 0\n",
        "#     start_time = time.time()\n",
        "#     for i, (train_index, val_index) in enumerate(kf.split(train_digits)):\n",
        "#         model = model_fn(input_shape_1=input_shape_1, input_shape_2=input_shape_2, output_size=output_size)\n",
        "\n",
        "#         train_data = [train_letters[train_index], train_pixels[train_index]]\n",
        "#         train_label = train_digits[train_index]\n",
        "\n",
        "#         val_data = [train_letters[val_index], train_pixels[val_index]]\n",
        "#         val_label = train_digits[val_index]\n",
        "\n",
        "#         history = model.fit(\n",
        "#             train_data, train_label,\n",
        "#             epochs=EPOCHS,\n",
        "#             validation_data=(val_data, val_label),\n",
        "#             batch_size=BATCH_SIZE,\n",
        "#             verbose=verbose,\n",
        "#             callbacks=[early_stopping],\n",
        "#         )\n",
        "\n",
        "#         sum_epoch += len(history.history['val_accuracy'])\n",
        "#         sum_accuracy += max(history.history['val_accuracy'])\n",
        "        \n",
        "#         if verbose >= 0:            \n",
        "#             print(f'{i+1}/{n_splits} fold result: ')\n",
        "#             print('epoch num:', len(history.history['val_accuracy']))\n",
        "#             print('best val accuracy: ', max(history.history['val_accuracy']))\n",
        "#             print('average 20: ', np.mean(history.history['val_accuracy'][-20:]))\n",
        "#             print('='*50)\n",
        "\n",
        "#     print('Average Accuracy: ', sum_accuracy/n_splits)\n",
        "#     print('Average Epoch: ', sum_epoch/n_splits)\n",
        "#     print('Time taken: ', time.time() - start_time)\n",
        "#     print('='*50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mqAFQzH5KNf"
      },
      "source": [
        "model_fn_list = [\n",
        "    thin_resnet_model,\n",
        "    vggnet_model,\n",
        "    resnet_model,\n",
        "    densenet_model,\n",
        "    xception_model,\n",
        "    inception_model\n",
        "]\n",
        "\n",
        "EPOCHS = 300\n",
        "BATCH_SIZE = 16\n",
        "CHECKPOINT_PATH = 'checkpoint/'\n",
        "MODEL_PATH = 'model/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXRQ63g85NT-"
      },
      "source": [
        "if os.path.isdir(CHECKPOINT_PATH):\n",
        "    shutil.rmtree(CHECKPOINT_PATH, ignore_errors=True)\n",
        "os.mkdir(CHECKPOINT_PATH)\n",
        "\n",
        "if os.path.isdir(MODEL_PATH):\n",
        "    shutil.rmtree(MODEL_PATH, ignore_errors=True)\n",
        "os.mkdir(MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPLl-TPl41Yw",
        "outputId": "3841c403-487c-4ea3-94bb-c6cefccba009"
      },
      "source": [
        "for model_fn in model_fn_list:\n",
        "    kf = KFold(n_splits=5)\n",
        "    for i, (train_index, val_index) in enumerate(kf.split(train_digits)):\n",
        "        start_time = time.time()\n",
        "        model = model_fn(input_shape_1=input_shape_1, input_shape_2=input_shape_2, output_size=output_size)\n",
        "\n",
        "        # Validation 점수가 가장 좋은 모델만 저장합니다.\n",
        "        checkpoint_path = os.path.join(CHECKPOINT_PATH, f'{model.name}_fold{i+1}')\n",
        "        if os.path.isdir(checkpoint_path):\n",
        "            shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
        "        os.mkdir(checkpoint_path)\n",
        "        checkpoint_file_path = os.path.join(checkpoint_path, 'Epoch_{epoch:03d}_Val_{val_loss:.3f}.hdf5')\n",
        "        checkpoint = ModelCheckpoint(filepath=checkpoint_file_path, monitor='val_accuracy', verbose=0, save_best_only=True)\n",
        "\n",
        "        # 30회 간 Validation 점수가 좋아지지 않으면 중지합니다.\n",
        "        early_stopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
        "\n",
        "        train_data = [train_letters[train_index], train_pixels[train_index]]\n",
        "        train_label = train_digits[train_index]\n",
        "\n",
        "        val_data = [train_letters[val_index], train_pixels[val_index]]\n",
        "        val_label = train_digits[val_index]\n",
        "\n",
        "        history = model.fit(\n",
        "            train_data, train_label,\n",
        "            epochs=EPOCHS,\n",
        "            validation_data=(val_data, val_label),\n",
        "            batch_size=BATCH_SIZE,\n",
        "            verbose=1,\n",
        "            callbacks=[early_stopping, checkpoint],\n",
        "        )\n",
        "\n",
        "        # 가장 좋은 모델의 weight를 불러옵니다.\n",
        "        weigth_file = glob.glob('{}/*.hdf5'.format(checkpoint_path))[-1]\n",
        "        model.load_weights(weigth_file)\n",
        "        model.save(os.path.join(MODEL_PATH, f'{model.name}_{i+1}.h5'))\n",
        "        \n",
        "        shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
        "\n",
        "        epoch_num = len(history.history['val_accuracy'])\n",
        "        max_accuracy = max(history.history['val_accuracy'])\n",
        "        print('='*50)\n",
        "        print(f'Result of {model.name}, fold {i+1}')\n",
        "        print(f'Epoch: {epoch_num}')\n",
        "        print(f'Accuracy: {max_accuracy}')\n",
        "        print('Time taken: ', time.time() - start_time)\n",
        "        print('='*50)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "103/103 [==============================] - 39s 30ms/step - loss: 0.1006 - accuracy: 0.1453 - val_loss: 0.0922 - val_accuracy: 0.1073\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0920 - accuracy: 0.1528 - val_loss: 0.1108 - val_accuracy: 0.1073\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0910 - accuracy: 0.2302 - val_loss: 0.1185 - val_accuracy: 0.1073\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0898 - accuracy: 0.2635 - val_loss: 0.1010 - val_accuracy: 0.1780\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 0.0874 - accuracy: 0.2606 - val_loss: 0.1067 - val_accuracy: 0.2317\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0839 - accuracy: 0.2933 - val_loss: 0.1149 - val_accuracy: 0.2195\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0860 - accuracy: 0.2873 - val_loss: 0.1732 - val_accuracy: 0.1341\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0862 - accuracy: 0.2803 - val_loss: 0.1691 - val_accuracy: 0.1073\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0829 - accuracy: 0.3118 - val_loss: 0.1303 - val_accuracy: 0.2195\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0830 - accuracy: 0.3164 - val_loss: 0.1384 - val_accuracy: 0.1366\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0838 - accuracy: 0.3046 - val_loss: 0.0840 - val_accuracy: 0.3268\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0796 - accuracy: 0.3518 - val_loss: 0.1037 - val_accuracy: 0.1854\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 0.0840 - accuracy: 0.3124 - val_loss: 0.1016 - val_accuracy: 0.1927\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0808 - accuracy: 0.3555 - val_loss: 0.1150 - val_accuracy: 0.2829\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0818 - accuracy: 0.3541 - val_loss: 0.1421 - val_accuracy: 0.1146\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0782 - accuracy: 0.3728 - val_loss: 0.0821 - val_accuracy: 0.3268\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0773 - accuracy: 0.4001 - val_loss: 0.0829 - val_accuracy: 0.3195\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 0.0781 - accuracy: 0.3630 - val_loss: 0.1005 - val_accuracy: 0.2707\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0750 - accuracy: 0.4156 - val_loss: 0.0938 - val_accuracy: 0.2976\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0783 - accuracy: 0.3734 - val_loss: 0.0910 - val_accuracy: 0.3415\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0751 - accuracy: 0.4191 - val_loss: 0.1237 - val_accuracy: 0.1634\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 0.0765 - accuracy: 0.3964 - val_loss: 0.0816 - val_accuracy: 0.3293\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0735 - accuracy: 0.4086 - val_loss: 0.0717 - val_accuracy: 0.4293\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0720 - accuracy: 0.4441 - val_loss: 0.0792 - val_accuracy: 0.3878\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0704 - accuracy: 0.4468 - val_loss: 0.0740 - val_accuracy: 0.4220\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0703 - accuracy: 0.4814 - val_loss: 0.1397 - val_accuracy: 0.1512\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0709 - accuracy: 0.4522 - val_loss: 0.0857 - val_accuracy: 0.3561\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0669 - accuracy: 0.5080 - val_loss: 0.0921 - val_accuracy: 0.3171\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0658 - accuracy: 0.4938 - val_loss: 0.0642 - val_accuracy: 0.5268\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0642 - accuracy: 0.5168 - val_loss: 0.0804 - val_accuracy: 0.3927\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0644 - accuracy: 0.4928 - val_loss: 0.0700 - val_accuracy: 0.4439\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0610 - accuracy: 0.5458 - val_loss: 0.0656 - val_accuracy: 0.4976\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0595 - accuracy: 0.5614 - val_loss: 0.0596 - val_accuracy: 0.5439\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0635 - accuracy: 0.5238 - val_loss: 0.0648 - val_accuracy: 0.5000\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0616 - accuracy: 0.5327 - val_loss: 0.0659 - val_accuracy: 0.4976\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0606 - accuracy: 0.5390 - val_loss: 0.0664 - val_accuracy: 0.4976\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0572 - accuracy: 0.5708 - val_loss: 0.0591 - val_accuracy: 0.5585\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0570 - accuracy: 0.5797 - val_loss: 0.0846 - val_accuracy: 0.4293\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0534 - accuracy: 0.6035 - val_loss: 0.0825 - val_accuracy: 0.3829\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.5980 - val_loss: 0.0750 - val_accuracy: 0.4415\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0530 - accuracy: 0.6094 - val_loss: 0.0605 - val_accuracy: 0.5732\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0556 - accuracy: 0.5818 - val_loss: 0.0550 - val_accuracy: 0.5805\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0499 - accuracy: 0.6320 - val_loss: 0.0617 - val_accuracy: 0.5341\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0546 - accuracy: 0.6028 - val_loss: 0.0545 - val_accuracy: 0.6073\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0516 - accuracy: 0.6164 - val_loss: 0.0636 - val_accuracy: 0.5341\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0469 - accuracy: 0.6623 - val_loss: 0.0508 - val_accuracy: 0.6366\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0450 - accuracy: 0.6798 - val_loss: 0.0583 - val_accuracy: 0.5976\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0493 - accuracy: 0.6410 - val_loss: 0.0530 - val_accuracy: 0.6146\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0455 - accuracy: 0.6735 - val_loss: 0.0544 - val_accuracy: 0.6098\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0496 - accuracy: 0.6330 - val_loss: 0.0698 - val_accuracy: 0.5000\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0457 - accuracy: 0.6712 - val_loss: 0.0675 - val_accuracy: 0.5268\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0469 - accuracy: 0.6711 - val_loss: 0.0628 - val_accuracy: 0.5220\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0409 - accuracy: 0.7034 - val_loss: 0.0478 - val_accuracy: 0.6634\n",
            "Epoch 54/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0430 - accuracy: 0.6769 - val_loss: 0.0721 - val_accuracy: 0.4707\n",
            "Epoch 55/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0387 - accuracy: 0.7297 - val_loss: 0.0455 - val_accuracy: 0.6829\n",
            "Epoch 56/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0387 - accuracy: 0.7230 - val_loss: 0.0582 - val_accuracy: 0.5780\n",
            "Epoch 57/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0391 - accuracy: 0.7334 - val_loss: 0.0399 - val_accuracy: 0.7000\n",
            "Epoch 58/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0438 - accuracy: 0.6849 - val_loss: 0.0462 - val_accuracy: 0.6780\n",
            "Epoch 59/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0411 - accuracy: 0.7055 - val_loss: 0.0621 - val_accuracy: 0.5415\n",
            "Epoch 60/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0412 - accuracy: 0.7063 - val_loss: 0.0513 - val_accuracy: 0.6463\n",
            "Epoch 61/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0376 - accuracy: 0.7353 - val_loss: 0.0437 - val_accuracy: 0.6854\n",
            "Epoch 62/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0362 - accuracy: 0.7365 - val_loss: 0.0498 - val_accuracy: 0.6488\n",
            "Epoch 63/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0367 - accuracy: 0.7400 - val_loss: 0.0455 - val_accuracy: 0.6878\n",
            "Epoch 64/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0364 - accuracy: 0.7460 - val_loss: 0.0392 - val_accuracy: 0.7366\n",
            "Epoch 65/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0388 - accuracy: 0.7328 - val_loss: 0.0387 - val_accuracy: 0.7073\n",
            "Epoch 66/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0344 - accuracy: 0.7572 - val_loss: 0.0473 - val_accuracy: 0.6854\n",
            "Epoch 67/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0349 - accuracy: 0.7518 - val_loss: 0.0523 - val_accuracy: 0.6366\n",
            "Epoch 68/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0383 - accuracy: 0.7266 - val_loss: 0.0552 - val_accuracy: 0.6000\n",
            "Epoch 69/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0352 - accuracy: 0.7604 - val_loss: 0.0463 - val_accuracy: 0.6732\n",
            "Epoch 70/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0351 - accuracy: 0.7538 - val_loss: 0.0572 - val_accuracy: 0.5780\n",
            "Epoch 71/300\n",
            "103/103 [==============================] - 2s 15ms/step - loss: 0.0332 - accuracy: 0.7664 - val_loss: 0.0487 - val_accuracy: 0.6585\n",
            "Epoch 72/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0321 - accuracy: 0.7728 - val_loss: 0.0471 - val_accuracy: 0.6756\n",
            "Epoch 73/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0327 - accuracy: 0.7653 - val_loss: 0.0547 - val_accuracy: 0.5927\n",
            "Epoch 74/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0335 - accuracy: 0.7722 - val_loss: 0.0325 - val_accuracy: 0.7902\n",
            "Epoch 75/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0285 - accuracy: 0.8110 - val_loss: 0.0371 - val_accuracy: 0.7415\n",
            "Epoch 76/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0290 - accuracy: 0.7893 - val_loss: 0.0414 - val_accuracy: 0.7220\n",
            "Epoch 77/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0321 - accuracy: 0.7729 - val_loss: 0.0398 - val_accuracy: 0.7293\n",
            "Epoch 78/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0326 - accuracy: 0.7678 - val_loss: 0.0554 - val_accuracy: 0.5951\n",
            "Epoch 79/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0311 - accuracy: 0.7906 - val_loss: 0.0443 - val_accuracy: 0.6780\n",
            "Epoch 80/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0317 - accuracy: 0.7842 - val_loss: 0.0370 - val_accuracy: 0.7341\n",
            "Epoch 81/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0301 - accuracy: 0.7826 - val_loss: 0.0546 - val_accuracy: 0.6024\n",
            "Epoch 82/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0292 - accuracy: 0.8045 - val_loss: 0.0392 - val_accuracy: 0.7268\n",
            "Epoch 83/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0276 - accuracy: 0.8196 - val_loss: 0.0739 - val_accuracy: 0.4756\n",
            "Epoch 84/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0327 - accuracy: 0.7751 - val_loss: 0.0511 - val_accuracy: 0.6415\n",
            "==================================================\n",
            "Result of thin_resnet_model, fold 1\n",
            "Epoch: 84\n",
            "Accuracy: 0.790243923664093\n",
            "Time taken:  193.3606677055359\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 5s 23ms/step - loss: 0.1012 - accuracy: 0.1146 - val_loss: 0.0931 - val_accuracy: 0.0902\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0918 - accuracy: 0.1866 - val_loss: 0.1553 - val_accuracy: 0.0878\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0894 - accuracy: 0.2022 - val_loss: 0.1242 - val_accuracy: 0.0878\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0893 - accuracy: 0.2613 - val_loss: 0.1001 - val_accuracy: 0.1805\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0873 - accuracy: 0.2554 - val_loss: 0.1467 - val_accuracy: 0.0927\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0891 - accuracy: 0.2388 - val_loss: 0.1750 - val_accuracy: 0.1220\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0870 - accuracy: 0.2722 - val_loss: 0.0954 - val_accuracy: 0.1878\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0839 - accuracy: 0.3118 - val_loss: 0.0966 - val_accuracy: 0.1366\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0835 - accuracy: 0.3382 - val_loss: 0.1665 - val_accuracy: 0.1195\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0852 - accuracy: 0.3199 - val_loss: 0.1069 - val_accuracy: 0.2098\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0827 - accuracy: 0.3144 - val_loss: 0.1173 - val_accuracy: 0.2439\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0812 - accuracy: 0.3492 - val_loss: 0.0852 - val_accuracy: 0.2829\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0838 - accuracy: 0.3247 - val_loss: 0.0788 - val_accuracy: 0.3463\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0814 - accuracy: 0.3399 - val_loss: 0.1048 - val_accuracy: 0.2268\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0796 - accuracy: 0.3364 - val_loss: 0.0881 - val_accuracy: 0.2951\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0796 - accuracy: 0.3480 - val_loss: 0.1136 - val_accuracy: 0.1683\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0778 - accuracy: 0.3948 - val_loss: 0.0911 - val_accuracy: 0.2561\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0753 - accuracy: 0.4009 - val_loss: 0.0758 - val_accuracy: 0.3951\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0754 - accuracy: 0.4148 - val_loss: 0.0771 - val_accuracy: 0.3976\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0748 - accuracy: 0.4112 - val_loss: 0.0785 - val_accuracy: 0.4146\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0691 - accuracy: 0.4656 - val_loss: 0.0661 - val_accuracy: 0.4683\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0674 - accuracy: 0.4795 - val_loss: 0.0701 - val_accuracy: 0.4439\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0707 - accuracy: 0.4447 - val_loss: 0.0709 - val_accuracy: 0.4463\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0690 - accuracy: 0.4718 - val_loss: 0.0791 - val_accuracy: 0.3902\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0666 - accuracy: 0.4931 - val_loss: 0.0668 - val_accuracy: 0.4805\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0656 - accuracy: 0.5222 - val_loss: 0.0617 - val_accuracy: 0.5220\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0641 - accuracy: 0.5137 - val_loss: 0.0778 - val_accuracy: 0.3829\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0658 - accuracy: 0.4961 - val_loss: 0.0742 - val_accuracy: 0.4317\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0608 - accuracy: 0.5522 - val_loss: 0.0743 - val_accuracy: 0.4122\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0609 - accuracy: 0.5473 - val_loss: 0.0585 - val_accuracy: 0.5659\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0582 - accuracy: 0.5758 - val_loss: 0.0673 - val_accuracy: 0.4805\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0586 - accuracy: 0.5645 - val_loss: 0.0577 - val_accuracy: 0.5829\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0627 - accuracy: 0.5307 - val_loss: 0.0536 - val_accuracy: 0.5854\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0586 - accuracy: 0.5490 - val_loss: 0.0766 - val_accuracy: 0.4195\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0559 - accuracy: 0.6004 - val_loss: 0.0622 - val_accuracy: 0.5171\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0577 - accuracy: 0.5753 - val_loss: 0.0772 - val_accuracy: 0.4488\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0553 - accuracy: 0.5966 - val_loss: 0.0998 - val_accuracy: 0.2854\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0558 - accuracy: 0.5889 - val_loss: 0.0581 - val_accuracy: 0.5976\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0503 - accuracy: 0.6368 - val_loss: 0.0505 - val_accuracy: 0.6171\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0514 - accuracy: 0.6256 - val_loss: 0.0602 - val_accuracy: 0.5366\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0542 - accuracy: 0.5949 - val_loss: 0.0760 - val_accuracy: 0.4122\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0540 - accuracy: 0.6025 - val_loss: 0.0598 - val_accuracy: 0.5902\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0476 - accuracy: 0.6450 - val_loss: 0.0557 - val_accuracy: 0.6073\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0500 - accuracy: 0.6383 - val_loss: 0.0454 - val_accuracy: 0.6659\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0493 - accuracy: 0.6564 - val_loss: 0.0545 - val_accuracy: 0.6195\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0462 - accuracy: 0.6574 - val_loss: 0.0867 - val_accuracy: 0.4000\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0480 - accuracy: 0.6549 - val_loss: 0.0565 - val_accuracy: 0.5780\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0461 - accuracy: 0.6574 - val_loss: 0.0442 - val_accuracy: 0.6902\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0440 - accuracy: 0.6807 - val_loss: 0.0514 - val_accuracy: 0.6268\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0436 - accuracy: 0.6969 - val_loss: 0.0455 - val_accuracy: 0.6707\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0445 - accuracy: 0.6757 - val_loss: 0.0451 - val_accuracy: 0.6732\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0434 - accuracy: 0.6842 - val_loss: 0.0525 - val_accuracy: 0.6073\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0420 - accuracy: 0.6966 - val_loss: 0.0453 - val_accuracy: 0.6829\n",
            "Epoch 54/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0429 - accuracy: 0.7018 - val_loss: 0.0433 - val_accuracy: 0.6902\n",
            "Epoch 55/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0379 - accuracy: 0.7282 - val_loss: 0.0507 - val_accuracy: 0.6439\n",
            "Epoch 56/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0407 - accuracy: 0.7088 - val_loss: 0.0591 - val_accuracy: 0.5854\n",
            "Epoch 57/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0404 - accuracy: 0.7068 - val_loss: 0.0486 - val_accuracy: 0.6537\n",
            "Epoch 58/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0385 - accuracy: 0.7312 - val_loss: 0.0419 - val_accuracy: 0.6976\n",
            "Epoch 59/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0375 - accuracy: 0.7222 - val_loss: 0.0396 - val_accuracy: 0.7268\n",
            "Epoch 60/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0398 - accuracy: 0.7198 - val_loss: 0.0523 - val_accuracy: 0.6317\n",
            "Epoch 61/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0367 - accuracy: 0.7530 - val_loss: 0.0434 - val_accuracy: 0.7073\n",
            "Epoch 62/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0360 - accuracy: 0.7599 - val_loss: 0.0377 - val_accuracy: 0.7293\n",
            "Epoch 63/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0339 - accuracy: 0.7718 - val_loss: 0.0497 - val_accuracy: 0.6390\n",
            "Epoch 64/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0341 - accuracy: 0.7557 - val_loss: 0.0404 - val_accuracy: 0.7000\n",
            "Epoch 65/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0365 - accuracy: 0.7494 - val_loss: 0.0410 - val_accuracy: 0.7195\n",
            "Epoch 66/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0316 - accuracy: 0.7816 - val_loss: 0.0561 - val_accuracy: 0.6146\n",
            "Epoch 67/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0357 - accuracy: 0.7445 - val_loss: 0.0521 - val_accuracy: 0.6268\n",
            "Epoch 68/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0357 - accuracy: 0.7504 - val_loss: 0.0387 - val_accuracy: 0.7366\n",
            "Epoch 69/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0367 - accuracy: 0.7478 - val_loss: 0.0395 - val_accuracy: 0.6976\n",
            "Epoch 70/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0321 - accuracy: 0.7716 - val_loss: 0.0367 - val_accuracy: 0.7488\n",
            "Epoch 71/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0358 - accuracy: 0.7457 - val_loss: 0.0637 - val_accuracy: 0.5610\n",
            "Epoch 72/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0335 - accuracy: 0.7594 - val_loss: 0.0340 - val_accuracy: 0.7610\n",
            "Epoch 73/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0309 - accuracy: 0.7850 - val_loss: 0.0301 - val_accuracy: 0.7976\n",
            "Epoch 74/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0292 - accuracy: 0.7984 - val_loss: 0.0369 - val_accuracy: 0.7537\n",
            "Epoch 75/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0300 - accuracy: 0.7994 - val_loss: 0.0410 - val_accuracy: 0.7146\n",
            "Epoch 76/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0333 - accuracy: 0.7637 - val_loss: 0.0956 - val_accuracy: 0.3829\n",
            "Epoch 77/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0281 - accuracy: 0.8064 - val_loss: 0.0403 - val_accuracy: 0.7195\n",
            "Epoch 78/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0292 - accuracy: 0.7799 - val_loss: 0.0357 - val_accuracy: 0.7537\n",
            "Epoch 79/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0293 - accuracy: 0.7947 - val_loss: 0.0354 - val_accuracy: 0.7683\n",
            "Epoch 80/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0288 - accuracy: 0.8047 - val_loss: 0.0773 - val_accuracy: 0.4341\n",
            "Epoch 81/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0290 - accuracy: 0.8029 - val_loss: 0.0375 - val_accuracy: 0.7341\n",
            "Epoch 82/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0302 - accuracy: 0.7883 - val_loss: 0.0316 - val_accuracy: 0.7780\n",
            "Epoch 83/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0286 - accuracy: 0.8051 - val_loss: 0.0310 - val_accuracy: 0.7829\n",
            "==================================================\n",
            "Result of thin_resnet_model, fold 2\n",
            "Epoch: 83\n",
            "Accuracy: 0.79756098985672\n",
            "Time taken:  160.1654405593872\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 5s 23ms/step - loss: 0.1005 - accuracy: 0.1434 - val_loss: 0.0915 - val_accuracy: 0.0976\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0907 - accuracy: 0.2017 - val_loss: 0.1078 - val_accuracy: 0.0976\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0922 - accuracy: 0.1724 - val_loss: 0.1510 - val_accuracy: 0.0976\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0897 - accuracy: 0.2562 - val_loss: 0.1270 - val_accuracy: 0.1098\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0886 - accuracy: 0.2591 - val_loss: 0.1109 - val_accuracy: 0.2878\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0879 - accuracy: 0.2451 - val_loss: 0.1055 - val_accuracy: 0.2854\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0861 - accuracy: 0.3118 - val_loss: 0.0991 - val_accuracy: 0.1561\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0852 - accuracy: 0.3050 - val_loss: 0.1628 - val_accuracy: 0.0854\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0840 - accuracy: 0.3210 - val_loss: 0.1189 - val_accuracy: 0.1634\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0829 - accuracy: 0.3339 - val_loss: 0.0878 - val_accuracy: 0.2244\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0865 - accuracy: 0.3011 - val_loss: 0.0728 - val_accuracy: 0.4488\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0829 - accuracy: 0.3396 - val_loss: 0.1210 - val_accuracy: 0.2390\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0822 - accuracy: 0.3483 - val_loss: 0.1021 - val_accuracy: 0.2220\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0859 - accuracy: 0.2895 - val_loss: 0.1581 - val_accuracy: 0.1146\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0860 - accuracy: 0.3237 - val_loss: 0.1343 - val_accuracy: 0.2195\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0804 - accuracy: 0.3674 - val_loss: 0.0860 - val_accuracy: 0.3293\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0810 - accuracy: 0.3541 - val_loss: 0.0849 - val_accuracy: 0.3195\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 2s 16ms/step - loss: 0.0780 - accuracy: 0.3898 - val_loss: 0.0930 - val_accuracy: 0.3195\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0793 - accuracy: 0.3525 - val_loss: 0.0681 - val_accuracy: 0.4463\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0784 - accuracy: 0.3834 - val_loss: 0.1245 - val_accuracy: 0.1073\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0833 - accuracy: 0.3109 - val_loss: 0.0867 - val_accuracy: 0.3073\n",
            "==================================================\n",
            "Result of thin_resnet_model, fold 3\n",
            "Epoch: 21\n",
            "Accuracy: 0.44878047704696655\n",
            "Time taken:  44.93249464035034\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 6s 30ms/step - loss: 0.1012 - accuracy: 0.1339 - val_loss: 0.0904 - val_accuracy: 0.1027\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0928 - accuracy: 0.1571 - val_loss: 0.0905 - val_accuracy: 0.0856\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0911 - accuracy: 0.2052 - val_loss: 0.1089 - val_accuracy: 0.1100\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0895 - accuracy: 0.2399 - val_loss: 0.0982 - val_accuracy: 0.1760\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0895 - accuracy: 0.2380 - val_loss: 0.1121 - val_accuracy: 0.1320\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0889 - accuracy: 0.2395 - val_loss: 0.1189 - val_accuracy: 0.1369\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0871 - accuracy: 0.2766 - val_loss: 0.1302 - val_accuracy: 0.1125\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0875 - accuracy: 0.2565 - val_loss: 0.1350 - val_accuracy: 0.1247\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0842 - accuracy: 0.3183 - val_loss: 0.0997 - val_accuracy: 0.1883\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0858 - accuracy: 0.2934 - val_loss: 0.0850 - val_accuracy: 0.3447\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0840 - accuracy: 0.3123 - val_loss: 0.0888 - val_accuracy: 0.2200\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0818 - accuracy: 0.3584 - val_loss: 0.1069 - val_accuracy: 0.2763\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0803 - accuracy: 0.3468 - val_loss: 0.0779 - val_accuracy: 0.3447\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0795 - accuracy: 0.3658 - val_loss: 0.0974 - val_accuracy: 0.1956\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0795 - accuracy: 0.3617 - val_loss: 0.0861 - val_accuracy: 0.2958\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0815 - accuracy: 0.3384 - val_loss: 0.1079 - val_accuracy: 0.2469\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0775 - accuracy: 0.3932 - val_loss: 0.0869 - val_accuracy: 0.2934\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0778 - accuracy: 0.3812 - val_loss: 0.0719 - val_accuracy: 0.4401\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0762 - accuracy: 0.3943 - val_loss: 0.0862 - val_accuracy: 0.2836\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0774 - accuracy: 0.3646 - val_loss: 0.0710 - val_accuracy: 0.4719\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0742 - accuracy: 0.4234 - val_loss: 0.0642 - val_accuracy: 0.4890\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0725 - accuracy: 0.4370 - val_loss: 0.0763 - val_accuracy: 0.3863\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0730 - accuracy: 0.4228 - val_loss: 0.0677 - val_accuracy: 0.4768\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0703 - accuracy: 0.4547 - val_loss: 0.0730 - val_accuracy: 0.4425\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0670 - accuracy: 0.4929 - val_loss: 0.0756 - val_accuracy: 0.4597\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0686 - accuracy: 0.4725 - val_loss: 0.0641 - val_accuracy: 0.4866\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0648 - accuracy: 0.5038 - val_loss: 0.0686 - val_accuracy: 0.4817\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0626 - accuracy: 0.5354 - val_loss: 0.0671 - val_accuracy: 0.4817\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0622 - accuracy: 0.5304 - val_loss: 0.0581 - val_accuracy: 0.5819\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0627 - accuracy: 0.5321 - val_loss: 0.0529 - val_accuracy: 0.6210\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0620 - accuracy: 0.5315 - val_loss: 0.0574 - val_accuracy: 0.5721\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0586 - accuracy: 0.5582 - val_loss: 0.0542 - val_accuracy: 0.5990\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0599 - accuracy: 0.5504 - val_loss: 0.0606 - val_accuracy: 0.5428\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0580 - accuracy: 0.5584 - val_loss: 0.0775 - val_accuracy: 0.3985\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0587 - accuracy: 0.5602 - val_loss: 0.0682 - val_accuracy: 0.4792\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0557 - accuracy: 0.5866 - val_loss: 0.0523 - val_accuracy: 0.6210\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0568 - accuracy: 0.5818 - val_loss: 0.0533 - val_accuracy: 0.6039\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0558 - accuracy: 0.5913 - val_loss: 0.0541 - val_accuracy: 0.6259\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0528 - accuracy: 0.5976 - val_loss: 0.0625 - val_accuracy: 0.5159\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0510 - accuracy: 0.6354 - val_loss: 0.0509 - val_accuracy: 0.6357\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0528 - accuracy: 0.6195 - val_loss: 0.0454 - val_accuracy: 0.6675\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0521 - accuracy: 0.6154 - val_loss: 0.0460 - val_accuracy: 0.6650\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0507 - accuracy: 0.6232 - val_loss: 0.0637 - val_accuracy: 0.5355\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0480 - accuracy: 0.6336 - val_loss: 0.0613 - val_accuracy: 0.5379\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0484 - accuracy: 0.6623 - val_loss: 0.0573 - val_accuracy: 0.5746\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0499 - accuracy: 0.6349 - val_loss: 0.0492 - val_accuracy: 0.6455\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0454 - accuracy: 0.6772 - val_loss: 0.0389 - val_accuracy: 0.7262\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0459 - accuracy: 0.6600 - val_loss: 0.0434 - val_accuracy: 0.6773\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0465 - accuracy: 0.6525 - val_loss: 0.0429 - val_accuracy: 0.6993\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0420 - accuracy: 0.6956 - val_loss: 0.0395 - val_accuracy: 0.7115\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0481 - accuracy: 0.6487 - val_loss: 0.0457 - val_accuracy: 0.6650\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0411 - accuracy: 0.6999 - val_loss: 0.0490 - val_accuracy: 0.6528\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0432 - accuracy: 0.6888 - val_loss: 0.0438 - val_accuracy: 0.6919\n",
            "Epoch 54/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0428 - accuracy: 0.7066 - val_loss: 0.0434 - val_accuracy: 0.6773\n",
            "Epoch 55/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0419 - accuracy: 0.7168 - val_loss: 0.0472 - val_accuracy: 0.6724\n",
            "Epoch 56/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0405 - accuracy: 0.7186 - val_loss: 0.0380 - val_accuracy: 0.7311\n",
            "Epoch 57/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0388 - accuracy: 0.7150 - val_loss: 0.0405 - val_accuracy: 0.6993\n",
            "Epoch 58/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0430 - accuracy: 0.6855 - val_loss: 0.0463 - val_accuracy: 0.6528\n",
            "Epoch 59/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0405 - accuracy: 0.7225 - val_loss: 0.0427 - val_accuracy: 0.6797\n",
            "Epoch 60/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0378 - accuracy: 0.7129 - val_loss: 0.0537 - val_accuracy: 0.5623\n",
            "Epoch 61/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0381 - accuracy: 0.7347 - val_loss: 0.0502 - val_accuracy: 0.6381\n",
            "Epoch 62/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0363 - accuracy: 0.7482 - val_loss: 0.0374 - val_accuracy: 0.7311\n",
            "Epoch 63/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0370 - accuracy: 0.7289 - val_loss: 0.0455 - val_accuracy: 0.6822\n",
            "Epoch 64/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0361 - accuracy: 0.7483 - val_loss: 0.0401 - val_accuracy: 0.7066\n",
            "Epoch 65/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0365 - accuracy: 0.7450 - val_loss: 0.0358 - val_accuracy: 0.7531\n",
            "Epoch 66/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0322 - accuracy: 0.7644 - val_loss: 0.0343 - val_accuracy: 0.7531\n",
            "Epoch 67/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0377 - accuracy: 0.7434 - val_loss: 0.0521 - val_accuracy: 0.6577\n",
            "Epoch 68/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0351 - accuracy: 0.7500 - val_loss: 0.0359 - val_accuracy: 0.7457\n",
            "Epoch 69/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0342 - accuracy: 0.7598 - val_loss: 0.0384 - val_accuracy: 0.7237\n",
            "Epoch 70/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0340 - accuracy: 0.7621 - val_loss: 0.0420 - val_accuracy: 0.7017\n",
            "Epoch 71/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0315 - accuracy: 0.7797 - val_loss: 0.0283 - val_accuracy: 0.8020\n",
            "Epoch 72/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0341 - accuracy: 0.7763 - val_loss: 0.0420 - val_accuracy: 0.6968\n",
            "Epoch 73/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0334 - accuracy: 0.7677 - val_loss: 0.0416 - val_accuracy: 0.6944\n",
            "Epoch 74/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0310 - accuracy: 0.7809 - val_loss: 0.0383 - val_accuracy: 0.7262\n",
            "Epoch 75/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0308 - accuracy: 0.7918 - val_loss: 0.0404 - val_accuracy: 0.7139\n",
            "Epoch 76/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0311 - accuracy: 0.7834 - val_loss: 0.0476 - val_accuracy: 0.6773\n",
            "Epoch 77/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0256 - accuracy: 0.8244 - val_loss: 0.0342 - val_accuracy: 0.7751\n",
            "Epoch 78/300\n",
            "103/103 [==============================] - 2s 21ms/step - loss: 0.0289 - accuracy: 0.7951 - val_loss: 0.0296 - val_accuracy: 0.7995\n",
            "Epoch 79/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0304 - accuracy: 0.7893 - val_loss: 0.0343 - val_accuracy: 0.7628\n",
            "Epoch 80/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0288 - accuracy: 0.7843 - val_loss: 0.0338 - val_accuracy: 0.7604\n",
            "Epoch 81/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0311 - accuracy: 0.7819 - val_loss: 0.0698 - val_accuracy: 0.5306\n",
            "==================================================\n",
            "Result of thin_resnet_model, fold 4\n",
            "Epoch: 81\n",
            "Accuracy: 0.8019559979438782\n",
            "Time taken:  162.74721932411194\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 6s 26ms/step - loss: 0.1020 - accuracy: 0.1211 - val_loss: 0.0919 - val_accuracy: 0.1149\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0928 - accuracy: 0.1807 - val_loss: 0.1081 - val_accuracy: 0.1076\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0905 - accuracy: 0.1972 - val_loss: 0.1562 - val_accuracy: 0.1076\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0898 - accuracy: 0.2360 - val_loss: 0.1599 - val_accuracy: 0.1076\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0871 - accuracy: 0.2701 - val_loss: 0.1193 - val_accuracy: 0.2176\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0869 - accuracy: 0.2924 - val_loss: 0.1133 - val_accuracy: 0.1687\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0854 - accuracy: 0.3022 - val_loss: 0.0925 - val_accuracy: 0.2347\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0861 - accuracy: 0.2880 - val_loss: 0.1313 - val_accuracy: 0.1418\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 2s 21ms/step - loss: 0.0836 - accuracy: 0.3228 - val_loss: 0.1676 - val_accuracy: 0.0733\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0855 - accuracy: 0.3113 - val_loss: 0.1456 - val_accuracy: 0.1125\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0825 - accuracy: 0.3447 - val_loss: 0.0947 - val_accuracy: 0.2592\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0824 - accuracy: 0.3399 - val_loss: 0.1202 - val_accuracy: 0.2225\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0817 - accuracy: 0.3555 - val_loss: 0.1793 - val_accuracy: 0.0856\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0816 - accuracy: 0.3442 - val_loss: 0.1081 - val_accuracy: 0.1638\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0812 - accuracy: 0.3482 - val_loss: 0.1144 - val_accuracy: 0.1247\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0809 - accuracy: 0.3450 - val_loss: 0.0849 - val_accuracy: 0.2518\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0815 - accuracy: 0.3439 - val_loss: 0.0760 - val_accuracy: 0.3863\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0822 - accuracy: 0.3302 - val_loss: 0.1258 - val_accuracy: 0.2200\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0780 - accuracy: 0.3943 - val_loss: 0.1187 - val_accuracy: 0.1345\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0789 - accuracy: 0.3936 - val_loss: 0.1076 - val_accuracy: 0.2103\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0771 - accuracy: 0.4146 - val_loss: 0.0990 - val_accuracy: 0.2298\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0780 - accuracy: 0.3732 - val_loss: 0.1332 - val_accuracy: 0.1467\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0824 - accuracy: 0.3479 - val_loss: 0.0911 - val_accuracy: 0.2445\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0740 - accuracy: 0.4243 - val_loss: 0.0792 - val_accuracy: 0.3863\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0750 - accuracy: 0.4088 - val_loss: 0.0717 - val_accuracy: 0.4450\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0745 - accuracy: 0.4201 - val_loss: 0.0847 - val_accuracy: 0.3814\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0720 - accuracy: 0.4586 - val_loss: 0.0727 - val_accuracy: 0.4425\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0698 - accuracy: 0.4528 - val_loss: 0.0737 - val_accuracy: 0.4523\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0647 - accuracy: 0.4991 - val_loss: 0.0697 - val_accuracy: 0.4572\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0691 - accuracy: 0.4651 - val_loss: 0.0879 - val_accuracy: 0.3399\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0696 - accuracy: 0.4701 - val_loss: 0.0722 - val_accuracy: 0.4963\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0665 - accuracy: 0.4964 - val_loss: 0.0607 - val_accuracy: 0.5452\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0659 - accuracy: 0.4908 - val_loss: 0.0951 - val_accuracy: 0.2836\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0631 - accuracy: 0.5224 - val_loss: 0.0809 - val_accuracy: 0.4059\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0637 - accuracy: 0.5220 - val_loss: 0.0690 - val_accuracy: 0.4645\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0617 - accuracy: 0.5230 - val_loss: 0.0956 - val_accuracy: 0.3570\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 2s 21ms/step - loss: 0.0587 - accuracy: 0.5704 - val_loss: 0.1042 - val_accuracy: 0.2469\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0617 - accuracy: 0.5429 - val_loss: 0.0993 - val_accuracy: 0.2567\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0586 - accuracy: 0.5760 - val_loss: 0.0784 - val_accuracy: 0.4059\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0574 - accuracy: 0.5846 - val_loss: 0.0537 - val_accuracy: 0.5966\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0562 - accuracy: 0.5887 - val_loss: 0.0687 - val_accuracy: 0.5012\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0548 - accuracy: 0.6054 - val_loss: 0.0860 - val_accuracy: 0.3716\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0559 - accuracy: 0.5861 - val_loss: 0.0525 - val_accuracy: 0.6210\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0545 - accuracy: 0.5987 - val_loss: 0.0896 - val_accuracy: 0.3276\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0535 - accuracy: 0.6108 - val_loss: 0.0505 - val_accuracy: 0.6259\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0506 - accuracy: 0.6382 - val_loss: 0.0477 - val_accuracy: 0.6479\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0503 - accuracy: 0.6240 - val_loss: 0.0530 - val_accuracy: 0.6015\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0488 - accuracy: 0.6342 - val_loss: 0.0480 - val_accuracy: 0.6553\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0484 - accuracy: 0.6498 - val_loss: 0.0722 - val_accuracy: 0.4377\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0495 - accuracy: 0.6272 - val_loss: 0.0455 - val_accuracy: 0.6870\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0446 - accuracy: 0.6918 - val_loss: 0.0505 - val_accuracy: 0.6406\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0450 - accuracy: 0.6773 - val_loss: 0.0497 - val_accuracy: 0.6430\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0463 - accuracy: 0.6662 - val_loss: 0.0536 - val_accuracy: 0.6259\n",
            "Epoch 54/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0430 - accuracy: 0.6932 - val_loss: 0.0489 - val_accuracy: 0.6430\n",
            "Epoch 55/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0445 - accuracy: 0.6658 - val_loss: 0.0726 - val_accuracy: 0.4963\n",
            "Epoch 56/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0445 - accuracy: 0.6802 - val_loss: 0.0589 - val_accuracy: 0.5648\n",
            "Epoch 57/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0413 - accuracy: 0.7023 - val_loss: 0.0422 - val_accuracy: 0.7213\n",
            "Epoch 58/300\n",
            "103/103 [==============================] - 2s 17ms/step - loss: 0.0393 - accuracy: 0.7227 - val_loss: 0.0522 - val_accuracy: 0.6015\n",
            "Epoch 59/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0405 - accuracy: 0.7113 - val_loss: 0.0742 - val_accuracy: 0.4694\n",
            "Epoch 60/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0391 - accuracy: 0.7152 - val_loss: 0.0493 - val_accuracy: 0.6601\n",
            "Epoch 61/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0381 - accuracy: 0.7300 - val_loss: 0.0606 - val_accuracy: 0.5648\n",
            "Epoch 62/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0397 - accuracy: 0.7202 - val_loss: 0.0373 - val_accuracy: 0.7604\n",
            "Epoch 63/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0331 - accuracy: 0.7701 - val_loss: 0.0433 - val_accuracy: 0.6968\n",
            "Epoch 64/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0408 - accuracy: 0.7029 - val_loss: 0.0514 - val_accuracy: 0.6406\n",
            "Epoch 65/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0390 - accuracy: 0.7181 - val_loss: 0.0515 - val_accuracy: 0.6430\n",
            "Epoch 66/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0365 - accuracy: 0.7385 - val_loss: 0.0413 - val_accuracy: 0.7237\n",
            "Epoch 67/300\n",
            "103/103 [==============================] - 2s 19ms/step - loss: 0.0354 - accuracy: 0.7551 - val_loss: 0.1193 - val_accuracy: 0.2396\n",
            "Epoch 68/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0341 - accuracy: 0.7578 - val_loss: 0.0413 - val_accuracy: 0.7164\n",
            "Epoch 69/300\n",
            "103/103 [==============================] - 2s 20ms/step - loss: 0.0332 - accuracy: 0.7590 - val_loss: 0.0401 - val_accuracy: 0.7311\n",
            "Epoch 70/300\n",
            "103/103 [==============================] - 2s 21ms/step - loss: 0.0354 - accuracy: 0.7569 - val_loss: 0.0530 - val_accuracy: 0.5941\n",
            "Epoch 71/300\n",
            "103/103 [==============================] - 2s 18ms/step - loss: 0.0327 - accuracy: 0.7700 - val_loss: 0.0394 - val_accuracy: 0.7311\n",
            "Epoch 72/300\n",
            "103/103 [==============================] - 2s 21ms/step - loss: 0.0327 - accuracy: 0.7750 - val_loss: 0.0550 - val_accuracy: 0.5844\n",
            "==================================================\n",
            "Result of thin_resnet_model, fold 5\n",
            "Epoch: 72\n",
            "Accuracy: 0.7603911757469177\n",
            "Time taken:  148.3608956336975\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 8s 59ms/step - loss: 0.0959 - accuracy: 0.1071 - val_loss: 0.1375 - val_accuracy: 0.1122\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 5s 46ms/step - loss: 0.0914 - accuracy: 0.1571 - val_loss: 0.1190 - val_accuracy: 0.1341\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 5s 46ms/step - loss: 0.0903 - accuracy: 0.1521 - val_loss: 0.0926 - val_accuracy: 0.0683\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0919 - accuracy: 0.1425 - val_loss: 0.0915 - val_accuracy: 0.1610\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0906 - accuracy: 0.1555 - val_loss: 0.0933 - val_accuracy: 0.0780\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0893 - accuracy: 0.1689 - val_loss: 0.1288 - val_accuracy: 0.1317\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0893 - accuracy: 0.1819 - val_loss: 0.1151 - val_accuracy: 0.1439\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0879 - accuracy: 0.2155 - val_loss: 0.1057 - val_accuracy: 0.1122\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0874 - accuracy: 0.2034 - val_loss: 0.0945 - val_accuracy: 0.0878\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0880 - accuracy: 0.1845 - val_loss: 0.0888 - val_accuracy: 0.2000\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0863 - accuracy: 0.2275 - val_loss: 0.0891 - val_accuracy: 0.1780\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0867 - accuracy: 0.2051 - val_loss: 0.0853 - val_accuracy: 0.2341\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0863 - accuracy: 0.2288 - val_loss: 0.1025 - val_accuracy: 0.1171\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0854 - accuracy: 0.2667 - val_loss: 0.0833 - val_accuracy: 0.2732\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 5s 46ms/step - loss: 0.0849 - accuracy: 0.2426 - val_loss: 0.0835 - val_accuracy: 0.2366\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0834 - accuracy: 0.2709 - val_loss: 0.0866 - val_accuracy: 0.2000\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0844 - accuracy: 0.2182 - val_loss: 0.0851 - val_accuracy: 0.2537\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0814 - accuracy: 0.2949 - val_loss: 0.0941 - val_accuracy: 0.1829\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0813 - accuracy: 0.2931 - val_loss: 0.0783 - val_accuracy: 0.3171\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0797 - accuracy: 0.3210 - val_loss: 0.0899 - val_accuracy: 0.1976\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 5s 46ms/step - loss: 0.0780 - accuracy: 0.3320 - val_loss: 0.0797 - val_accuracy: 0.3024\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0790 - accuracy: 0.3152 - val_loss: 0.1274 - val_accuracy: 0.1390\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0777 - accuracy: 0.3352 - val_loss: 0.0864 - val_accuracy: 0.2268\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0741 - accuracy: 0.3874 - val_loss: 0.0771 - val_accuracy: 0.3366\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0766 - accuracy: 0.3662 - val_loss: 0.0766 - val_accuracy: 0.3366\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0760 - accuracy: 0.3434 - val_loss: 0.0729 - val_accuracy: 0.3902\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0715 - accuracy: 0.4163 - val_loss: 0.0806 - val_accuracy: 0.3293\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0717 - accuracy: 0.4028 - val_loss: 0.0774 - val_accuracy: 0.3341\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0694 - accuracy: 0.4366 - val_loss: 0.0704 - val_accuracy: 0.4537\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0673 - accuracy: 0.4523 - val_loss: 0.0762 - val_accuracy: 0.3293\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0708 - accuracy: 0.4122 - val_loss: 0.0780 - val_accuracy: 0.3561\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0657 - accuracy: 0.4597 - val_loss: 0.0720 - val_accuracy: 0.3707\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0688 - accuracy: 0.4193 - val_loss: 0.0691 - val_accuracy: 0.4390\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0672 - accuracy: 0.4664 - val_loss: 0.0693 - val_accuracy: 0.4049\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0659 - accuracy: 0.4621 - val_loss: 0.0731 - val_accuracy: 0.3756\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0634 - accuracy: 0.4819 - val_loss: 0.0715 - val_accuracy: 0.4000\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0606 - accuracy: 0.5134 - val_loss: 0.0733 - val_accuracy: 0.3659\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0611 - accuracy: 0.5224 - val_loss: 0.0761 - val_accuracy: 0.3585\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0567 - accuracy: 0.5744 - val_loss: 0.0631 - val_accuracy: 0.4976\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0569 - accuracy: 0.5652 - val_loss: 0.0671 - val_accuracy: 0.4659\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 5s 46ms/step - loss: 0.0574 - accuracy: 0.5679 - val_loss: 0.0636 - val_accuracy: 0.5122\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0547 - accuracy: 0.6053 - val_loss: 0.0607 - val_accuracy: 0.5341\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0522 - accuracy: 0.6017 - val_loss: 0.0620 - val_accuracy: 0.5195\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0543 - accuracy: 0.5791 - val_loss: 0.0672 - val_accuracy: 0.4707\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0487 - accuracy: 0.6384 - val_loss: 0.0670 - val_accuracy: 0.4829\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0491 - accuracy: 0.6411 - val_loss: 0.0579 - val_accuracy: 0.5634\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0479 - accuracy: 0.6489 - val_loss: 0.0608 - val_accuracy: 0.5317\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0466 - accuracy: 0.6673 - val_loss: 0.0645 - val_accuracy: 0.5244\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0482 - accuracy: 0.6535 - val_loss: 0.0737 - val_accuracy: 0.4244\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0447 - accuracy: 0.6947 - val_loss: 0.0422 - val_accuracy: 0.6756\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0414 - accuracy: 0.7195 - val_loss: 0.0574 - val_accuracy: 0.5780\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0434 - accuracy: 0.6995 - val_loss: 0.0393 - val_accuracy: 0.7244\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0379 - accuracy: 0.7490 - val_loss: 0.0527 - val_accuracy: 0.6317\n",
            "Epoch 54/300\n",
            "103/103 [==============================] - 5s 46ms/step - loss: 0.0382 - accuracy: 0.7273 - val_loss: 0.0726 - val_accuracy: 0.4195\n",
            "Epoch 55/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0407 - accuracy: 0.7119 - val_loss: 0.0440 - val_accuracy: 0.6951\n",
            "Epoch 56/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0354 - accuracy: 0.7378 - val_loss: 0.0596 - val_accuracy: 0.5585\n",
            "Epoch 57/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0383 - accuracy: 0.7378 - val_loss: 0.0435 - val_accuracy: 0.6878\n",
            "Epoch 58/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0348 - accuracy: 0.7558 - val_loss: 0.0402 - val_accuracy: 0.7220\n",
            "Epoch 59/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0319 - accuracy: 0.7833 - val_loss: 0.0578 - val_accuracy: 0.5780\n",
            "Epoch 60/300\n",
            "103/103 [==============================] - 5s 46ms/step - loss: 0.0304 - accuracy: 0.7924 - val_loss: 0.0442 - val_accuracy: 0.6902\n",
            "Epoch 61/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0337 - accuracy: 0.7715 - val_loss: 0.0554 - val_accuracy: 0.5829\n",
            "Epoch 62/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0332 - accuracy: 0.7721 - val_loss: 0.0402 - val_accuracy: 0.7122\n",
            "==================================================\n",
            "Result of vggnet_model, fold 1\n",
            "Epoch: 62\n",
            "Accuracy: 0.7243902683258057\n",
            "Time taken:  313.39548110961914\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 7s 50ms/step - loss: 0.0963 - accuracy: 0.0935 - val_loss: 0.1006 - val_accuracy: 0.0976\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0955 - accuracy: 0.1087 - val_loss: 0.1534 - val_accuracy: 0.0878\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0940 - accuracy: 0.1130 - val_loss: 0.1166 - val_accuracy: 0.1024\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0917 - accuracy: 0.1321 - val_loss: 0.1798 - val_accuracy: 0.0927\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0897 - accuracy: 0.1655 - val_loss: 0.0885 - val_accuracy: 0.1732\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0880 - accuracy: 0.1972 - val_loss: 0.0931 - val_accuracy: 0.1585\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0876 - accuracy: 0.1994 - val_loss: 0.1299 - val_accuracy: 0.1512\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0861 - accuracy: 0.2155 - val_loss: 0.1449 - val_accuracy: 0.1122\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0851 - accuracy: 0.2477 - val_loss: 0.0889 - val_accuracy: 0.2854\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0863 - accuracy: 0.2126 - val_loss: 0.0847 - val_accuracy: 0.2610\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0848 - accuracy: 0.2439 - val_loss: 0.0840 - val_accuracy: 0.2317\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0846 - accuracy: 0.2462 - val_loss: 0.0928 - val_accuracy: 0.1537\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0847 - accuracy: 0.2664 - val_loss: 0.0939 - val_accuracy: 0.1878\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0827 - accuracy: 0.2702 - val_loss: 0.1174 - val_accuracy: 0.1829\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0817 - accuracy: 0.2797 - val_loss: 0.0836 - val_accuracy: 0.2488\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0819 - accuracy: 0.2578 - val_loss: 0.0928 - val_accuracy: 0.1756\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0810 - accuracy: 0.2630 - val_loss: 0.1059 - val_accuracy: 0.1854\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0804 - accuracy: 0.2891 - val_loss: 0.0852 - val_accuracy: 0.2049\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0808 - accuracy: 0.2985 - val_loss: 0.0917 - val_accuracy: 0.1951\n",
            "==================================================\n",
            "Result of vggnet_model, fold 2\n",
            "Epoch: 19\n",
            "Accuracy: 0.28536584973335266\n",
            "Time taken:  97.49620866775513\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 7s 50ms/step - loss: 0.0962 - accuracy: 0.0836 - val_loss: 0.0914 - val_accuracy: 0.1000\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0925 - accuracy: 0.1420 - val_loss: 0.0888 - val_accuracy: 0.1634\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0907 - accuracy: 0.1448 - val_loss: 0.0905 - val_accuracy: 0.1976\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0901 - accuracy: 0.1515 - val_loss: 0.0910 - val_accuracy: 0.0976\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0907 - accuracy: 0.1357 - val_loss: 0.0920 - val_accuracy: 0.1951\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0889 - accuracy: 0.2088 - val_loss: 0.0856 - val_accuracy: 0.2488\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0868 - accuracy: 0.2253 - val_loss: 0.0858 - val_accuracy: 0.2171\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0866 - accuracy: 0.2317 - val_loss: 0.0876 - val_accuracy: 0.2000\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0856 - accuracy: 0.2364 - val_loss: 0.0893 - val_accuracy: 0.2610\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0858 - accuracy: 0.2517 - val_loss: 0.0871 - val_accuracy: 0.1829\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0839 - accuracy: 0.2620 - val_loss: 0.1077 - val_accuracy: 0.1439\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0842 - accuracy: 0.2495 - val_loss: 0.0850 - val_accuracy: 0.2415\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0829 - accuracy: 0.2677 - val_loss: 0.0758 - val_accuracy: 0.3537\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0828 - accuracy: 0.2640 - val_loss: 0.0977 - val_accuracy: 0.1780\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0804 - accuracy: 0.3074 - val_loss: 0.0936 - val_accuracy: 0.2390\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0786 - accuracy: 0.3225 - val_loss: 0.0792 - val_accuracy: 0.2927\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0792 - accuracy: 0.3272 - val_loss: 0.0720 - val_accuracy: 0.4049\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0763 - accuracy: 0.3664 - val_loss: 0.0825 - val_accuracy: 0.3512\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0745 - accuracy: 0.3826 - val_loss: 0.0744 - val_accuracy: 0.3732\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0738 - accuracy: 0.4191 - val_loss: 0.0836 - val_accuracy: 0.3585\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0737 - accuracy: 0.3985 - val_loss: 0.0720 - val_accuracy: 0.3976\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0699 - accuracy: 0.4562 - val_loss: 0.0714 - val_accuracy: 0.3976\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0712 - accuracy: 0.4365 - val_loss: 0.0683 - val_accuracy: 0.4463\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0698 - accuracy: 0.4411 - val_loss: 0.0732 - val_accuracy: 0.3927\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0691 - accuracy: 0.4452 - val_loss: 0.0639 - val_accuracy: 0.4805\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0668 - accuracy: 0.4726 - val_loss: 0.0800 - val_accuracy: 0.3683\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0705 - accuracy: 0.4428 - val_loss: 0.0693 - val_accuracy: 0.4244\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0676 - accuracy: 0.4622 - val_loss: 0.0695 - val_accuracy: 0.4195\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0648 - accuracy: 0.4872 - val_loss: 0.0642 - val_accuracy: 0.5049\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0647 - accuracy: 0.4854 - val_loss: 0.0679 - val_accuracy: 0.4805\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0632 - accuracy: 0.5222 - val_loss: 0.0621 - val_accuracy: 0.4854\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 5s 46ms/step - loss: 0.0605 - accuracy: 0.5288 - val_loss: 0.0627 - val_accuracy: 0.5146\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0631 - accuracy: 0.5169 - val_loss: 0.0775 - val_accuracy: 0.3878\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0601 - accuracy: 0.5510 - val_loss: 0.0579 - val_accuracy: 0.5634\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0589 - accuracy: 0.5512 - val_loss: 0.0632 - val_accuracy: 0.5171\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0579 - accuracy: 0.5552 - val_loss: 0.0609 - val_accuracy: 0.5171\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0547 - accuracy: 0.6022 - val_loss: 0.0644 - val_accuracy: 0.5024\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0551 - accuracy: 0.5928 - val_loss: 0.0554 - val_accuracy: 0.5780\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0525 - accuracy: 0.6115 - val_loss: 0.0506 - val_accuracy: 0.6244\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0560 - accuracy: 0.5866 - val_loss: 0.0649 - val_accuracy: 0.5073\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0508 - accuracy: 0.6371 - val_loss: 0.0572 - val_accuracy: 0.5927\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0508 - accuracy: 0.6278 - val_loss: 0.0502 - val_accuracy: 0.6415\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0489 - accuracy: 0.6528 - val_loss: 0.0695 - val_accuracy: 0.4902\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0528 - accuracy: 0.6243 - val_loss: 0.0578 - val_accuracy: 0.5537\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0464 - accuracy: 0.6755 - val_loss: 0.0474 - val_accuracy: 0.6561\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0448 - accuracy: 0.6836 - val_loss: 0.0467 - val_accuracy: 0.6512\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0457 - accuracy: 0.6655 - val_loss: 0.0583 - val_accuracy: 0.5512\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0439 - accuracy: 0.6792 - val_loss: 0.0518 - val_accuracy: 0.5878\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0436 - accuracy: 0.6858 - val_loss: 0.0461 - val_accuracy: 0.6585\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0383 - accuracy: 0.7274 - val_loss: 0.0476 - val_accuracy: 0.6512\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0377 - accuracy: 0.7342 - val_loss: 0.0487 - val_accuracy: 0.6537\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0390 - accuracy: 0.7396 - val_loss: 0.0429 - val_accuracy: 0.6927\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0342 - accuracy: 0.7620 - val_loss: 0.0478 - val_accuracy: 0.6610\n",
            "Epoch 54/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0368 - accuracy: 0.7482 - val_loss: 0.0428 - val_accuracy: 0.7000\n",
            "Epoch 55/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0385 - accuracy: 0.7463 - val_loss: 0.0411 - val_accuracy: 0.7098\n",
            "Epoch 56/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0360 - accuracy: 0.7565 - val_loss: 0.0390 - val_accuracy: 0.7415\n",
            "Epoch 57/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0347 - accuracy: 0.7615 - val_loss: 0.0377 - val_accuracy: 0.7268\n",
            "Epoch 58/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0340 - accuracy: 0.7665 - val_loss: 0.0415 - val_accuracy: 0.6927\n",
            "Epoch 59/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0316 - accuracy: 0.7898 - val_loss: 0.0450 - val_accuracy: 0.6780\n",
            "Epoch 60/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0321 - accuracy: 0.7804 - val_loss: 0.0380 - val_accuracy: 0.7512\n",
            "Epoch 61/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0284 - accuracy: 0.8063 - val_loss: 0.0383 - val_accuracy: 0.7171\n",
            "Epoch 62/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0269 - accuracy: 0.8196 - val_loss: 0.0379 - val_accuracy: 0.7415\n",
            "Epoch 63/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0289 - accuracy: 0.7980 - val_loss: 0.0345 - val_accuracy: 0.7561\n",
            "Epoch 64/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0240 - accuracy: 0.8442 - val_loss: 0.0351 - val_accuracy: 0.7561\n",
            "Epoch 65/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0292 - accuracy: 0.8021 - val_loss: 0.0378 - val_accuracy: 0.7488\n",
            "Epoch 66/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0276 - accuracy: 0.8138 - val_loss: 0.0390 - val_accuracy: 0.7220\n",
            "Epoch 67/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0232 - accuracy: 0.8413 - val_loss: 0.0305 - val_accuracy: 0.7927\n",
            "Epoch 68/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0230 - accuracy: 0.8418 - val_loss: 0.0365 - val_accuracy: 0.7488\n",
            "Epoch 69/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0227 - accuracy: 0.8479 - val_loss: 0.0465 - val_accuracy: 0.6756\n",
            "Epoch 70/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0220 - accuracy: 0.8498 - val_loss: 0.0334 - val_accuracy: 0.7683\n",
            "Epoch 71/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0191 - accuracy: 0.8725 - val_loss: 0.0315 - val_accuracy: 0.7780\n",
            "Epoch 72/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0195 - accuracy: 0.8672 - val_loss: 0.0425 - val_accuracy: 0.7171\n",
            "Epoch 73/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0189 - accuracy: 0.8763 - val_loss: 0.0294 - val_accuracy: 0.7976\n",
            "Epoch 74/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0177 - accuracy: 0.8785 - val_loss: 0.0335 - val_accuracy: 0.7829\n",
            "Epoch 75/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0185 - accuracy: 0.8757 - val_loss: 0.0310 - val_accuracy: 0.7878\n",
            "Epoch 76/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0187 - accuracy: 0.8748 - val_loss: 0.0295 - val_accuracy: 0.7976\n",
            "Epoch 77/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0172 - accuracy: 0.8904 - val_loss: 0.0303 - val_accuracy: 0.7927\n",
            "Epoch 78/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0155 - accuracy: 0.8952 - val_loss: 0.0320 - val_accuracy: 0.7707\n",
            "Epoch 79/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0166 - accuracy: 0.8937 - val_loss: 0.0304 - val_accuracy: 0.7951\n",
            "Epoch 80/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0150 - accuracy: 0.9060 - val_loss: 0.0321 - val_accuracy: 0.7878\n",
            "Epoch 81/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0181 - accuracy: 0.8817 - val_loss: 0.0302 - val_accuracy: 0.7854\n",
            "Epoch 82/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0171 - accuracy: 0.9021 - val_loss: 0.0306 - val_accuracy: 0.7927\n",
            "Epoch 83/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0169 - accuracy: 0.8978 - val_loss: 0.0326 - val_accuracy: 0.7951\n",
            "==================================================\n",
            "Result of vggnet_model, fold 3\n",
            "Epoch: 83\n",
            "Accuracy: 0.79756098985672\n",
            "Time taken:  424.95081639289856\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 8s 65ms/step - loss: 0.0949 - accuracy: 0.1142 - val_loss: 0.1146 - val_accuracy: 0.1320\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0911 - accuracy: 0.1323 - val_loss: 0.0884 - val_accuracy: 0.1663\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0911 - accuracy: 0.1515 - val_loss: 0.1094 - val_accuracy: 0.1198\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0908 - accuracy: 0.1430 - val_loss: 0.0933 - val_accuracy: 0.1516\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0904 - accuracy: 0.1606 - val_loss: 0.0968 - val_accuracy: 0.1491\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0894 - accuracy: 0.1531 - val_loss: 0.0963 - val_accuracy: 0.1247\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0881 - accuracy: 0.2011 - val_loss: 0.0945 - val_accuracy: 0.1418\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0894 - accuracy: 0.1616 - val_loss: 0.0908 - val_accuracy: 0.1711\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0885 - accuracy: 0.1938 - val_loss: 0.1148 - val_accuracy: 0.0782\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0883 - accuracy: 0.1882 - val_loss: 0.0880 - val_accuracy: 0.1491\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0866 - accuracy: 0.2119 - val_loss: 0.0865 - val_accuracy: 0.1883\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0854 - accuracy: 0.2481 - val_loss: 0.0968 - val_accuracy: 0.1589\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0846 - accuracy: 0.2546 - val_loss: 0.0918 - val_accuracy: 0.2274\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0842 - accuracy: 0.2433 - val_loss: 0.0878 - val_accuracy: 0.2054\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0849 - accuracy: 0.2400 - val_loss: 0.0884 - val_accuracy: 0.2152\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0833 - accuracy: 0.2541 - val_loss: 0.0806 - val_accuracy: 0.2616\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0803 - accuracy: 0.3197 - val_loss: 0.0808 - val_accuracy: 0.2738\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0812 - accuracy: 0.2964 - val_loss: 0.0976 - val_accuracy: 0.2176\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0809 - accuracy: 0.2930 - val_loss: 0.0837 - val_accuracy: 0.3056\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0799 - accuracy: 0.3214 - val_loss: 0.0809 - val_accuracy: 0.2910\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0776 - accuracy: 0.3517 - val_loss: 0.0783 - val_accuracy: 0.3252\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0762 - accuracy: 0.3569 - val_loss: 0.0758 - val_accuracy: 0.3667\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0732 - accuracy: 0.3972 - val_loss: 0.0914 - val_accuracy: 0.1736\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0764 - accuracy: 0.3652 - val_loss: 0.0794 - val_accuracy: 0.3178\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0708 - accuracy: 0.4322 - val_loss: 0.0750 - val_accuracy: 0.4108\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0723 - accuracy: 0.4164 - val_loss: 0.0701 - val_accuracy: 0.4328\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0686 - accuracy: 0.4600 - val_loss: 0.0677 - val_accuracy: 0.4450\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0686 - accuracy: 0.4612 - val_loss: 0.0677 - val_accuracy: 0.4670\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0678 - accuracy: 0.4568 - val_loss: 0.0584 - val_accuracy: 0.5697\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0674 - accuracy: 0.4743 - val_loss: 0.0677 - val_accuracy: 0.4670\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0663 - accuracy: 0.4789 - val_loss: 0.0871 - val_accuracy: 0.2249\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0674 - accuracy: 0.4975 - val_loss: 0.0680 - val_accuracy: 0.4792\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0665 - accuracy: 0.4697 - val_loss: 0.0674 - val_accuracy: 0.4401\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0670 - accuracy: 0.4670 - val_loss: 0.0666 - val_accuracy: 0.4645\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0645 - accuracy: 0.5018 - val_loss: 0.0627 - val_accuracy: 0.4914\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0617 - accuracy: 0.5359 - val_loss: 0.0655 - val_accuracy: 0.4792\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0590 - accuracy: 0.5486 - val_loss: 0.0770 - val_accuracy: 0.4328\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0590 - accuracy: 0.5432 - val_loss: 0.0693 - val_accuracy: 0.4743\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0641 - accuracy: 0.4959 - val_loss: 0.0644 - val_accuracy: 0.4743\n",
            "==================================================\n",
            "Result of vggnet_model, fold 4\n",
            "Epoch: 39\n",
            "Accuracy: 0.5696821808815002\n",
            "Time taken:  204.2816867828369\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 7s 50ms/step - loss: 0.0970 - accuracy: 0.1104 - val_loss: 0.1536 - val_accuracy: 0.0905\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0958 - accuracy: 0.0961 - val_loss: 0.1017 - val_accuracy: 0.1076\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0921 - accuracy: 0.1205 - val_loss: 0.0926 - val_accuracy: 0.1051\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0913 - accuracy: 0.1496 - val_loss: 0.0947 - val_accuracy: 0.1320\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0900 - accuracy: 0.1765 - val_loss: 0.0890 - val_accuracy: 0.1687\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0894 - accuracy: 0.1786 - val_loss: 0.0917 - val_accuracy: 0.1760\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0879 - accuracy: 0.1925 - val_loss: 0.0884 - val_accuracy: 0.1883\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0869 - accuracy: 0.2234 - val_loss: 0.1066 - val_accuracy: 0.1076\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0885 - accuracy: 0.1936 - val_loss: 0.0999 - val_accuracy: 0.1198\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0870 - accuracy: 0.2008 - val_loss: 0.0941 - val_accuracy: 0.1198\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0860 - accuracy: 0.2160 - val_loss: 0.0851 - val_accuracy: 0.2738\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0852 - accuracy: 0.2467 - val_loss: 0.0839 - val_accuracy: 0.2714\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0850 - accuracy: 0.2436 - val_loss: 0.0960 - val_accuracy: 0.1369\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0838 - accuracy: 0.2608 - val_loss: 0.0846 - val_accuracy: 0.2372\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0831 - accuracy: 0.2725 - val_loss: 0.0925 - val_accuracy: 0.2347\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0823 - accuracy: 0.3022 - val_loss: 0.0791 - val_accuracy: 0.3423\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0794 - accuracy: 0.3257 - val_loss: 0.0815 - val_accuracy: 0.2836\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0765 - accuracy: 0.3680 - val_loss: 0.0758 - val_accuracy: 0.3130\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0747 - accuracy: 0.3778 - val_loss: 0.0764 - val_accuracy: 0.3545\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0743 - accuracy: 0.3820 - val_loss: 0.0757 - val_accuracy: 0.3643\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0746 - accuracy: 0.3699 - val_loss: 0.0715 - val_accuracy: 0.3936\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 5s 46ms/step - loss: 0.0726 - accuracy: 0.3957 - val_loss: 0.0721 - val_accuracy: 0.3716\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0697 - accuracy: 0.4197 - val_loss: 0.0752 - val_accuracy: 0.3790\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0718 - accuracy: 0.4018 - val_loss: 0.0669 - val_accuracy: 0.4499\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0676 - accuracy: 0.4631 - val_loss: 0.0705 - val_accuracy: 0.3765\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0662 - accuracy: 0.4644 - val_loss: 0.0674 - val_accuracy: 0.4548\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0658 - accuracy: 0.4812 - val_loss: 0.0703 - val_accuracy: 0.4132\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0633 - accuracy: 0.4983 - val_loss: 0.1128 - val_accuracy: 0.1516\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0681 - accuracy: 0.4579 - val_loss: 0.0674 - val_accuracy: 0.4401\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0605 - accuracy: 0.5337 - val_loss: 0.0707 - val_accuracy: 0.3814\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0625 - accuracy: 0.5059 - val_loss: 0.0658 - val_accuracy: 0.4817\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0621 - accuracy: 0.5274 - val_loss: 0.0648 - val_accuracy: 0.4841\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0646 - accuracy: 0.5031 - val_loss: 0.0629 - val_accuracy: 0.5037\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0597 - accuracy: 0.5417 - val_loss: 0.0625 - val_accuracy: 0.4743\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0589 - accuracy: 0.5614 - val_loss: 0.0643 - val_accuracy: 0.5086\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0560 - accuracy: 0.5718 - val_loss: 0.0581 - val_accuracy: 0.5623\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0558 - accuracy: 0.5822 - val_loss: 0.0609 - val_accuracy: 0.5452\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0566 - accuracy: 0.5617 - val_loss: 0.0554 - val_accuracy: 0.5575\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0540 - accuracy: 0.5967 - val_loss: 0.0657 - val_accuracy: 0.4866\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0504 - accuracy: 0.6242 - val_loss: 0.0621 - val_accuracy: 0.5159\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0493 - accuracy: 0.6434 - val_loss: 0.0589 - val_accuracy: 0.5501\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0477 - accuracy: 0.6519 - val_loss: 0.0670 - val_accuracy: 0.4719\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0523 - accuracy: 0.6223 - val_loss: 0.0654 - val_accuracy: 0.5037\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0474 - accuracy: 0.6608 - val_loss: 0.0527 - val_accuracy: 0.6284\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0470 - accuracy: 0.6658 - val_loss: 0.0561 - val_accuracy: 0.5819\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0459 - accuracy: 0.6804 - val_loss: 0.0561 - val_accuracy: 0.6039\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0420 - accuracy: 0.7010 - val_loss: 0.0547 - val_accuracy: 0.5941\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0466 - accuracy: 0.6776 - val_loss: 0.0587 - val_accuracy: 0.5575\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0445 - accuracy: 0.6932 - val_loss: 0.0627 - val_accuracy: 0.5257\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0416 - accuracy: 0.7070 - val_loss: 0.0526 - val_accuracy: 0.6039\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 5s 46ms/step - loss: 0.0387 - accuracy: 0.7326 - val_loss: 0.0619 - val_accuracy: 0.5599\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0393 - accuracy: 0.7142 - val_loss: 0.0588 - val_accuracy: 0.5746\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0405 - accuracy: 0.7192 - val_loss: 0.0452 - val_accuracy: 0.6748\n",
            "Epoch 54/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0357 - accuracy: 0.7679 - val_loss: 0.0551 - val_accuracy: 0.6088\n",
            "Epoch 55/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0396 - accuracy: 0.7158 - val_loss: 0.0554 - val_accuracy: 0.5966\n",
            "Epoch 56/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0334 - accuracy: 0.7821 - val_loss: 0.0619 - val_accuracy: 0.5672\n",
            "Epoch 57/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0386 - accuracy: 0.7430 - val_loss: 0.0420 - val_accuracy: 0.7090\n",
            "Epoch 58/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0326 - accuracy: 0.7862 - val_loss: 0.0630 - val_accuracy: 0.5477\n",
            "Epoch 59/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0344 - accuracy: 0.7537 - val_loss: 0.0510 - val_accuracy: 0.6333\n",
            "Epoch 60/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0346 - accuracy: 0.7522 - val_loss: 0.0586 - val_accuracy: 0.5746\n",
            "Epoch 61/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0290 - accuracy: 0.8006 - val_loss: 0.0459 - val_accuracy: 0.6699\n",
            "Epoch 62/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0336 - accuracy: 0.7678 - val_loss: 0.0493 - val_accuracy: 0.6430\n",
            "Epoch 63/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0317 - accuracy: 0.7935 - val_loss: 0.0503 - val_accuracy: 0.6357\n",
            "Epoch 64/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0290 - accuracy: 0.8056 - val_loss: 0.0496 - val_accuracy: 0.6284\n",
            "Epoch 65/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0292 - accuracy: 0.7981 - val_loss: 0.0450 - val_accuracy: 0.6870\n",
            "Epoch 66/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0290 - accuracy: 0.8000 - val_loss: 0.0521 - val_accuracy: 0.6406\n",
            "Epoch 67/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0266 - accuracy: 0.8184 - val_loss: 0.0408 - val_accuracy: 0.7237\n",
            "Epoch 68/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0237 - accuracy: 0.8433 - val_loss: 0.0410 - val_accuracy: 0.7262\n",
            "Epoch 69/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0248 - accuracy: 0.8367 - val_loss: 0.0446 - val_accuracy: 0.6895\n",
            "Epoch 70/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0261 - accuracy: 0.8275 - val_loss: 0.0437 - val_accuracy: 0.7042\n",
            "Epoch 71/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0227 - accuracy: 0.8432 - val_loss: 0.0394 - val_accuracy: 0.7335\n",
            "Epoch 72/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0277 - accuracy: 0.8011 - val_loss: 0.0408 - val_accuracy: 0.7115\n",
            "Epoch 73/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0232 - accuracy: 0.8528 - val_loss: 0.0387 - val_accuracy: 0.7286\n",
            "Epoch 74/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0246 - accuracy: 0.8290 - val_loss: 0.0394 - val_accuracy: 0.7311\n",
            "Epoch 75/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0229 - accuracy: 0.8432 - val_loss: 0.0379 - val_accuracy: 0.7506\n",
            "Epoch 76/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0228 - accuracy: 0.8513 - val_loss: 0.0397 - val_accuracy: 0.7384\n",
            "Epoch 77/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0204 - accuracy: 0.8663 - val_loss: 0.0403 - val_accuracy: 0.7262\n",
            "Epoch 78/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0209 - accuracy: 0.8653 - val_loss: 0.0462 - val_accuracy: 0.6895\n",
            "Epoch 79/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0237 - accuracy: 0.8347 - val_loss: 0.0349 - val_accuracy: 0.7628\n",
            "Epoch 80/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0207 - accuracy: 0.8617 - val_loss: 0.0380 - val_accuracy: 0.7457\n",
            "Epoch 81/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0209 - accuracy: 0.8584 - val_loss: 0.0354 - val_accuracy: 0.7702\n",
            "Epoch 82/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0154 - accuracy: 0.8917 - val_loss: 0.0402 - val_accuracy: 0.7164\n",
            "Epoch 83/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0188 - accuracy: 0.8802 - val_loss: 0.0383 - val_accuracy: 0.7457\n",
            "Epoch 84/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0219 - accuracy: 0.8520 - val_loss: 0.0354 - val_accuracy: 0.7555\n",
            "Epoch 85/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0218 - accuracy: 0.8537 - val_loss: 0.0374 - val_accuracy: 0.7579\n",
            "Epoch 86/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0180 - accuracy: 0.8840 - val_loss: 0.0433 - val_accuracy: 0.7042\n",
            "Epoch 87/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0193 - accuracy: 0.8653 - val_loss: 0.0364 - val_accuracy: 0.7482\n",
            "Epoch 88/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0180 - accuracy: 0.8786 - val_loss: 0.0385 - val_accuracy: 0.7457\n",
            "Epoch 89/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0175 - accuracy: 0.8850 - val_loss: 0.0384 - val_accuracy: 0.7262\n",
            "Epoch 90/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0169 - accuracy: 0.8876 - val_loss: 0.0330 - val_accuracy: 0.7800\n",
            "Epoch 91/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0143 - accuracy: 0.9013 - val_loss: 0.0364 - val_accuracy: 0.7702\n",
            "Epoch 92/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0200 - accuracy: 0.8660 - val_loss: 0.0399 - val_accuracy: 0.7286\n",
            "Epoch 93/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0161 - accuracy: 0.8936 - val_loss: 0.0377 - val_accuracy: 0.7482\n",
            "Epoch 94/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0165 - accuracy: 0.8895 - val_loss: 0.0407 - val_accuracy: 0.7188\n",
            "Epoch 95/300\n",
            "103/103 [==============================] - 5s 48ms/step - loss: 0.0146 - accuracy: 0.9085 - val_loss: 0.0359 - val_accuracy: 0.7604\n",
            "Epoch 96/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0167 - accuracy: 0.8959 - val_loss: 0.0352 - val_accuracy: 0.7751\n",
            "Epoch 97/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0118 - accuracy: 0.9270 - val_loss: 0.0338 - val_accuracy: 0.7726\n",
            "Epoch 98/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0156 - accuracy: 0.8911 - val_loss: 0.0318 - val_accuracy: 0.7702\n",
            "Epoch 99/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0125 - accuracy: 0.9135 - val_loss: 0.0356 - val_accuracy: 0.7677\n",
            "Epoch 100/300\n",
            "103/103 [==============================] - 5s 47ms/step - loss: 0.0112 - accuracy: 0.9282 - val_loss: 0.0437 - val_accuracy: 0.7042\n",
            "==================================================\n",
            "Result of vggnet_model, fold 5\n",
            "Epoch: 100\n",
            "Accuracy: 0.7799510955810547\n",
            "Time taken:  508.9638526439667\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 23s 115ms/step - loss: 0.0961 - accuracy: 0.1146 - val_loss: 0.0953 - val_accuracy: 0.1073\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0923 - accuracy: 0.1237 - val_loss: 0.1084 - val_accuracy: 0.1073\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0905 - accuracy: 0.1587 - val_loss: 0.0942 - val_accuracy: 0.1049\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0905 - accuracy: 0.1776 - val_loss: 0.0911 - val_accuracy: 0.1415\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0880 - accuracy: 0.2109 - val_loss: 0.0960 - val_accuracy: 0.1341\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 8s 81ms/step - loss: 0.0879 - accuracy: 0.2318 - val_loss: 0.0865 - val_accuracy: 0.2146\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0887 - accuracy: 0.2214 - val_loss: 0.1099 - val_accuracy: 0.1805\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0856 - accuracy: 0.2466 - val_loss: 0.1052 - val_accuracy: 0.1561\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 8s 80ms/step - loss: 0.0843 - accuracy: 0.2515 - val_loss: 0.1810 - val_accuracy: 0.0951\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0848 - accuracy: 0.2639 - val_loss: 0.1321 - val_accuracy: 0.1732\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0838 - accuracy: 0.2816 - val_loss: 0.0817 - val_accuracy: 0.3195\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0823 - accuracy: 0.2987 - val_loss: 0.0867 - val_accuracy: 0.2341\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0794 - accuracy: 0.3360 - val_loss: 0.0851 - val_accuracy: 0.3122\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0798 - accuracy: 0.3398 - val_loss: 0.0948 - val_accuracy: 0.2244\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0815 - accuracy: 0.3244 - val_loss: 0.0781 - val_accuracy: 0.3512\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0785 - accuracy: 0.3515 - val_loss: 0.0958 - val_accuracy: 0.1439\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0801 - accuracy: 0.3105 - val_loss: 0.1083 - val_accuracy: 0.2780\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0759 - accuracy: 0.3827 - val_loss: 0.0943 - val_accuracy: 0.1610\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0752 - accuracy: 0.3974 - val_loss: 0.0973 - val_accuracy: 0.1732\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0739 - accuracy: 0.4096 - val_loss: 0.0778 - val_accuracy: 0.3902\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0696 - accuracy: 0.4584 - val_loss: 0.0789 - val_accuracy: 0.3902\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0697 - accuracy: 0.4559 - val_loss: 0.0971 - val_accuracy: 0.1927\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0667 - accuracy: 0.4695 - val_loss: 0.1179 - val_accuracy: 0.2561\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0658 - accuracy: 0.4885 - val_loss: 0.1181 - val_accuracy: 0.2024\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0649 - accuracy: 0.5113 - val_loss: 0.0732 - val_accuracy: 0.4049\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0604 - accuracy: 0.5615 - val_loss: 0.1053 - val_accuracy: 0.1756\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0633 - accuracy: 0.5279 - val_loss: 0.1015 - val_accuracy: 0.2244\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 8s 80ms/step - loss: 0.0579 - accuracy: 0.5733 - val_loss: 0.0882 - val_accuracy: 0.3195\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0587 - accuracy: 0.5725 - val_loss: 0.1394 - val_accuracy: 0.1439\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0583 - accuracy: 0.5581 - val_loss: 0.0642 - val_accuracy: 0.5098\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0532 - accuracy: 0.6131 - val_loss: 0.1370 - val_accuracy: 0.1293\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0562 - accuracy: 0.5867 - val_loss: 0.0788 - val_accuracy: 0.4049\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0520 - accuracy: 0.6206 - val_loss: 0.1362 - val_accuracy: 0.1634\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0501 - accuracy: 0.6402 - val_loss: 0.0755 - val_accuracy: 0.5024\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0462 - accuracy: 0.6655 - val_loss: 0.1171 - val_accuracy: 0.2268\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 8s 81ms/step - loss: 0.0483 - accuracy: 0.6476 - val_loss: 0.1485 - val_accuracy: 0.1439\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 8s 81ms/step - loss: 0.0498 - accuracy: 0.6539 - val_loss: 0.0791 - val_accuracy: 0.4098\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0450 - accuracy: 0.6932 - val_loss: 0.1355 - val_accuracy: 0.1561\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 8s 81ms/step - loss: 0.0436 - accuracy: 0.6988 - val_loss: 0.0604 - val_accuracy: 0.6073\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 9s 82ms/step - loss: 0.0488 - accuracy: 0.6497 - val_loss: 0.0646 - val_accuracy: 0.5146\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0443 - accuracy: 0.6881 - val_loss: 0.0576 - val_accuracy: 0.5878\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0434 - accuracy: 0.6950 - val_loss: 0.0545 - val_accuracy: 0.5951\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0477 - accuracy: 0.6584 - val_loss: 0.0711 - val_accuracy: 0.4829\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0386 - accuracy: 0.7414 - val_loss: 0.1028 - val_accuracy: 0.2098\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0396 - accuracy: 0.7241 - val_loss: 0.1084 - val_accuracy: 0.2317\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0400 - accuracy: 0.7139 - val_loss: 0.0657 - val_accuracy: 0.5122\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0382 - accuracy: 0.7240 - val_loss: 0.0418 - val_accuracy: 0.7000\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0371 - accuracy: 0.7265 - val_loss: 0.0428 - val_accuracy: 0.7024\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0371 - accuracy: 0.7484 - val_loss: 0.0451 - val_accuracy: 0.6902\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0384 - accuracy: 0.7251 - val_loss: 0.0613 - val_accuracy: 0.5659\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0337 - accuracy: 0.7764 - val_loss: 0.0428 - val_accuracy: 0.7049\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0363 - accuracy: 0.7508 - val_loss: 0.0671 - val_accuracy: 0.4780\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 8s 83ms/step - loss: 0.0337 - accuracy: 0.7610 - val_loss: 0.0472 - val_accuracy: 0.6659\n",
            "Epoch 54/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0311 - accuracy: 0.7819 - val_loss: 0.0424 - val_accuracy: 0.7220\n",
            "Epoch 55/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0345 - accuracy: 0.7607 - val_loss: 0.0408 - val_accuracy: 0.7073\n",
            "Epoch 56/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0284 - accuracy: 0.8043 - val_loss: 0.0399 - val_accuracy: 0.7439\n",
            "Epoch 57/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0264 - accuracy: 0.8345 - val_loss: 0.0439 - val_accuracy: 0.6951\n",
            "Epoch 58/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0331 - accuracy: 0.7781 - val_loss: 0.0467 - val_accuracy: 0.6829\n",
            "Epoch 59/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0302 - accuracy: 0.8002 - val_loss: 0.0812 - val_accuracy: 0.3951\n",
            "Epoch 60/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0338 - accuracy: 0.7738 - val_loss: 0.0470 - val_accuracy: 0.6634\n",
            "Epoch 61/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0267 - accuracy: 0.8163 - val_loss: 0.0934 - val_accuracy: 0.2976\n",
            "Epoch 62/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0276 - accuracy: 0.8153 - val_loss: 0.0394 - val_accuracy: 0.7341\n",
            "Epoch 63/300\n",
            "103/103 [==============================] - 8s 83ms/step - loss: 0.0302 - accuracy: 0.7929 - val_loss: 0.0702 - val_accuracy: 0.4976\n",
            "Epoch 64/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0298 - accuracy: 0.8007 - val_loss: 0.0469 - val_accuracy: 0.6683\n",
            "Epoch 65/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0256 - accuracy: 0.8288 - val_loss: 0.0912 - val_accuracy: 0.3439\n",
            "Epoch 66/300\n",
            "103/103 [==============================] - 8s 81ms/step - loss: 0.0223 - accuracy: 0.8445 - val_loss: 0.0387 - val_accuracy: 0.7293\n",
            "==================================================\n",
            "Result of resnet_model, fold 1\n",
            "Epoch: 66\n",
            "Accuracy: 0.7439024448394775\n",
            "Time taken:  618.9456174373627\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 20s 100ms/step - loss: 0.0945 - accuracy: 0.1055 - val_loss: 0.0924 - val_accuracy: 0.0927\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0916 - accuracy: 0.1418 - val_loss: 0.0940 - val_accuracy: 0.0878\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0907 - accuracy: 0.1737 - val_loss: 0.1054 - val_accuracy: 0.0878\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0898 - accuracy: 0.1863 - val_loss: 0.1025 - val_accuracy: 0.0976\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0880 - accuracy: 0.2045 - val_loss: 0.0902 - val_accuracy: 0.2707\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0865 - accuracy: 0.2383 - val_loss: 0.0853 - val_accuracy: 0.2634\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0843 - accuracy: 0.2806 - val_loss: 0.1064 - val_accuracy: 0.2098\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 8s 83ms/step - loss: 0.0793 - accuracy: 0.3130 - val_loss: 0.1796 - val_accuracy: 0.0902\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0826 - accuracy: 0.2951 - val_loss: 0.1311 - val_accuracy: 0.1732\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0798 - accuracy: 0.3431 - val_loss: 0.1104 - val_accuracy: 0.2683\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0787 - accuracy: 0.3589 - val_loss: 0.1425 - val_accuracy: 0.1439\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0744 - accuracy: 0.3873 - val_loss: 0.1038 - val_accuracy: 0.2951\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0726 - accuracy: 0.4214 - val_loss: 0.0805 - val_accuracy: 0.3098\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0728 - accuracy: 0.4057 - val_loss: 0.1164 - val_accuracy: 0.1585\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0697 - accuracy: 0.4474 - val_loss: 0.0993 - val_accuracy: 0.1146\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0707 - accuracy: 0.4317 - val_loss: 0.0961 - val_accuracy: 0.2805\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0713 - accuracy: 0.4348 - val_loss: 0.1014 - val_accuracy: 0.1854\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0642 - accuracy: 0.5064 - val_loss: 0.0757 - val_accuracy: 0.4585\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0668 - accuracy: 0.5025 - val_loss: 0.1255 - val_accuracy: 0.2024\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0649 - accuracy: 0.5182 - val_loss: 0.0929 - val_accuracy: 0.3122\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0607 - accuracy: 0.5506 - val_loss: 0.0740 - val_accuracy: 0.3976\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0574 - accuracy: 0.5813 - val_loss: 0.1494 - val_accuracy: 0.1512\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0603 - accuracy: 0.5729 - val_loss: 0.0884 - val_accuracy: 0.3561\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 8s 81ms/step - loss: 0.0623 - accuracy: 0.5507 - val_loss: 0.0713 - val_accuracy: 0.4756\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0515 - accuracy: 0.6448 - val_loss: 0.1136 - val_accuracy: 0.2659\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0518 - accuracy: 0.6133 - val_loss: 0.0623 - val_accuracy: 0.5366\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0546 - accuracy: 0.6077 - val_loss: 0.0617 - val_accuracy: 0.5902\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0507 - accuracy: 0.6409 - val_loss: 0.1323 - val_accuracy: 0.1341\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0473 - accuracy: 0.6723 - val_loss: 0.0772 - val_accuracy: 0.4171\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0501 - accuracy: 0.6521 - val_loss: 0.1534 - val_accuracy: 0.1293\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0467 - accuracy: 0.6763 - val_loss: 0.0937 - val_accuracy: 0.2366\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0421 - accuracy: 0.6952 - val_loss: 0.1012 - val_accuracy: 0.2707\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0482 - accuracy: 0.6487 - val_loss: 0.1003 - val_accuracy: 0.2585\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0437 - accuracy: 0.7003 - val_loss: 0.0702 - val_accuracy: 0.4951\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0440 - accuracy: 0.7025 - val_loss: 0.1314 - val_accuracy: 0.1927\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0382 - accuracy: 0.7416 - val_loss: 0.0961 - val_accuracy: 0.3341\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0389 - accuracy: 0.7311 - val_loss: 0.0842 - val_accuracy: 0.4293\n",
            "==================================================\n",
            "Result of resnet_model, fold 2\n",
            "Epoch: 37\n",
            "Accuracy: 0.5902438759803772\n",
            "Time taken:  359.0955080986023\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 20s 99ms/step - loss: 0.0960 - accuracy: 0.0997 - val_loss: 0.0923 - val_accuracy: 0.1000\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0915 - accuracy: 0.1429 - val_loss: 0.0918 - val_accuracy: 0.1098\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0909 - accuracy: 0.1598 - val_loss: 0.0947 - val_accuracy: 0.0976\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 8s 83ms/step - loss: 0.0896 - accuracy: 0.1879 - val_loss: 0.0974 - val_accuracy: 0.0976\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0885 - accuracy: 0.2032 - val_loss: 0.0907 - val_accuracy: 0.2024\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0841 - accuracy: 0.2696 - val_loss: 0.1232 - val_accuracy: 0.1195\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0859 - accuracy: 0.2629 - val_loss: 0.0826 - val_accuracy: 0.3049\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0840 - accuracy: 0.2826 - val_loss: 0.0920 - val_accuracy: 0.2512\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0776 - accuracy: 0.3556 - val_loss: 0.0806 - val_accuracy: 0.3512\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0809 - accuracy: 0.3203 - val_loss: 0.0838 - val_accuracy: 0.2488\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0799 - accuracy: 0.3236 - val_loss: 0.0806 - val_accuracy: 0.3366\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0755 - accuracy: 0.3940 - val_loss: 0.0911 - val_accuracy: 0.3537\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0777 - accuracy: 0.3535 - val_loss: 0.1224 - val_accuracy: 0.1171\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0754 - accuracy: 0.3920 - val_loss: 0.0964 - val_accuracy: 0.2780\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0719 - accuracy: 0.4321 - val_loss: 0.1022 - val_accuracy: 0.1732\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0669 - accuracy: 0.4748 - val_loss: 0.0999 - val_accuracy: 0.1634\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0664 - accuracy: 0.4974 - val_loss: 0.0726 - val_accuracy: 0.4488\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0684 - accuracy: 0.4576 - val_loss: 0.1647 - val_accuracy: 0.1293\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0669 - accuracy: 0.4922 - val_loss: 0.1235 - val_accuracy: 0.1512\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0621 - accuracy: 0.5368 - val_loss: 0.0921 - val_accuracy: 0.4268\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0608 - accuracy: 0.5405 - val_loss: 0.1240 - val_accuracy: 0.1512\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0610 - accuracy: 0.5302 - val_loss: 0.1231 - val_accuracy: 0.1488\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0568 - accuracy: 0.5769 - val_loss: 0.1178 - val_accuracy: 0.1732\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0528 - accuracy: 0.6177 - val_loss: 0.1501 - val_accuracy: 0.1195\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0508 - accuracy: 0.6298 - val_loss: 0.0625 - val_accuracy: 0.5317\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0521 - accuracy: 0.6290 - val_loss: 0.1283 - val_accuracy: 0.1878\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0549 - accuracy: 0.5996 - val_loss: 0.0997 - val_accuracy: 0.3390\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0512 - accuracy: 0.6409 - val_loss: 0.1341 - val_accuracy: 0.1463\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0520 - accuracy: 0.6217 - val_loss: 0.0489 - val_accuracy: 0.6439\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0487 - accuracy: 0.6568 - val_loss: 0.0645 - val_accuracy: 0.5195\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0434 - accuracy: 0.7030 - val_loss: 0.1379 - val_accuracy: 0.1293\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0498 - accuracy: 0.6403 - val_loss: 0.0551 - val_accuracy: 0.6000\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0428 - accuracy: 0.6982 - val_loss: 0.0884 - val_accuracy: 0.3439\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0453 - accuracy: 0.6695 - val_loss: 0.0402 - val_accuracy: 0.7268\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0420 - accuracy: 0.7163 - val_loss: 0.1804 - val_accuracy: 0.0780\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0397 - accuracy: 0.7213 - val_loss: 0.0490 - val_accuracy: 0.6439\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0387 - accuracy: 0.7505 - val_loss: 0.0900 - val_accuracy: 0.3268\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0381 - accuracy: 0.7397 - val_loss: 0.0607 - val_accuracy: 0.5537\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0458 - accuracy: 0.6753 - val_loss: 0.0510 - val_accuracy: 0.6341\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0373 - accuracy: 0.7284 - val_loss: 0.1144 - val_accuracy: 0.1951\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0366 - accuracy: 0.7432 - val_loss: 0.0509 - val_accuracy: 0.6146\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0380 - accuracy: 0.7371 - val_loss: 0.1427 - val_accuracy: 0.1610\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0393 - accuracy: 0.7258 - val_loss: 0.0448 - val_accuracy: 0.6927\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0356 - accuracy: 0.7477 - val_loss: 0.0653 - val_accuracy: 0.5146\n",
            "==================================================\n",
            "Result of resnet_model, fold 3\n",
            "Epoch: 44\n",
            "Accuracy: 0.7268292903900146\n",
            "Time taken:  423.77747344970703\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 22s 114ms/step - loss: 0.0959 - accuracy: 0.1096 - val_loss: 0.0914 - val_accuracy: 0.0929\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0924 - accuracy: 0.1397 - val_loss: 0.0917 - val_accuracy: 0.0929\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0911 - accuracy: 0.1721 - val_loss: 0.0929 - val_accuracy: 0.0929\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0905 - accuracy: 0.1565 - val_loss: 0.0988 - val_accuracy: 0.1100\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0875 - accuracy: 0.2185 - val_loss: 0.1089 - val_accuracy: 0.1345\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0856 - accuracy: 0.2721 - val_loss: 0.1206 - val_accuracy: 0.2249\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0833 - accuracy: 0.2784 - val_loss: 0.1200 - val_accuracy: 0.1443\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0826 - accuracy: 0.3047 - val_loss: 0.1314 - val_accuracy: 0.1760\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0807 - accuracy: 0.3327 - val_loss: 0.1489 - val_accuracy: 0.1516\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0795 - accuracy: 0.3374 - val_loss: 0.0999 - val_accuracy: 0.2738\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0795 - accuracy: 0.3439 - val_loss: 0.0769 - val_accuracy: 0.3692\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0787 - accuracy: 0.3448 - val_loss: 0.1299 - val_accuracy: 0.1956\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0774 - accuracy: 0.3658 - val_loss: 0.1553 - val_accuracy: 0.0978\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0753 - accuracy: 0.3897 - val_loss: 0.0850 - val_accuracy: 0.3178\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0757 - accuracy: 0.3841 - val_loss: 0.0740 - val_accuracy: 0.4254\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0708 - accuracy: 0.4321 - val_loss: 0.0976 - val_accuracy: 0.3154\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0713 - accuracy: 0.4133 - val_loss: 0.1311 - val_accuracy: 0.1614\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0699 - accuracy: 0.4452 - val_loss: 0.0868 - val_accuracy: 0.3692\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0671 - accuracy: 0.4735 - val_loss: 0.1035 - val_accuracy: 0.2078\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0669 - accuracy: 0.4829 - val_loss: 0.0810 - val_accuracy: 0.3472\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0642 - accuracy: 0.5167 - val_loss: 0.0800 - val_accuracy: 0.3350\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0620 - accuracy: 0.5386 - val_loss: 0.0847 - val_accuracy: 0.2885\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0590 - accuracy: 0.5752 - val_loss: 0.1289 - val_accuracy: 0.1687\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0592 - accuracy: 0.5598 - val_loss: 0.0584 - val_accuracy: 0.5599\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0597 - accuracy: 0.5509 - val_loss: 0.1345 - val_accuracy: 0.1638\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0584 - accuracy: 0.5509 - val_loss: 0.1418 - val_accuracy: 0.0929\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0539 - accuracy: 0.6003 - val_loss: 0.0789 - val_accuracy: 0.4523\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0560 - accuracy: 0.5717 - val_loss: 0.1738 - val_accuracy: 0.0831\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0499 - accuracy: 0.6508 - val_loss: 0.0630 - val_accuracy: 0.5330\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0527 - accuracy: 0.6260 - val_loss: 0.0536 - val_accuracy: 0.6064\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0517 - accuracy: 0.6342 - val_loss: 0.0425 - val_accuracy: 0.7017\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0461 - accuracy: 0.6736 - val_loss: 0.0990 - val_accuracy: 0.3570\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0490 - accuracy: 0.6473 - val_loss: 0.0645 - val_accuracy: 0.5526\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0450 - accuracy: 0.6821 - val_loss: 0.0450 - val_accuracy: 0.6870\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0444 - accuracy: 0.6851 - val_loss: 0.0478 - val_accuracy: 0.6577\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0446 - accuracy: 0.6935 - val_loss: 0.1335 - val_accuracy: 0.1369\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0430 - accuracy: 0.6964 - val_loss: 0.0582 - val_accuracy: 0.5795\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0447 - accuracy: 0.6646 - val_loss: 0.1276 - val_accuracy: 0.1614\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0431 - accuracy: 0.7062 - val_loss: 0.1455 - val_accuracy: 0.1467\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0383 - accuracy: 0.7211 - val_loss: 0.1107 - val_accuracy: 0.2176\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0332 - accuracy: 0.7703 - val_loss: 0.0480 - val_accuracy: 0.6479\n",
            "==================================================\n",
            "Result of resnet_model, fold 4\n",
            "Epoch: 41\n",
            "Accuracy: 0.7017114758491516\n",
            "Time taken:  398.8878245353699\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 21s 101ms/step - loss: 0.0951 - accuracy: 0.1198 - val_loss: 0.0903 - val_accuracy: 0.1076\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0918 - accuracy: 0.1353 - val_loss: 0.0913 - val_accuracy: 0.1051\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0902 - accuracy: 0.1748 - val_loss: 0.0954 - val_accuracy: 0.0685\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0891 - accuracy: 0.1903 - val_loss: 0.0954 - val_accuracy: 0.1369\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0896 - accuracy: 0.1804 - val_loss: 0.0908 - val_accuracy: 0.1614\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0857 - accuracy: 0.2671 - val_loss: 0.0851 - val_accuracy: 0.2396\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0868 - accuracy: 0.2255 - val_loss: 0.0992 - val_accuracy: 0.2152\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0860 - accuracy: 0.2650 - val_loss: 0.0775 - val_accuracy: 0.3765\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0854 - accuracy: 0.2537 - val_loss: 0.0830 - val_accuracy: 0.3032\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0849 - accuracy: 0.2625 - val_loss: 0.1032 - val_accuracy: 0.2298\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0824 - accuracy: 0.2864 - val_loss: 0.0935 - val_accuracy: 0.1663\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0823 - accuracy: 0.2801 - val_loss: 0.1168 - val_accuracy: 0.1956\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 8s 83ms/step - loss: 0.0774 - accuracy: 0.3567 - val_loss: 0.0801 - val_accuracy: 0.3374\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0775 - accuracy: 0.3510 - val_loss: 0.1434 - val_accuracy: 0.0758\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0766 - accuracy: 0.3657 - val_loss: 0.1399 - val_accuracy: 0.1002\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0753 - accuracy: 0.3928 - val_loss: 0.1217 - val_accuracy: 0.1002\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0728 - accuracy: 0.4470 - val_loss: 0.0949 - val_accuracy: 0.2225\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0681 - accuracy: 0.4550 - val_loss: 0.0796 - val_accuracy: 0.3716\n",
            "==================================================\n",
            "Result of resnet_model, fold 5\n",
            "Epoch: 18\n",
            "Accuracy: 0.3765281140804291\n",
            "Time taken:  185.00662446022034\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 27s 128ms/step - loss: 0.0955 - accuracy: 0.1293 - val_loss: 0.0972 - val_accuracy: 0.1073\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0906 - accuracy: 0.1579 - val_loss: 0.1439 - val_accuracy: 0.1049\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0902 - accuracy: 0.1659 - val_loss: 0.1200 - val_accuracy: 0.1366\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 9s 89ms/step - loss: 0.0866 - accuracy: 0.2391 - val_loss: 0.1051 - val_accuracy: 0.1512\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0833 - accuracy: 0.2845 - val_loss: 0.1199 - val_accuracy: 0.1902\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0806 - accuracy: 0.3116 - val_loss: 0.1195 - val_accuracy: 0.2268\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0768 - accuracy: 0.3641 - val_loss: 0.0732 - val_accuracy: 0.3878\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0763 - accuracy: 0.3759 - val_loss: 0.0868 - val_accuracy: 0.2366\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0708 - accuracy: 0.4491 - val_loss: 0.0839 - val_accuracy: 0.2707\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0726 - accuracy: 0.4349 - val_loss: 0.0954 - val_accuracy: 0.3024\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0632 - accuracy: 0.5219 - val_loss: 0.1081 - val_accuracy: 0.1537\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0586 - accuracy: 0.5617 - val_loss: 0.0775 - val_accuracy: 0.3390\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0566 - accuracy: 0.5791 - val_loss: 0.0750 - val_accuracy: 0.3976\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0525 - accuracy: 0.6178 - val_loss: 0.0777 - val_accuracy: 0.3317\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0483 - accuracy: 0.6511 - val_loss: 0.0677 - val_accuracy: 0.4732\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0458 - accuracy: 0.6737 - val_loss: 0.1115 - val_accuracy: 0.1829\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0457 - accuracy: 0.6714 - val_loss: 0.0702 - val_accuracy: 0.4390\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0448 - accuracy: 0.6711 - val_loss: 0.0688 - val_accuracy: 0.4488\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 9s 82ms/step - loss: 0.0376 - accuracy: 0.7341 - val_loss: 0.1284 - val_accuracy: 0.2000\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0430 - accuracy: 0.6871 - val_loss: 0.0658 - val_accuracy: 0.4756\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0429 - accuracy: 0.6950 - val_loss: 0.0719 - val_accuracy: 0.4049\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0348 - accuracy: 0.7505 - val_loss: 0.0879 - val_accuracy: 0.3171\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 9s 89ms/step - loss: 0.0361 - accuracy: 0.7332 - val_loss: 0.0810 - val_accuracy: 0.3171\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0353 - accuracy: 0.7538 - val_loss: 0.0983 - val_accuracy: 0.3146\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0367 - accuracy: 0.7377 - val_loss: 0.1063 - val_accuracy: 0.2049\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0347 - accuracy: 0.7585 - val_loss: 0.1156 - val_accuracy: 0.1756\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0325 - accuracy: 0.7640 - val_loss: 0.0751 - val_accuracy: 0.3439\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0316 - accuracy: 0.7758 - val_loss: 0.0943 - val_accuracy: 0.2683\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0313 - accuracy: 0.7785 - val_loss: 0.0469 - val_accuracy: 0.6585\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0312 - accuracy: 0.7928 - val_loss: 0.0466 - val_accuracy: 0.6829\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0273 - accuracy: 0.7990 - val_loss: 0.0788 - val_accuracy: 0.3244\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0279 - accuracy: 0.8116 - val_loss: 0.0800 - val_accuracy: 0.3463\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0273 - accuracy: 0.8019 - val_loss: 0.0673 - val_accuracy: 0.4585\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0279 - accuracy: 0.8031 - val_loss: 0.0837 - val_accuracy: 0.2659\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0248 - accuracy: 0.8263 - val_loss: 0.0725 - val_accuracy: 0.4268\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0258 - accuracy: 0.8314 - val_loss: 0.0337 - val_accuracy: 0.7829\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0249 - accuracy: 0.8321 - val_loss: 0.0716 - val_accuracy: 0.4659\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0212 - accuracy: 0.8673 - val_loss: 0.0267 - val_accuracy: 0.8366\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0235 - accuracy: 0.8450 - val_loss: 0.0284 - val_accuracy: 0.8390\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0216 - accuracy: 0.8541 - val_loss: 0.0837 - val_accuracy: 0.3512\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0262 - accuracy: 0.8248 - val_loss: 0.0422 - val_accuracy: 0.6878\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0268 - accuracy: 0.8128 - val_loss: 0.0390 - val_accuracy: 0.6951\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0208 - accuracy: 0.8666 - val_loss: 0.1110 - val_accuracy: 0.1585\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0216 - accuracy: 0.8592 - val_loss: 0.0992 - val_accuracy: 0.2220\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0233 - accuracy: 0.8396 - val_loss: 0.0361 - val_accuracy: 0.7512\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0217 - accuracy: 0.8474 - val_loss: 0.1187 - val_accuracy: 0.2000\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0205 - accuracy: 0.8598 - val_loss: 0.1083 - val_accuracy: 0.1976\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0191 - accuracy: 0.8773 - val_loss: 0.0416 - val_accuracy: 0.6976\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0185 - accuracy: 0.8768 - val_loss: 0.0485 - val_accuracy: 0.6659\n",
            "==================================================\n",
            "Result of densenet_model, fold 1\n",
            "Epoch: 49\n",
            "Accuracy: 0.8390243649482727\n",
            "Time taken:  469.11491775512695\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 22s 106ms/step - loss: 0.0946 - accuracy: 0.1171 - val_loss: 0.0905 - val_accuracy: 0.0878\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0908 - accuracy: 0.1693 - val_loss: 0.1696 - val_accuracy: 0.0878\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0863 - accuracy: 0.2386 - val_loss: 0.1678 - val_accuracy: 0.0878\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0851 - accuracy: 0.2492 - val_loss: 0.1366 - val_accuracy: 0.1195\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 9s 89ms/step - loss: 0.0823 - accuracy: 0.3203 - val_loss: 0.0854 - val_accuracy: 0.3146\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 9s 89ms/step - loss: 0.0807 - accuracy: 0.3249 - val_loss: 0.0712 - val_accuracy: 0.4073\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0752 - accuracy: 0.3992 - val_loss: 0.0875 - val_accuracy: 0.2537\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0722 - accuracy: 0.4284 - val_loss: 0.0858 - val_accuracy: 0.3024\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0700 - accuracy: 0.4441 - val_loss: 0.0730 - val_accuracy: 0.4220\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0656 - accuracy: 0.4932 - val_loss: 0.1376 - val_accuracy: 0.0951\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0587 - accuracy: 0.5567 - val_loss: 0.0736 - val_accuracy: 0.3707\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0605 - accuracy: 0.5454 - val_loss: 0.0906 - val_accuracy: 0.2341\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0566 - accuracy: 0.5683 - val_loss: 0.0911 - val_accuracy: 0.3610\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0514 - accuracy: 0.6270 - val_loss: 0.1087 - val_accuracy: 0.1951\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0469 - accuracy: 0.6636 - val_loss: 0.0629 - val_accuracy: 0.5488\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0487 - accuracy: 0.6612 - val_loss: 0.0956 - val_accuracy: 0.2244\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0455 - accuracy: 0.6832 - val_loss: 0.1192 - val_accuracy: 0.1707\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0415 - accuracy: 0.7056 - val_loss: 0.0720 - val_accuracy: 0.4561\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0377 - accuracy: 0.7369 - val_loss: 0.1009 - val_accuracy: 0.1878\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0410 - accuracy: 0.6984 - val_loss: 0.0959 - val_accuracy: 0.1829\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0400 - accuracy: 0.7111 - val_loss: 0.1214 - val_accuracy: 0.1732\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0362 - accuracy: 0.7351 - val_loss: 0.1079 - val_accuracy: 0.1780\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0314 - accuracy: 0.7866 - val_loss: 0.0919 - val_accuracy: 0.2537\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0384 - accuracy: 0.7195 - val_loss: 0.0887 - val_accuracy: 0.2927\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 9s 89ms/step - loss: 0.0336 - accuracy: 0.7727 - val_loss: 0.0458 - val_accuracy: 0.6732\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0349 - accuracy: 0.7625 - val_loss: 0.0466 - val_accuracy: 0.6585\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0293 - accuracy: 0.8078 - val_loss: 0.0841 - val_accuracy: 0.3049\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0331 - accuracy: 0.7762 - val_loss: 0.0425 - val_accuracy: 0.7000\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0292 - accuracy: 0.8034 - val_loss: 0.0291 - val_accuracy: 0.8293\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0273 - accuracy: 0.8131 - val_loss: 0.0721 - val_accuracy: 0.3829\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0303 - accuracy: 0.7907 - val_loss: 0.0366 - val_accuracy: 0.7439\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0302 - accuracy: 0.7997 - val_loss: 0.0286 - val_accuracy: 0.8244\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0263 - accuracy: 0.8324 - val_loss: 0.0694 - val_accuracy: 0.4463\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0283 - accuracy: 0.7992 - val_loss: 0.0778 - val_accuracy: 0.3537\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 8s 82ms/step - loss: 0.0266 - accuracy: 0.8166 - val_loss: 0.0928 - val_accuracy: 0.2463\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0243 - accuracy: 0.8316 - val_loss: 0.1603 - val_accuracy: 0.0902\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0269 - accuracy: 0.8127 - val_loss: 0.1277 - val_accuracy: 0.1561\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0266 - accuracy: 0.8182 - val_loss: 0.1068 - val_accuracy: 0.1854\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0261 - accuracy: 0.8211 - val_loss: 0.0262 - val_accuracy: 0.8463\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0252 - accuracy: 0.8289 - val_loss: 0.0430 - val_accuracy: 0.6927\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0237 - accuracy: 0.8388 - val_loss: 0.0815 - val_accuracy: 0.3707\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0222 - accuracy: 0.8522 - val_loss: 0.0510 - val_accuracy: 0.6244\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0228 - accuracy: 0.8429 - val_loss: 0.0794 - val_accuracy: 0.3878\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0227 - accuracy: 0.8445 - val_loss: 0.0939 - val_accuracy: 0.3195\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0200 - accuracy: 0.8684 - val_loss: 0.0988 - val_accuracy: 0.2268\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0245 - accuracy: 0.8430 - val_loss: 0.0848 - val_accuracy: 0.3195\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0238 - accuracy: 0.8370 - val_loss: 0.0284 - val_accuracy: 0.8171\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0229 - accuracy: 0.8420 - val_loss: 0.0516 - val_accuracy: 0.6171\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0209 - accuracy: 0.8597 - val_loss: 0.0269 - val_accuracy: 0.8244\n",
            "==================================================\n",
            "Result of densenet_model, fold 2\n",
            "Epoch: 49\n",
            "Accuracy: 0.8463414907455444\n",
            "Time taken:  461.40014243125916\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 22s 104ms/step - loss: 0.0958 - accuracy: 0.1158 - val_loss: 0.0932 - val_accuracy: 0.0976\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0909 - accuracy: 0.1388 - val_loss: 0.1289 - val_accuracy: 0.0976\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0879 - accuracy: 0.2236 - val_loss: 0.1336 - val_accuracy: 0.0976\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0849 - accuracy: 0.2587 - val_loss: 0.1738 - val_accuracy: 0.0976\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0828 - accuracy: 0.3056 - val_loss: 0.1361 - val_accuracy: 0.1439\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0797 - accuracy: 0.3335 - val_loss: 0.0807 - val_accuracy: 0.3317\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0755 - accuracy: 0.3926 - val_loss: 0.0923 - val_accuracy: 0.2488\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0727 - accuracy: 0.4419 - val_loss: 0.0780 - val_accuracy: 0.3463\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0681 - accuracy: 0.4610 - val_loss: 0.1386 - val_accuracy: 0.0902\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0644 - accuracy: 0.5073 - val_loss: 0.1025 - val_accuracy: 0.2707\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0620 - accuracy: 0.5329 - val_loss: 0.1004 - val_accuracy: 0.2366\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0565 - accuracy: 0.5965 - val_loss: 0.0903 - val_accuracy: 0.2829\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0513 - accuracy: 0.6168 - val_loss: 0.1020 - val_accuracy: 0.2073\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0504 - accuracy: 0.6327 - val_loss: 0.1264 - val_accuracy: 0.1805\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0471 - accuracy: 0.6633 - val_loss: 0.0792 - val_accuracy: 0.3317\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0479 - accuracy: 0.6512 - val_loss: 0.0734 - val_accuracy: 0.4463\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0410 - accuracy: 0.7124 - val_loss: 0.1037 - val_accuracy: 0.2366\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0430 - accuracy: 0.6988 - val_loss: 0.0536 - val_accuracy: 0.6098\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0393 - accuracy: 0.7244 - val_loss: 0.1723 - val_accuracy: 0.0878\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 9s 89ms/step - loss: 0.0391 - accuracy: 0.7199 - val_loss: 0.1376 - val_accuracy: 0.1439\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0368 - accuracy: 0.7310 - val_loss: 0.1242 - val_accuracy: 0.1341\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0380 - accuracy: 0.7379 - val_loss: 0.1436 - val_accuracy: 0.1512\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0380 - accuracy: 0.7278 - val_loss: 0.0793 - val_accuracy: 0.3098\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0301 - accuracy: 0.7938 - val_loss: 0.0943 - val_accuracy: 0.2902\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0324 - accuracy: 0.7754 - val_loss: 0.1152 - val_accuracy: 0.1634\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0324 - accuracy: 0.7751 - val_loss: 0.0397 - val_accuracy: 0.7098\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0292 - accuracy: 0.7971 - val_loss: 0.1244 - val_accuracy: 0.2707\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0293 - accuracy: 0.7963 - val_loss: 0.0895 - val_accuracy: 0.2927\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0309 - accuracy: 0.7833 - val_loss: 0.0904 - val_accuracy: 0.2317\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0264 - accuracy: 0.8128 - val_loss: 0.0479 - val_accuracy: 0.6439\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0298 - accuracy: 0.7864 - val_loss: 0.0524 - val_accuracy: 0.6098\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0239 - accuracy: 0.8405 - val_loss: 0.1392 - val_accuracy: 0.1341\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 9s 89ms/step - loss: 0.0264 - accuracy: 0.8177 - val_loss: 0.0461 - val_accuracy: 0.6707\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0272 - accuracy: 0.8195 - val_loss: 0.1392 - val_accuracy: 0.1439\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0247 - accuracy: 0.8309 - val_loss: 0.0902 - val_accuracy: 0.3024\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0262 - accuracy: 0.8197 - val_loss: 0.1484 - val_accuracy: 0.1073\n",
            "==================================================\n",
            "Result of densenet_model, fold 3\n",
            "Epoch: 36\n",
            "Accuracy: 0.709756076335907\n",
            "Time taken:  345.42944955825806\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 25s 127ms/step - loss: 0.0950 - accuracy: 0.1092 - val_loss: 0.0955 - val_accuracy: 0.0929\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 9s 89ms/step - loss: 0.0908 - accuracy: 0.1724 - val_loss: 0.1599 - val_accuracy: 0.0929\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0874 - accuracy: 0.2472 - val_loss: 0.1735 - val_accuracy: 0.0929\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0844 - accuracy: 0.2917 - val_loss: 0.1738 - val_accuracy: 0.0929\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0827 - accuracy: 0.2879 - val_loss: 0.0948 - val_accuracy: 0.2225\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0789 - accuracy: 0.3315 - val_loss: 0.0884 - val_accuracy: 0.3154\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0757 - accuracy: 0.3830 - val_loss: 0.1730 - val_accuracy: 0.0929\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0733 - accuracy: 0.4089 - val_loss: 0.0939 - val_accuracy: 0.2274\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0636 - accuracy: 0.5249 - val_loss: 0.1042 - val_accuracy: 0.2176\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0667 - accuracy: 0.4967 - val_loss: 0.1031 - val_accuracy: 0.1785\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0563 - accuracy: 0.5819 - val_loss: 0.0662 - val_accuracy: 0.5061\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0546 - accuracy: 0.5957 - val_loss: 0.1456 - val_accuracy: 0.1296\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0530 - accuracy: 0.6296 - val_loss: 0.0773 - val_accuracy: 0.3472\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 9s 89ms/step - loss: 0.0500 - accuracy: 0.6369 - val_loss: 0.1082 - val_accuracy: 0.1222\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0497 - accuracy: 0.6308 - val_loss: 0.0777 - val_accuracy: 0.3496\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0450 - accuracy: 0.6718 - val_loss: 0.1152 - val_accuracy: 0.1491\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0433 - accuracy: 0.6890 - val_loss: 0.0429 - val_accuracy: 0.7213\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0414 - accuracy: 0.6955 - val_loss: 0.0700 - val_accuracy: 0.4425\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0382 - accuracy: 0.7211 - val_loss: 0.1090 - val_accuracy: 0.1809\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 9s 89ms/step - loss: 0.0350 - accuracy: 0.7626 - val_loss: 0.1057 - val_accuracy: 0.2714\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0382 - accuracy: 0.7275 - val_loss: 0.0950 - val_accuracy: 0.2249\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0384 - accuracy: 0.7218 - val_loss: 0.1200 - val_accuracy: 0.1516\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0340 - accuracy: 0.7614 - val_loss: 0.0497 - val_accuracy: 0.6553\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0334 - accuracy: 0.7723 - val_loss: 0.1252 - val_accuracy: 0.1174\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0314 - accuracy: 0.7907 - val_loss: 0.0447 - val_accuracy: 0.6870\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0304 - accuracy: 0.7921 - val_loss: 0.1143 - val_accuracy: 0.1174\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0297 - accuracy: 0.7917 - val_loss: 0.0550 - val_accuracy: 0.5966\n",
            "==================================================\n",
            "Result of densenet_model, fold 4\n",
            "Epoch: 27\n",
            "Accuracy: 0.7212713956832886\n",
            "Time taken:  264.8977587223053\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 22s 105ms/step - loss: 0.0955 - accuracy: 0.1126 - val_loss: 0.0929 - val_accuracy: 0.1076\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0911 - accuracy: 0.1655 - val_loss: 0.1204 - val_accuracy: 0.0685\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0882 - accuracy: 0.2143 - val_loss: 0.1237 - val_accuracy: 0.1076\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 9s 83ms/step - loss: 0.0849 - accuracy: 0.2689 - val_loss: 0.1769 - val_accuracy: 0.1076\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0837 - accuracy: 0.2813 - val_loss: 0.0988 - val_accuracy: 0.2127\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 9s 90ms/step - loss: 0.0796 - accuracy: 0.3398 - val_loss: 0.1204 - val_accuracy: 0.1467\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0787 - accuracy: 0.3702 - val_loss: 0.1476 - val_accuracy: 0.1296\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0740 - accuracy: 0.4126 - val_loss: 0.0903 - val_accuracy: 0.2274\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0687 - accuracy: 0.4721 - val_loss: 0.1197 - val_accuracy: 0.1516\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0674 - accuracy: 0.4739 - val_loss: 0.0948 - val_accuracy: 0.2005\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0637 - accuracy: 0.5191 - val_loss: 0.1211 - val_accuracy: 0.2127\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0616 - accuracy: 0.5261 - val_loss: 0.1327 - val_accuracy: 0.1125\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0563 - accuracy: 0.5751 - val_loss: 0.0699 - val_accuracy: 0.5086\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0531 - accuracy: 0.6096 - val_loss: 0.1401 - val_accuracy: 0.1418\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0517 - accuracy: 0.6200 - val_loss: 0.0643 - val_accuracy: 0.5134\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0477 - accuracy: 0.6679 - val_loss: 0.1056 - val_accuracy: 0.1932\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0482 - accuracy: 0.6450 - val_loss: 0.1015 - val_accuracy: 0.2372\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0462 - accuracy: 0.6632 - val_loss: 0.0750 - val_accuracy: 0.3716\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 9s 88ms/step - loss: 0.0383 - accuracy: 0.7222 - val_loss: 0.1092 - val_accuracy: 0.2127\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 9s 90ms/step - loss: 0.0371 - accuracy: 0.7304 - val_loss: 0.0991 - val_accuracy: 0.2249\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0389 - accuracy: 0.7162 - val_loss: 0.0492 - val_accuracy: 0.6675\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0348 - accuracy: 0.7575 - val_loss: 0.1150 - val_accuracy: 0.1883\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 9s 89ms/step - loss: 0.0375 - accuracy: 0.7299 - val_loss: 0.1500 - val_accuracy: 0.1247\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 9s 90ms/step - loss: 0.0358 - accuracy: 0.7554 - val_loss: 0.1374 - val_accuracy: 0.1394\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0368 - accuracy: 0.7440 - val_loss: 0.0376 - val_accuracy: 0.7311\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0300 - accuracy: 0.7879 - val_loss: 0.0735 - val_accuracy: 0.4768\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 9s 90ms/step - loss: 0.0323 - accuracy: 0.7688 - val_loss: 0.0319 - val_accuracy: 0.7946\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0313 - accuracy: 0.7745 - val_loss: 0.1343 - val_accuracy: 0.1222\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0319 - accuracy: 0.7831 - val_loss: 0.1249 - val_accuracy: 0.1589\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0270 - accuracy: 0.8192 - val_loss: 0.1335 - val_accuracy: 0.1394\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0278 - accuracy: 0.8109 - val_loss: 0.0681 - val_accuracy: 0.5526\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 9s 86ms/step - loss: 0.0292 - accuracy: 0.8058 - val_loss: 0.1044 - val_accuracy: 0.2054\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0261 - accuracy: 0.8101 - val_loss: 0.0925 - val_accuracy: 0.2078\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 9s 87ms/step - loss: 0.0313 - accuracy: 0.7780 - val_loss: 0.0854 - val_accuracy: 0.3276\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0250 - accuracy: 0.8273 - val_loss: 0.0793 - val_accuracy: 0.4083\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 9s 84ms/step - loss: 0.0245 - accuracy: 0.8268 - val_loss: 0.0942 - val_accuracy: 0.2910\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 9s 85ms/step - loss: 0.0249 - accuracy: 0.8213 - val_loss: 0.0803 - val_accuracy: 0.3325\n",
            "==================================================\n",
            "Result of densenet_model, fold 5\n",
            "Epoch: 37\n",
            "Accuracy: 0.7946210503578186\n",
            "Time taken:  356.0958185195923\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 18s 117ms/step - loss: 0.0953 - accuracy: 0.1407 - val_loss: 0.0977 - val_accuracy: 0.1049\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0907 - accuracy: 0.1651 - val_loss: 0.1407 - val_accuracy: 0.1049\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 10s 102ms/step - loss: 0.0876 - accuracy: 0.2164 - val_loss: 0.1069 - val_accuracy: 0.0976\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 10s 101ms/step - loss: 0.0859 - accuracy: 0.2737 - val_loss: 0.1014 - val_accuracy: 0.1098\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0839 - accuracy: 0.2885 - val_loss: 0.1076 - val_accuracy: 0.1268\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0826 - accuracy: 0.2919 - val_loss: 0.1319 - val_accuracy: 0.1220\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0812 - accuracy: 0.3152 - val_loss: 0.1150 - val_accuracy: 0.2902\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0751 - accuracy: 0.4004 - val_loss: 0.1405 - val_accuracy: 0.1829\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0740 - accuracy: 0.4022 - val_loss: 0.1165 - val_accuracy: 0.2585\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0667 - accuracy: 0.4953 - val_loss: 0.1130 - val_accuracy: 0.2976\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0645 - accuracy: 0.5019 - val_loss: 0.0894 - val_accuracy: 0.4024\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0629 - accuracy: 0.5099 - val_loss: 0.0728 - val_accuracy: 0.4634\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0610 - accuracy: 0.5452 - val_loss: 0.1308 - val_accuracy: 0.2073\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0573 - accuracy: 0.5791 - val_loss: 0.0652 - val_accuracy: 0.5073\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 10s 101ms/step - loss: 0.0524 - accuracy: 0.6344 - val_loss: 0.0918 - val_accuracy: 0.3488\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0489 - accuracy: 0.6463 - val_loss: 0.0731 - val_accuracy: 0.5146\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0491 - accuracy: 0.6335 - val_loss: 0.0713 - val_accuracy: 0.4780\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0442 - accuracy: 0.6930 - val_loss: 0.0527 - val_accuracy: 0.6341\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0503 - accuracy: 0.6502 - val_loss: 0.0591 - val_accuracy: 0.6073\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0446 - accuracy: 0.6823 - val_loss: 0.0516 - val_accuracy: 0.6610\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0399 - accuracy: 0.7360 - val_loss: 0.0548 - val_accuracy: 0.5927\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0379 - accuracy: 0.7295 - val_loss: 0.1024 - val_accuracy: 0.3561\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0379 - accuracy: 0.7373 - val_loss: 0.0672 - val_accuracy: 0.4951\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0360 - accuracy: 0.7477 - val_loss: 0.0372 - val_accuracy: 0.7317\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0401 - accuracy: 0.7128 - val_loss: 0.0468 - val_accuracy: 0.6780\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 10s 101ms/step - loss: 0.0323 - accuracy: 0.7710 - val_loss: 0.0489 - val_accuracy: 0.6951\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0298 - accuracy: 0.7881 - val_loss: 0.0385 - val_accuracy: 0.7146\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0347 - accuracy: 0.7639 - val_loss: 0.0485 - val_accuracy: 0.6366\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0365 - accuracy: 0.7423 - val_loss: 0.0558 - val_accuracy: 0.6000\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0309 - accuracy: 0.7841 - val_loss: 0.0353 - val_accuracy: 0.7512\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0307 - accuracy: 0.7829 - val_loss: 0.0539 - val_accuracy: 0.6073\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0305 - accuracy: 0.7936 - val_loss: 0.0290 - val_accuracy: 0.7902\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0306 - accuracy: 0.8036 - val_loss: 0.0309 - val_accuracy: 0.7854\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0304 - accuracy: 0.7925 - val_loss: 0.0566 - val_accuracy: 0.5780\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0266 - accuracy: 0.8160 - val_loss: 0.0462 - val_accuracy: 0.6561\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0214 - accuracy: 0.8614 - val_loss: 0.0451 - val_accuracy: 0.6683\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0285 - accuracy: 0.8019 - val_loss: 0.0269 - val_accuracy: 0.8171\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0220 - accuracy: 0.8625 - val_loss: 0.0566 - val_accuracy: 0.5902\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0239 - accuracy: 0.8288 - val_loss: 0.0370 - val_accuracy: 0.7463\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0246 - accuracy: 0.8349 - val_loss: 0.0488 - val_accuracy: 0.6366\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0246 - accuracy: 0.8311 - val_loss: 0.0266 - val_accuracy: 0.8122\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0230 - accuracy: 0.8400 - val_loss: 0.0299 - val_accuracy: 0.8024\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0192 - accuracy: 0.8742 - val_loss: 0.0275 - val_accuracy: 0.8268\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0226 - accuracy: 0.8516 - val_loss: 0.0252 - val_accuracy: 0.8341\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0220 - accuracy: 0.8572 - val_loss: 0.0451 - val_accuracy: 0.6683\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0220 - accuracy: 0.8527 - val_loss: 0.0589 - val_accuracy: 0.5415\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0221 - accuracy: 0.8562 - val_loss: 0.0356 - val_accuracy: 0.7537\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0223 - accuracy: 0.8537 - val_loss: 0.0301 - val_accuracy: 0.7805\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0249 - accuracy: 0.8354 - val_loss: 0.0286 - val_accuracy: 0.8195\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0208 - accuracy: 0.8607 - val_loss: 0.0277 - val_accuracy: 0.8122\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0206 - accuracy: 0.8583 - val_loss: 0.0515 - val_accuracy: 0.6317\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0192 - accuracy: 0.8674 - val_loss: 0.0243 - val_accuracy: 0.8512\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0176 - accuracy: 0.8831 - val_loss: 0.0310 - val_accuracy: 0.7829\n",
            "Epoch 54/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0188 - accuracy: 0.8860 - val_loss: 0.0249 - val_accuracy: 0.8341\n",
            "Epoch 55/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0161 - accuracy: 0.8913 - val_loss: 0.0320 - val_accuracy: 0.7951\n",
            "Epoch 56/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0179 - accuracy: 0.8740 - val_loss: 0.0521 - val_accuracy: 0.6244\n",
            "Epoch 57/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0204 - accuracy: 0.8683 - val_loss: 0.0395 - val_accuracy: 0.7146\n",
            "Epoch 58/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0170 - accuracy: 0.8920 - val_loss: 0.0385 - val_accuracy: 0.7171\n",
            "Epoch 59/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0183 - accuracy: 0.8820 - val_loss: 0.0269 - val_accuracy: 0.8220\n",
            "Epoch 60/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0171 - accuracy: 0.8799 - val_loss: 0.0256 - val_accuracy: 0.8341\n",
            "Epoch 61/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0187 - accuracy: 0.8628 - val_loss: 0.0291 - val_accuracy: 0.8024\n",
            "Epoch 62/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0139 - accuracy: 0.9022 - val_loss: 0.0235 - val_accuracy: 0.8463\n",
            "==================================================\n",
            "Result of xception_model, fold 1\n",
            "Epoch: 62\n",
            "Accuracy: 0.8512195348739624\n",
            "Time taken:  664.3794994354248\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 16s 107ms/step - loss: 0.0961 - accuracy: 0.1131 - val_loss: 0.0957 - val_accuracy: 0.1049\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 10s 101ms/step - loss: 0.0923 - accuracy: 0.1234 - val_loss: 0.1035 - val_accuracy: 0.0927\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0871 - accuracy: 0.2284 - val_loss: 0.0939 - val_accuracy: 0.0976\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0867 - accuracy: 0.2337 - val_loss: 0.1056 - val_accuracy: 0.0902\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0861 - accuracy: 0.2373 - val_loss: 0.0918 - val_accuracy: 0.1878\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0843 - accuracy: 0.2791 - val_loss: 0.0957 - val_accuracy: 0.2610\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0828 - accuracy: 0.2973 - val_loss: 0.0910 - val_accuracy: 0.2756\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0808 - accuracy: 0.3186 - val_loss: 0.0999 - val_accuracy: 0.2341\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 10s 101ms/step - loss: 0.0784 - accuracy: 0.3593 - val_loss: 0.1445 - val_accuracy: 0.1634\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 10s 101ms/step - loss: 0.0748 - accuracy: 0.4127 - val_loss: 0.0856 - val_accuracy: 0.3854\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0722 - accuracy: 0.4243 - val_loss: 0.0823 - val_accuracy: 0.3707\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0696 - accuracy: 0.4562 - val_loss: 0.0944 - val_accuracy: 0.3293\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0630 - accuracy: 0.5211 - val_loss: 0.1325 - val_accuracy: 0.2366\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0605 - accuracy: 0.5466 - val_loss: 0.0702 - val_accuracy: 0.4854\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0547 - accuracy: 0.6113 - val_loss: 0.1366 - val_accuracy: 0.1854\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0595 - accuracy: 0.5676 - val_loss: 0.0950 - val_accuracy: 0.4293\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0489 - accuracy: 0.6460 - val_loss: 0.0904 - val_accuracy: 0.3683\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0464 - accuracy: 0.6638 - val_loss: 0.0556 - val_accuracy: 0.6073\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0478 - accuracy: 0.6571 - val_loss: 0.0394 - val_accuracy: 0.7390\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0452 - accuracy: 0.6755 - val_loss: 0.0578 - val_accuracy: 0.5878\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0458 - accuracy: 0.6722 - val_loss: 0.0379 - val_accuracy: 0.7341\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 10s 102ms/step - loss: 0.0401 - accuracy: 0.7137 - val_loss: 0.0381 - val_accuracy: 0.7293\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0399 - accuracy: 0.7289 - val_loss: 0.0390 - val_accuracy: 0.7122\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0349 - accuracy: 0.7565 - val_loss: 0.0333 - val_accuracy: 0.7732\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 10s 101ms/step - loss: 0.0344 - accuracy: 0.7638 - val_loss: 0.0664 - val_accuracy: 0.5463\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0379 - accuracy: 0.7409 - val_loss: 0.0327 - val_accuracy: 0.7829\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0317 - accuracy: 0.7842 - val_loss: 0.0389 - val_accuracy: 0.7366\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0338 - accuracy: 0.7622 - val_loss: 0.0299 - val_accuracy: 0.7878\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 10s 101ms/step - loss: 0.0323 - accuracy: 0.7693 - val_loss: 0.0726 - val_accuracy: 0.4780\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 10s 101ms/step - loss: 0.0307 - accuracy: 0.7845 - val_loss: 0.0253 - val_accuracy: 0.8244\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0307 - accuracy: 0.7828 - val_loss: 0.0286 - val_accuracy: 0.8024\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 11s 103ms/step - loss: 0.0306 - accuracy: 0.7868 - val_loss: 0.0387 - val_accuracy: 0.7268\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0306 - accuracy: 0.7756 - val_loss: 0.0355 - val_accuracy: 0.7683\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0296 - accuracy: 0.7950 - val_loss: 0.0298 - val_accuracy: 0.7829\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0228 - accuracy: 0.8490 - val_loss: 0.0212 - val_accuracy: 0.8561\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0230 - accuracy: 0.8496 - val_loss: 0.0600 - val_accuracy: 0.5707\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0247 - accuracy: 0.8296 - val_loss: 0.0272 - val_accuracy: 0.8049\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0250 - accuracy: 0.8281 - val_loss: 0.0289 - val_accuracy: 0.8024\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0242 - accuracy: 0.8344 - val_loss: 0.0467 - val_accuracy: 0.6780\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0254 - accuracy: 0.8183 - val_loss: 0.0333 - val_accuracy: 0.7756\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 10s 101ms/step - loss: 0.0276 - accuracy: 0.8146 - val_loss: 0.0340 - val_accuracy: 0.7634\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0215 - accuracy: 0.8563 - val_loss: 0.0249 - val_accuracy: 0.8366\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0237 - accuracy: 0.8407 - val_loss: 0.0328 - val_accuracy: 0.7585\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0222 - accuracy: 0.8493 - val_loss: 0.0474 - val_accuracy: 0.6976\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0241 - accuracy: 0.8363 - val_loss: 0.0244 - val_accuracy: 0.8366\n",
            "==================================================\n",
            "Result of xception_model, fold 2\n",
            "Epoch: 45\n",
            "Accuracy: 0.8560975790023804\n",
            "Time taken:  485.3394274711609\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 16s 108ms/step - loss: 0.0956 - accuracy: 0.1205 - val_loss: 0.0977 - val_accuracy: 0.0878\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0912 - accuracy: 0.1583 - val_loss: 0.1196 - val_accuracy: 0.0878\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0896 - accuracy: 0.2260 - val_loss: 0.1033 - val_accuracy: 0.1098\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0876 - accuracy: 0.2377 - val_loss: 0.1006 - val_accuracy: 0.0878\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0853 - accuracy: 0.2550 - val_loss: 0.0946 - val_accuracy: 0.1293\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0849 - accuracy: 0.2741 - val_loss: 0.0793 - val_accuracy: 0.3317\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0833 - accuracy: 0.3079 - val_loss: 0.0775 - val_accuracy: 0.3927\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0763 - accuracy: 0.3781 - val_loss: 0.0728 - val_accuracy: 0.4024\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0728 - accuracy: 0.4251 - val_loss: 0.0647 - val_accuracy: 0.5293\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0718 - accuracy: 0.4372 - val_loss: 0.0927 - val_accuracy: 0.3537\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0700 - accuracy: 0.4647 - val_loss: 0.1126 - val_accuracy: 0.3000\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0640 - accuracy: 0.5023 - val_loss: 0.0669 - val_accuracy: 0.5390\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0595 - accuracy: 0.5593 - val_loss: 0.0909 - val_accuracy: 0.4317\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0553 - accuracy: 0.5932 - val_loss: 0.0528 - val_accuracy: 0.6341\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0549 - accuracy: 0.5989 - val_loss: 0.1005 - val_accuracy: 0.3683\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0522 - accuracy: 0.6173 - val_loss: 0.0701 - val_accuracy: 0.5390\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0525 - accuracy: 0.6130 - val_loss: 0.0467 - val_accuracy: 0.6951\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0500 - accuracy: 0.6569 - val_loss: 0.0556 - val_accuracy: 0.6098\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0475 - accuracy: 0.6565 - val_loss: 0.0596 - val_accuracy: 0.5902\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0437 - accuracy: 0.6915 - val_loss: 0.0400 - val_accuracy: 0.7415\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0387 - accuracy: 0.7397 - val_loss: 0.0392 - val_accuracy: 0.7439\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0412 - accuracy: 0.7069 - val_loss: 0.0388 - val_accuracy: 0.7317\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0429 - accuracy: 0.6828 - val_loss: 0.0448 - val_accuracy: 0.6585\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0346 - accuracy: 0.7587 - val_loss: 0.0332 - val_accuracy: 0.7659\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0354 - accuracy: 0.7543 - val_loss: 0.0445 - val_accuracy: 0.7049\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0370 - accuracy: 0.7309 - val_loss: 0.0337 - val_accuracy: 0.7610\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0349 - accuracy: 0.7570 - val_loss: 0.0424 - val_accuracy: 0.7195\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0320 - accuracy: 0.7724 - val_loss: 0.0514 - val_accuracy: 0.6341\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0333 - accuracy: 0.7625 - val_loss: 0.0509 - val_accuracy: 0.6415\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0330 - accuracy: 0.7731 - val_loss: 0.0472 - val_accuracy: 0.6561\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0282 - accuracy: 0.8029 - val_loss: 0.0268 - val_accuracy: 0.8000\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0324 - accuracy: 0.7766 - val_loss: 0.0603 - val_accuracy: 0.5585\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0289 - accuracy: 0.7943 - val_loss: 0.0270 - val_accuracy: 0.8171\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0266 - accuracy: 0.8144 - val_loss: 0.0681 - val_accuracy: 0.5268\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0268 - accuracy: 0.8268 - val_loss: 0.0336 - val_accuracy: 0.7756\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0296 - accuracy: 0.7858 - val_loss: 0.0338 - val_accuracy: 0.7659\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0254 - accuracy: 0.8342 - val_loss: 0.0407 - val_accuracy: 0.7171\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 10s 96ms/step - loss: 0.0280 - accuracy: 0.8136 - val_loss: 0.0403 - val_accuracy: 0.7122\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0241 - accuracy: 0.8357 - val_loss: 0.0278 - val_accuracy: 0.8244\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0262 - accuracy: 0.8125 - val_loss: 0.0249 - val_accuracy: 0.8244\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0208 - accuracy: 0.8600 - val_loss: 0.0356 - val_accuracy: 0.7585\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0234 - accuracy: 0.8395 - val_loss: 0.0221 - val_accuracy: 0.8488\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0260 - accuracy: 0.8241 - val_loss: 0.0191 - val_accuracy: 0.8829\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0213 - accuracy: 0.8596 - val_loss: 0.0254 - val_accuracy: 0.8195\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0227 - accuracy: 0.8393 - val_loss: 0.0264 - val_accuracy: 0.8341\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0219 - accuracy: 0.8493 - val_loss: 0.0239 - val_accuracy: 0.8317\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0214 - accuracy: 0.8573 - val_loss: 0.0283 - val_accuracy: 0.7927\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0202 - accuracy: 0.8595 - val_loss: 0.0290 - val_accuracy: 0.8390\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0196 - accuracy: 0.8656 - val_loss: 0.0262 - val_accuracy: 0.8341\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0182 - accuracy: 0.8798 - val_loss: 0.0279 - val_accuracy: 0.8171\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0216 - accuracy: 0.8577 - val_loss: 0.0229 - val_accuracy: 0.8537\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0194 - accuracy: 0.8626 - val_loss: 0.0643 - val_accuracy: 0.5293\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0175 - accuracy: 0.8784 - val_loss: 0.0236 - val_accuracy: 0.8585\n",
            "==================================================\n",
            "Result of xception_model, fold 3\n",
            "Epoch: 53\n",
            "Accuracy: 0.8829268217086792\n",
            "Time taken:  559.514322757721\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 17s 116ms/step - loss: 0.0943 - accuracy: 0.1162 - val_loss: 0.0921 - val_accuracy: 0.1100\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0917 - accuracy: 0.1422 - val_loss: 0.0939 - val_accuracy: 0.1002\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0905 - accuracy: 0.1652 - val_loss: 0.1007 - val_accuracy: 0.1002\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0893 - accuracy: 0.1954 - val_loss: 0.0934 - val_accuracy: 0.0929\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0861 - accuracy: 0.2376 - val_loss: 0.0942 - val_accuracy: 0.1345\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0844 - accuracy: 0.2691 - val_loss: 0.1109 - val_accuracy: 0.1467\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0832 - accuracy: 0.2857 - val_loss: 0.0826 - val_accuracy: 0.2567\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0795 - accuracy: 0.3464 - val_loss: 0.1439 - val_accuracy: 0.1296\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0812 - accuracy: 0.3503 - val_loss: 0.1076 - val_accuracy: 0.1883\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0752 - accuracy: 0.3920 - val_loss: 0.0806 - val_accuracy: 0.3496\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0658 - accuracy: 0.5013 - val_loss: 0.0957 - val_accuracy: 0.3007\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0665 - accuracy: 0.5039 - val_loss: 0.0932 - val_accuracy: 0.3594\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0644 - accuracy: 0.5128 - val_loss: 0.0916 - val_accuracy: 0.3423\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0609 - accuracy: 0.5320 - val_loss: 0.0608 - val_accuracy: 0.5892\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0611 - accuracy: 0.5318 - val_loss: 0.0673 - val_accuracy: 0.5501\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0539 - accuracy: 0.6058 - val_loss: 0.0600 - val_accuracy: 0.6161\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0508 - accuracy: 0.6228 - val_loss: 0.0492 - val_accuracy: 0.6479\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0511 - accuracy: 0.6325 - val_loss: 0.0606 - val_accuracy: 0.5330\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0442 - accuracy: 0.6960 - val_loss: 0.1030 - val_accuracy: 0.3105\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0424 - accuracy: 0.7051 - val_loss: 0.0746 - val_accuracy: 0.4768\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0455 - accuracy: 0.6802 - val_loss: 0.0446 - val_accuracy: 0.6699\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0388 - accuracy: 0.7348 - val_loss: 0.0376 - val_accuracy: 0.7457\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0462 - accuracy: 0.6747 - val_loss: 0.0352 - val_accuracy: 0.7555\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0387 - accuracy: 0.7249 - val_loss: 0.0441 - val_accuracy: 0.6650\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0393 - accuracy: 0.7249 - val_loss: 0.0467 - val_accuracy: 0.6650\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0339 - accuracy: 0.7611 - val_loss: 0.0930 - val_accuracy: 0.3423\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0416 - accuracy: 0.6956 - val_loss: 0.0323 - val_accuracy: 0.7848\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0334 - accuracy: 0.7668 - val_loss: 0.0370 - val_accuracy: 0.7359\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0329 - accuracy: 0.7756 - val_loss: 0.0356 - val_accuracy: 0.7579\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0306 - accuracy: 0.7841 - val_loss: 0.0370 - val_accuracy: 0.7311\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0286 - accuracy: 0.8072 - val_loss: 0.0330 - val_accuracy: 0.7628\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0303 - accuracy: 0.7702 - val_loss: 0.0310 - val_accuracy: 0.7824\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0296 - accuracy: 0.8045 - val_loss: 0.0360 - val_accuracy: 0.7335\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0331 - accuracy: 0.7675 - val_loss: 0.0262 - val_accuracy: 0.8289\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0253 - accuracy: 0.8303 - val_loss: 0.0418 - val_accuracy: 0.6944\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0303 - accuracy: 0.7827 - val_loss: 0.0329 - val_accuracy: 0.7824\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0269 - accuracy: 0.8023 - val_loss: 0.0260 - val_accuracy: 0.8264\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0272 - accuracy: 0.8175 - val_loss: 0.0268 - val_accuracy: 0.8289\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0272 - accuracy: 0.8084 - val_loss: 0.0330 - val_accuracy: 0.7482\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0263 - accuracy: 0.8180 - val_loss: 0.0360 - val_accuracy: 0.7506\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0261 - accuracy: 0.8212 - val_loss: 0.0400 - val_accuracy: 0.7188\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0253 - accuracy: 0.8323 - val_loss: 0.0266 - val_accuracy: 0.8142\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0203 - accuracy: 0.8626 - val_loss: 0.0386 - val_accuracy: 0.6993\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0238 - accuracy: 0.8464 - val_loss: 0.0454 - val_accuracy: 0.6724\n",
            "==================================================\n",
            "Result of xception_model, fold 4\n",
            "Epoch: 44\n",
            "Accuracy: 0.8288508653640747\n",
            "Time taken:  469.01433277130127\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 15s 104ms/step - loss: 0.0948 - accuracy: 0.1113 - val_loss: 0.0936 - val_accuracy: 0.0758\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0918 - accuracy: 0.1496 - val_loss: 0.1011 - val_accuracy: 0.0685\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0903 - accuracy: 0.1696 - val_loss: 0.1014 - val_accuracy: 0.0685\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0868 - accuracy: 0.2371 - val_loss: 0.1013 - val_accuracy: 0.0685\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0867 - accuracy: 0.2172 - val_loss: 0.0926 - val_accuracy: 0.1051\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0858 - accuracy: 0.2633 - val_loss: 0.0816 - val_accuracy: 0.3105\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0811 - accuracy: 0.3289 - val_loss: 0.0745 - val_accuracy: 0.3985\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0828 - accuracy: 0.3092 - val_loss: 0.0841 - val_accuracy: 0.3545\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0770 - accuracy: 0.3886 - val_loss: 0.1166 - val_accuracy: 0.1638\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0767 - accuracy: 0.3805 - val_loss: 0.0956 - val_accuracy: 0.2934\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0710 - accuracy: 0.4391 - val_loss: 0.1028 - val_accuracy: 0.2763\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0631 - accuracy: 0.5110 - val_loss: 0.0986 - val_accuracy: 0.3301\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0618 - accuracy: 0.5236 - val_loss: 0.0744 - val_accuracy: 0.4963\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0586 - accuracy: 0.5499 - val_loss: 0.1077 - val_accuracy: 0.2372\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0606 - accuracy: 0.5486 - val_loss: 0.0727 - val_accuracy: 0.4694\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0547 - accuracy: 0.5881 - val_loss: 0.0856 - val_accuracy: 0.4205\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0552 - accuracy: 0.5965 - val_loss: 0.1372 - val_accuracy: 0.2176\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0522 - accuracy: 0.6240 - val_loss: 0.0408 - val_accuracy: 0.7066\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 10s 100ms/step - loss: 0.0470 - accuracy: 0.6531 - val_loss: 0.0410 - val_accuracy: 0.7139\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0425 - accuracy: 0.6931 - val_loss: 0.0667 - val_accuracy: 0.5257\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0480 - accuracy: 0.6646 - val_loss: 0.0379 - val_accuracy: 0.7433\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0406 - accuracy: 0.7143 - val_loss: 0.0429 - val_accuracy: 0.6870\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0404 - accuracy: 0.7017 - val_loss: 0.0554 - val_accuracy: 0.5648\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0373 - accuracy: 0.7291 - val_loss: 0.0369 - val_accuracy: 0.7359\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0382 - accuracy: 0.7276 - val_loss: 0.0422 - val_accuracy: 0.7042\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0337 - accuracy: 0.7618 - val_loss: 0.0296 - val_accuracy: 0.7922\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0323 - accuracy: 0.7799 - val_loss: 0.0332 - val_accuracy: 0.7751\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0341 - accuracy: 0.7712 - val_loss: 0.0365 - val_accuracy: 0.7482\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0334 - accuracy: 0.7678 - val_loss: 0.0463 - val_accuracy: 0.6455\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0328 - accuracy: 0.7731 - val_loss: 0.0807 - val_accuracy: 0.4401\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0287 - accuracy: 0.8077 - val_loss: 0.0270 - val_accuracy: 0.8068\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0300 - accuracy: 0.7839 - val_loss: 0.0521 - val_accuracy: 0.5844\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0323 - accuracy: 0.7824 - val_loss: 0.0276 - val_accuracy: 0.8289\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0290 - accuracy: 0.8065 - val_loss: 0.0289 - val_accuracy: 0.8240\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0281 - accuracy: 0.8105 - val_loss: 0.0302 - val_accuracy: 0.8044\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0259 - accuracy: 0.8178 - val_loss: 0.0243 - val_accuracy: 0.8509\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0248 - accuracy: 0.8263 - val_loss: 0.0302 - val_accuracy: 0.8411\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0276 - accuracy: 0.8139 - val_loss: 0.0438 - val_accuracy: 0.7017\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0269 - accuracy: 0.8242 - val_loss: 0.0256 - val_accuracy: 0.8313\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0240 - accuracy: 0.8303 - val_loss: 0.0380 - val_accuracy: 0.7408\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0246 - accuracy: 0.8297 - val_loss: 0.0557 - val_accuracy: 0.5844\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0238 - accuracy: 0.8421 - val_loss: 0.0333 - val_accuracy: 0.7653\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0230 - accuracy: 0.8450 - val_loss: 0.0242 - val_accuracy: 0.8484\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0224 - accuracy: 0.8443 - val_loss: 0.0230 - val_accuracy: 0.8655\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0214 - accuracy: 0.8534 - val_loss: 0.0339 - val_accuracy: 0.7628\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 10s 97ms/step - loss: 0.0199 - accuracy: 0.8688 - val_loss: 0.0198 - val_accuracy: 0.8851\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0219 - accuracy: 0.8424 - val_loss: 0.0253 - val_accuracy: 0.8460\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0220 - accuracy: 0.8529 - val_loss: 0.0375 - val_accuracy: 0.7164\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0208 - accuracy: 0.8627 - val_loss: 0.0271 - val_accuracy: 0.8435\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0182 - accuracy: 0.8753 - val_loss: 0.0264 - val_accuracy: 0.8240\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0196 - accuracy: 0.8619 - val_loss: 0.0353 - val_accuracy: 0.7677\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0180 - accuracy: 0.8827 - val_loss: 0.0256 - val_accuracy: 0.8484\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0187 - accuracy: 0.8752 - val_loss: 0.0252 - val_accuracy: 0.8386\n",
            "Epoch 54/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0171 - accuracy: 0.8810 - val_loss: 0.0261 - val_accuracy: 0.8093\n",
            "Epoch 55/300\n",
            "103/103 [==============================] - 10s 99ms/step - loss: 0.0203 - accuracy: 0.8714 - val_loss: 0.0238 - val_accuracy: 0.8631\n",
            "Epoch 56/300\n",
            "103/103 [==============================] - 10s 98ms/step - loss: 0.0196 - accuracy: 0.8748 - val_loss: 0.0215 - val_accuracy: 0.8704\n",
            "==================================================\n",
            "Result of xception_model, fold 5\n",
            "Epoch: 56\n",
            "Accuracy: 0.8850855827331543\n",
            "Time taken:  588.9199087619781\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 20s 106ms/step - loss: 0.0962 - accuracy: 0.1099 - val_loss: 0.0975 - val_accuracy: 0.1049\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0918 - accuracy: 0.1375 - val_loss: 0.1037 - val_accuracy: 0.1049\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0911 - accuracy: 0.1398 - val_loss: 0.1013 - val_accuracy: 0.1073\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0895 - accuracy: 0.1732 - val_loss: 0.1028 - val_accuracy: 0.1195\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0890 - accuracy: 0.1773 - val_loss: 0.1257 - val_accuracy: 0.1366\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0864 - accuracy: 0.2537 - val_loss: 0.0876 - val_accuracy: 0.2488\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0854 - accuracy: 0.2370 - val_loss: 0.1017 - val_accuracy: 0.2000\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0849 - accuracy: 0.2617 - val_loss: 0.0885 - val_accuracy: 0.2829\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0859 - accuracy: 0.2748 - val_loss: 0.1103 - val_accuracy: 0.0878\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0852 - accuracy: 0.2499 - val_loss: 0.1072 - val_accuracy: 0.2268\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0838 - accuracy: 0.2791 - val_loss: 0.0876 - val_accuracy: 0.3024\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0822 - accuracy: 0.3052 - val_loss: 0.0882 - val_accuracy: 0.3268\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0831 - accuracy: 0.2705 - val_loss: 0.0969 - val_accuracy: 0.2463\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0806 - accuracy: 0.3270 - val_loss: 0.1037 - val_accuracy: 0.2561\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 7s 69ms/step - loss: 0.0786 - accuracy: 0.3598 - val_loss: 0.0759 - val_accuracy: 0.3878\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0777 - accuracy: 0.3359 - val_loss: 0.1277 - val_accuracy: 0.2171\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0797 - accuracy: 0.3522 - val_loss: 0.0697 - val_accuracy: 0.4390\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0739 - accuracy: 0.4208 - val_loss: 0.0941 - val_accuracy: 0.3171\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0750 - accuracy: 0.3893 - val_loss: 0.0786 - val_accuracy: 0.4366\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0698 - accuracy: 0.4621 - val_loss: 0.0762 - val_accuracy: 0.4585\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0701 - accuracy: 0.4572 - val_loss: 0.0659 - val_accuracy: 0.5122\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0644 - accuracy: 0.5203 - val_loss: 0.0644 - val_accuracy: 0.5415\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0631 - accuracy: 0.5190 - val_loss: 0.0881 - val_accuracy: 0.4000\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0612 - accuracy: 0.5516 - val_loss: 0.0677 - val_accuracy: 0.5195\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 7s 69ms/step - loss: 0.0583 - accuracy: 0.5605 - val_loss: 0.0752 - val_accuracy: 0.4390\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0563 - accuracy: 0.5864 - val_loss: 0.0612 - val_accuracy: 0.5220\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0609 - accuracy: 0.5475 - val_loss: 0.0609 - val_accuracy: 0.5659\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0582 - accuracy: 0.5764 - val_loss: 0.0550 - val_accuracy: 0.6244\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0577 - accuracy: 0.5942 - val_loss: 0.0489 - val_accuracy: 0.6585\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0549 - accuracy: 0.6094 - val_loss: 0.0400 - val_accuracy: 0.7341\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0482 - accuracy: 0.6545 - val_loss: 0.0494 - val_accuracy: 0.6829\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 8s 74ms/step - loss: 0.0493 - accuracy: 0.6471 - val_loss: 0.0356 - val_accuracy: 0.7610\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0487 - accuracy: 0.6595 - val_loss: 0.0498 - val_accuracy: 0.6878\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0513 - accuracy: 0.6323 - val_loss: 0.0545 - val_accuracy: 0.6073\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0445 - accuracy: 0.6876 - val_loss: 0.0735 - val_accuracy: 0.4780\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0471 - accuracy: 0.6637 - val_loss: 0.0436 - val_accuracy: 0.6780\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0447 - accuracy: 0.6838 - val_loss: 0.0609 - val_accuracy: 0.6000\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0445 - accuracy: 0.6834 - val_loss: 0.0416 - val_accuracy: 0.7220\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0393 - accuracy: 0.7311 - val_loss: 0.0402 - val_accuracy: 0.7073\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0419 - accuracy: 0.7193 - val_loss: 0.0569 - val_accuracy: 0.5927\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0404 - accuracy: 0.7289 - val_loss: 0.0330 - val_accuracy: 0.7707\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0421 - accuracy: 0.7185 - val_loss: 0.0437 - val_accuracy: 0.7024\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0393 - accuracy: 0.7208 - val_loss: 0.0419 - val_accuracy: 0.7049\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 7s 69ms/step - loss: 0.0361 - accuracy: 0.7658 - val_loss: 0.0623 - val_accuracy: 0.5585\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0411 - accuracy: 0.7163 - val_loss: 0.0389 - val_accuracy: 0.7341\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0340 - accuracy: 0.7680 - val_loss: 0.0873 - val_accuracy: 0.4463\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0408 - accuracy: 0.7237 - val_loss: 0.0395 - val_accuracy: 0.7537\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0354 - accuracy: 0.7604 - val_loss: 0.0326 - val_accuracy: 0.7780\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0337 - accuracy: 0.7728 - val_loss: 0.0264 - val_accuracy: 0.8171\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0331 - accuracy: 0.7701 - val_loss: 0.0337 - val_accuracy: 0.7585\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0351 - accuracy: 0.7555 - val_loss: 0.0339 - val_accuracy: 0.7732\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0330 - accuracy: 0.7755 - val_loss: 0.0369 - val_accuracy: 0.7512\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0327 - accuracy: 0.7811 - val_loss: 0.0300 - val_accuracy: 0.7927\n",
            "Epoch 54/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0294 - accuracy: 0.8129 - val_loss: 0.0308 - val_accuracy: 0.8000\n",
            "Epoch 55/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0371 - accuracy: 0.7447 - val_loss: 0.0596 - val_accuracy: 0.5927\n",
            "Epoch 56/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0295 - accuracy: 0.8014 - val_loss: 0.0256 - val_accuracy: 0.8293\n",
            "Epoch 57/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0317 - accuracy: 0.7879 - val_loss: 0.0546 - val_accuracy: 0.6098\n",
            "Epoch 58/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0322 - accuracy: 0.7877 - val_loss: 0.0383 - val_accuracy: 0.7366\n",
            "Epoch 59/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0300 - accuracy: 0.7983 - val_loss: 0.0399 - val_accuracy: 0.7220\n",
            "Epoch 60/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0360 - accuracy: 0.7520 - val_loss: 0.0379 - val_accuracy: 0.7268\n",
            "Epoch 61/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0307 - accuracy: 0.7957 - val_loss: 0.0284 - val_accuracy: 0.8098\n",
            "Epoch 62/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0286 - accuracy: 0.8093 - val_loss: 0.0554 - val_accuracy: 0.6341\n",
            "Epoch 63/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0295 - accuracy: 0.8033 - val_loss: 0.0357 - val_accuracy: 0.7707\n",
            "Epoch 64/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0246 - accuracy: 0.8339 - val_loss: 0.0287 - val_accuracy: 0.8098\n",
            "Epoch 65/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0247 - accuracy: 0.8289 - val_loss: 0.0223 - val_accuracy: 0.8512\n",
            "Epoch 66/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0250 - accuracy: 0.8319 - val_loss: 0.0315 - val_accuracy: 0.7829\n",
            "Epoch 67/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0245 - accuracy: 0.8286 - val_loss: 0.0354 - val_accuracy: 0.7512\n",
            "Epoch 68/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0242 - accuracy: 0.8385 - val_loss: 0.0266 - val_accuracy: 0.8220\n",
            "Epoch 69/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0287 - accuracy: 0.7935 - val_loss: 0.0209 - val_accuracy: 0.8561\n",
            "Epoch 70/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0248 - accuracy: 0.8261 - val_loss: 0.0287 - val_accuracy: 0.7976\n",
            "Epoch 71/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0203 - accuracy: 0.8661 - val_loss: 0.0370 - val_accuracy: 0.7537\n",
            "Epoch 72/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0270 - accuracy: 0.8102 - val_loss: 0.0199 - val_accuracy: 0.8659\n",
            "Epoch 73/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0251 - accuracy: 0.8365 - val_loss: 0.0276 - val_accuracy: 0.8098\n",
            "Epoch 74/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0268 - accuracy: 0.8250 - val_loss: 0.0228 - val_accuracy: 0.8512\n",
            "Epoch 75/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0238 - accuracy: 0.8425 - val_loss: 0.0212 - val_accuracy: 0.8561\n",
            "Epoch 76/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0260 - accuracy: 0.8295 - val_loss: 0.0221 - val_accuracy: 0.8488\n",
            "Epoch 77/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0241 - accuracy: 0.8418 - val_loss: 0.0285 - val_accuracy: 0.8098\n",
            "Epoch 78/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0264 - accuracy: 0.8163 - val_loss: 0.0222 - val_accuracy: 0.8537\n",
            "Epoch 79/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0247 - accuracy: 0.8444 - val_loss: 0.0377 - val_accuracy: 0.7390\n",
            "Epoch 80/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0225 - accuracy: 0.8447 - val_loss: 0.0225 - val_accuracy: 0.8561\n",
            "Epoch 81/300\n",
            "103/103 [==============================] - 8s 74ms/step - loss: 0.0216 - accuracy: 0.8582 - val_loss: 0.0238 - val_accuracy: 0.8585\n",
            "Epoch 82/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0224 - accuracy: 0.8569 - val_loss: 0.0304 - val_accuracy: 0.7854\n",
            "==================================================\n",
            "Result of inception_model, fold 1\n",
            "Epoch: 82\n",
            "Accuracy: 0.8658536672592163\n",
            "Time taken:  651.4394624233246\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 16s 82ms/step - loss: 0.0958 - accuracy: 0.1001 - val_loss: 0.0916 - val_accuracy: 0.0878\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0913 - accuracy: 0.1567 - val_loss: 0.0957 - val_accuracy: 0.0780\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0908 - accuracy: 0.1563 - val_loss: 0.0935 - val_accuracy: 0.0878\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 8s 74ms/step - loss: 0.0899 - accuracy: 0.1701 - val_loss: 0.1816 - val_accuracy: 0.0878\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0886 - accuracy: 0.1861 - val_loss: 0.1412 - val_accuracy: 0.0927\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0901 - accuracy: 0.1816 - val_loss: 0.1181 - val_accuracy: 0.2024\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0882 - accuracy: 0.1756 - val_loss: 0.1162 - val_accuracy: 0.1756\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0870 - accuracy: 0.2139 - val_loss: 0.0934 - val_accuracy: 0.2293\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0857 - accuracy: 0.2485 - val_loss: 0.0794 - val_accuracy: 0.3317\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 8s 74ms/step - loss: 0.0843 - accuracy: 0.2760 - val_loss: 0.0866 - val_accuracy: 0.3537\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0806 - accuracy: 0.3195 - val_loss: 0.1092 - val_accuracy: 0.2146\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0820 - accuracy: 0.3094 - val_loss: 0.0753 - val_accuracy: 0.4537\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0798 - accuracy: 0.3327 - val_loss: 0.1099 - val_accuracy: 0.2512\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0804 - accuracy: 0.3416 - val_loss: 0.0648 - val_accuracy: 0.4756\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0788 - accuracy: 0.3851 - val_loss: 0.0834 - val_accuracy: 0.4195\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0726 - accuracy: 0.4347 - val_loss: 0.0827 - val_accuracy: 0.4268\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0743 - accuracy: 0.3938 - val_loss: 0.0712 - val_accuracy: 0.4244\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 7s 69ms/step - loss: 0.0768 - accuracy: 0.4004 - val_loss: 0.0625 - val_accuracy: 0.5244\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0719 - accuracy: 0.4466 - val_loss: 0.0693 - val_accuracy: 0.4561\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0669 - accuracy: 0.4905 - val_loss: 0.0643 - val_accuracy: 0.4951\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0661 - accuracy: 0.5085 - val_loss: 0.0615 - val_accuracy: 0.5439\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0669 - accuracy: 0.5106 - val_loss: 0.0541 - val_accuracy: 0.6220\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0628 - accuracy: 0.5427 - val_loss: 0.0506 - val_accuracy: 0.6439\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0587 - accuracy: 0.5797 - val_loss: 0.1634 - val_accuracy: 0.1220\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0582 - accuracy: 0.5831 - val_loss: 0.0404 - val_accuracy: 0.7220\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0584 - accuracy: 0.5807 - val_loss: 0.0519 - val_accuracy: 0.6488\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0551 - accuracy: 0.6043 - val_loss: 0.0779 - val_accuracy: 0.4439\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0546 - accuracy: 0.6009 - val_loss: 0.0489 - val_accuracy: 0.6537\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0534 - accuracy: 0.6218 - val_loss: 0.0426 - val_accuracy: 0.7146\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0521 - accuracy: 0.6202 - val_loss: 0.0388 - val_accuracy: 0.7220\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0507 - accuracy: 0.6295 - val_loss: 0.0368 - val_accuracy: 0.7488\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0519 - accuracy: 0.6275 - val_loss: 0.0424 - val_accuracy: 0.7220\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0487 - accuracy: 0.6425 - val_loss: 0.0388 - val_accuracy: 0.7195\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0480 - accuracy: 0.6607 - val_loss: 0.0434 - val_accuracy: 0.6927\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0483 - accuracy: 0.6637 - val_loss: 0.0544 - val_accuracy: 0.6122\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0469 - accuracy: 0.6716 - val_loss: 0.0564 - val_accuracy: 0.5707\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0462 - accuracy: 0.6760 - val_loss: 0.0442 - val_accuracy: 0.6854\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0405 - accuracy: 0.7253 - val_loss: 0.0599 - val_accuracy: 0.5659\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0439 - accuracy: 0.6952 - val_loss: 0.0333 - val_accuracy: 0.7707\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0399 - accuracy: 0.7124 - val_loss: 0.0559 - val_accuracy: 0.5829\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 7s 68ms/step - loss: 0.0410 - accuracy: 0.7211 - val_loss: 0.0321 - val_accuracy: 0.7805\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0384 - accuracy: 0.7359 - val_loss: 0.0381 - val_accuracy: 0.7585\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0379 - accuracy: 0.7367 - val_loss: 0.0343 - val_accuracy: 0.7659\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 7s 68ms/step - loss: 0.0387 - accuracy: 0.7280 - val_loss: 0.0299 - val_accuracy: 0.8098\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0449 - accuracy: 0.6713 - val_loss: 0.0372 - val_accuracy: 0.7439\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0372 - accuracy: 0.7423 - val_loss: 0.0371 - val_accuracy: 0.7439\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 7s 68ms/step - loss: 0.0352 - accuracy: 0.7607 - val_loss: 0.0432 - val_accuracy: 0.6902\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 7s 68ms/step - loss: 0.0381 - accuracy: 0.7347 - val_loss: 0.0307 - val_accuracy: 0.7878\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0321 - accuracy: 0.7847 - val_loss: 0.0373 - val_accuracy: 0.7146\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 7s 69ms/step - loss: 0.0346 - accuracy: 0.7754 - val_loss: 0.0317 - val_accuracy: 0.7927\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0369 - accuracy: 0.7419 - val_loss: 0.0458 - val_accuracy: 0.6878\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0361 - accuracy: 0.7574 - val_loss: 0.0406 - val_accuracy: 0.7122\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0303 - accuracy: 0.7888 - val_loss: 0.0360 - val_accuracy: 0.7683\n",
            "Epoch 54/300\n",
            "103/103 [==============================] - 7s 69ms/step - loss: 0.0324 - accuracy: 0.7839 - val_loss: 0.0304 - val_accuracy: 0.7732\n",
            "==================================================\n",
            "Result of inception_model, fold 2\n",
            "Epoch: 54\n",
            "Accuracy: 0.8097561001777649\n",
            "Time taken:  430.46444725990295\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 16s 89ms/step - loss: 0.0962 - accuracy: 0.0982 - val_loss: 0.0905 - val_accuracy: 0.1024\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0920 - accuracy: 0.1314 - val_loss: 0.0930 - val_accuracy: 0.0829\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0903 - accuracy: 0.1779 - val_loss: 0.0938 - val_accuracy: 0.0976\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0889 - accuracy: 0.1978 - val_loss: 0.1167 - val_accuracy: 0.0976\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0888 - accuracy: 0.1982 - val_loss: 0.0838 - val_accuracy: 0.2561\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0883 - accuracy: 0.2179 - val_loss: 0.0915 - val_accuracy: 0.2439\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0867 - accuracy: 0.2435 - val_loss: 0.0848 - val_accuracy: 0.2756\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0848 - accuracy: 0.2490 - val_loss: 0.0893 - val_accuracy: 0.2585\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0837 - accuracy: 0.2874 - val_loss: 0.0801 - val_accuracy: 0.3024\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0840 - accuracy: 0.2556 - val_loss: 0.1166 - val_accuracy: 0.1732\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0833 - accuracy: 0.2934 - val_loss: 0.0781 - val_accuracy: 0.3488\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0814 - accuracy: 0.3200 - val_loss: 0.0807 - val_accuracy: 0.3683\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0825 - accuracy: 0.3088 - val_loss: 0.0767 - val_accuracy: 0.3805\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0810 - accuracy: 0.3280 - val_loss: 0.0648 - val_accuracy: 0.4756\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0773 - accuracy: 0.3812 - val_loss: 0.0822 - val_accuracy: 0.3390\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0759 - accuracy: 0.3737 - val_loss: 0.0687 - val_accuracy: 0.4927\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0731 - accuracy: 0.4323 - val_loss: 0.0849 - val_accuracy: 0.3659\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0771 - accuracy: 0.3720 - val_loss: 0.0661 - val_accuracy: 0.5561\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0733 - accuracy: 0.4325 - val_loss: 0.0577 - val_accuracy: 0.5805\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0738 - accuracy: 0.4297 - val_loss: 0.0850 - val_accuracy: 0.3537\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0686 - accuracy: 0.4966 - val_loss: 0.0768 - val_accuracy: 0.4683\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0661 - accuracy: 0.4923 - val_loss: 0.0531 - val_accuracy: 0.6268\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0631 - accuracy: 0.5309 - val_loss: 0.0559 - val_accuracy: 0.5707\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0630 - accuracy: 0.5365 - val_loss: 0.1265 - val_accuracy: 0.2634\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0607 - accuracy: 0.5519 - val_loss: 0.1018 - val_accuracy: 0.3122\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0559 - accuracy: 0.6071 - val_loss: 0.0396 - val_accuracy: 0.7341\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0549 - accuracy: 0.6135 - val_loss: 0.0535 - val_accuracy: 0.6220\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0577 - accuracy: 0.5870 - val_loss: 0.0489 - val_accuracy: 0.6610\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0542 - accuracy: 0.6092 - val_loss: 0.0550 - val_accuracy: 0.6195\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0487 - accuracy: 0.6400 - val_loss: 0.0660 - val_accuracy: 0.5805\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0499 - accuracy: 0.6488 - val_loss: 0.1278 - val_accuracy: 0.2732\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0525 - accuracy: 0.6257 - val_loss: 0.0358 - val_accuracy: 0.7585\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0475 - accuracy: 0.6595 - val_loss: 0.0414 - val_accuracy: 0.7268\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0465 - accuracy: 0.6632 - val_loss: 0.0438 - val_accuracy: 0.7146\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0483 - accuracy: 0.6593 - val_loss: 0.0486 - val_accuracy: 0.6585\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0447 - accuracy: 0.6938 - val_loss: 0.0333 - val_accuracy: 0.7683\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 8s 74ms/step - loss: 0.0442 - accuracy: 0.6858 - val_loss: 0.0811 - val_accuracy: 0.4829\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0434 - accuracy: 0.6997 - val_loss: 0.0476 - val_accuracy: 0.6634\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0449 - accuracy: 0.6753 - val_loss: 0.0347 - val_accuracy: 0.7634\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0392 - accuracy: 0.7317 - val_loss: 0.0393 - val_accuracy: 0.7293\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0401 - accuracy: 0.7216 - val_loss: 0.0339 - val_accuracy: 0.7829\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0388 - accuracy: 0.7258 - val_loss: 0.0472 - val_accuracy: 0.6902\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0371 - accuracy: 0.7450 - val_loss: 0.0393 - val_accuracy: 0.7293\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0348 - accuracy: 0.7600 - val_loss: 0.0461 - val_accuracy: 0.6610\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0367 - accuracy: 0.7424 - val_loss: 0.0327 - val_accuracy: 0.7902\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0393 - accuracy: 0.7315 - val_loss: 0.0336 - val_accuracy: 0.7780\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0381 - accuracy: 0.7451 - val_loss: 0.0293 - val_accuracy: 0.7951\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0352 - accuracy: 0.7547 - val_loss: 0.0243 - val_accuracy: 0.8293\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0331 - accuracy: 0.7732 - val_loss: 0.0315 - val_accuracy: 0.7707\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0341 - accuracy: 0.7675 - val_loss: 0.0332 - val_accuracy: 0.7659\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0374 - accuracy: 0.7396 - val_loss: 0.0354 - val_accuracy: 0.7561\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0319 - accuracy: 0.7834 - val_loss: 0.0266 - val_accuracy: 0.8293\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0310 - accuracy: 0.7895 - val_loss: 0.0315 - val_accuracy: 0.7780\n",
            "Epoch 54/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0323 - accuracy: 0.7869 - val_loss: 0.0258 - val_accuracy: 0.8171\n",
            "Epoch 55/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0314 - accuracy: 0.7902 - val_loss: 0.0365 - val_accuracy: 0.7366\n",
            "Epoch 56/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0308 - accuracy: 0.7919 - val_loss: 0.0343 - val_accuracy: 0.7780\n",
            "Epoch 57/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0328 - accuracy: 0.7787 - val_loss: 0.0234 - val_accuracy: 0.8463\n",
            "Epoch 58/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0299 - accuracy: 0.7963 - val_loss: 0.0300 - val_accuracy: 0.7878\n",
            "Epoch 59/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0275 - accuracy: 0.8089 - val_loss: 0.0230 - val_accuracy: 0.8415\n",
            "Epoch 60/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0306 - accuracy: 0.7901 - val_loss: 0.0310 - val_accuracy: 0.7732\n",
            "Epoch 61/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0291 - accuracy: 0.8070 - val_loss: 0.0317 - val_accuracy: 0.7780\n",
            "Epoch 62/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0287 - accuracy: 0.8061 - val_loss: 0.0265 - val_accuracy: 0.8341\n",
            "Epoch 63/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0273 - accuracy: 0.8132 - val_loss: 0.0244 - val_accuracy: 0.8415\n",
            "Epoch 64/300\n",
            "103/103 [==============================] - 8s 74ms/step - loss: 0.0277 - accuracy: 0.8184 - val_loss: 0.0361 - val_accuracy: 0.7512\n",
            "Epoch 65/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0318 - accuracy: 0.7866 - val_loss: 0.0249 - val_accuracy: 0.8366\n",
            "Epoch 66/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0253 - accuracy: 0.8237 - val_loss: 0.0343 - val_accuracy: 0.7756\n",
            "Epoch 67/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0246 - accuracy: 0.8322 - val_loss: 0.0328 - val_accuracy: 0.7878\n",
            "==================================================\n",
            "Result of inception_model, fold 3\n",
            "Epoch: 67\n",
            "Accuracy: 0.8463414907455444\n",
            "Time taken:  535.1030848026276\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 18s 108ms/step - loss: 0.0961 - accuracy: 0.0923 - val_loss: 0.0910 - val_accuracy: 0.0587\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0932 - accuracy: 0.1186 - val_loss: 0.0902 - val_accuracy: 0.1076\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0918 - accuracy: 0.1384 - val_loss: 0.0975 - val_accuracy: 0.0929\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0904 - accuracy: 0.1578 - val_loss: 0.1077 - val_accuracy: 0.0929\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0898 - accuracy: 0.1919 - val_loss: 0.1046 - val_accuracy: 0.1002\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0889 - accuracy: 0.1761 - val_loss: 0.1645 - val_accuracy: 0.1051\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0862 - accuracy: 0.2222 - val_loss: 0.1084 - val_accuracy: 0.2103\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0892 - accuracy: 0.2033 - val_loss: 0.1297 - val_accuracy: 0.1711\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0862 - accuracy: 0.2289 - val_loss: 0.0905 - val_accuracy: 0.2029\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0844 - accuracy: 0.2476 - val_loss: 0.0858 - val_accuracy: 0.2445\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0835 - accuracy: 0.2764 - val_loss: 0.1190 - val_accuracy: 0.1711\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0825 - accuracy: 0.2919 - val_loss: 0.0891 - val_accuracy: 0.2983\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0837 - accuracy: 0.2812 - val_loss: 0.0909 - val_accuracy: 0.2714\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0829 - accuracy: 0.2794 - val_loss: 0.0886 - val_accuracy: 0.2714\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0793 - accuracy: 0.3608 - val_loss: 0.1113 - val_accuracy: 0.1736\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0786 - accuracy: 0.3457 - val_loss: 0.0800 - val_accuracy: 0.4132\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0760 - accuracy: 0.3651 - val_loss: 0.0702 - val_accuracy: 0.4548\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0775 - accuracy: 0.3742 - val_loss: 0.1264 - val_accuracy: 0.3007\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0771 - accuracy: 0.3659 - val_loss: 0.0748 - val_accuracy: 0.4230\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0736 - accuracy: 0.4207 - val_loss: 0.0706 - val_accuracy: 0.4132\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0772 - accuracy: 0.3567 - val_loss: 0.1161 - val_accuracy: 0.2836\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0722 - accuracy: 0.4148 - val_loss: 0.1079 - val_accuracy: 0.3545\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0728 - accuracy: 0.4229 - val_loss: 0.0653 - val_accuracy: 0.5061\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0711 - accuracy: 0.4536 - val_loss: 0.0658 - val_accuracy: 0.4914\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0731 - accuracy: 0.4180 - val_loss: 0.0673 - val_accuracy: 0.4450\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0665 - accuracy: 0.4805 - val_loss: 0.0572 - val_accuracy: 0.5623\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0701 - accuracy: 0.4299 - val_loss: 0.0959 - val_accuracy: 0.3399\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0634 - accuracy: 0.5168 - val_loss: 0.0722 - val_accuracy: 0.4792\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0609 - accuracy: 0.5365 - val_loss: 0.0581 - val_accuracy: 0.5697\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0635 - accuracy: 0.5068 - val_loss: 0.0458 - val_accuracy: 0.6944\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0564 - accuracy: 0.5849 - val_loss: 0.0538 - val_accuracy: 0.6406\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0579 - accuracy: 0.5771 - val_loss: 0.0434 - val_accuracy: 0.7066\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0558 - accuracy: 0.5838 - val_loss: 0.0425 - val_accuracy: 0.6944\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0531 - accuracy: 0.6163 - val_loss: 0.0390 - val_accuracy: 0.7213\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0509 - accuracy: 0.6323 - val_loss: 0.0440 - val_accuracy: 0.6822\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0513 - accuracy: 0.6307 - val_loss: 0.0532 - val_accuracy: 0.6161\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0473 - accuracy: 0.6715 - val_loss: 0.0620 - val_accuracy: 0.5819\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0525 - accuracy: 0.6172 - val_loss: 0.0338 - val_accuracy: 0.7677\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 8s 74ms/step - loss: 0.0427 - accuracy: 0.7091 - val_loss: 0.0477 - val_accuracy: 0.6675\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0448 - accuracy: 0.6784 - val_loss: 0.0341 - val_accuracy: 0.7897\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0435 - accuracy: 0.6855 - val_loss: 0.0324 - val_accuracy: 0.7531\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0373 - accuracy: 0.7379 - val_loss: 0.0668 - val_accuracy: 0.5452\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0451 - accuracy: 0.6837 - val_loss: 0.0435 - val_accuracy: 0.7066\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0403 - accuracy: 0.7285 - val_loss: 0.0264 - val_accuracy: 0.8166\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0391 - accuracy: 0.7400 - val_loss: 0.0269 - val_accuracy: 0.8044\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0419 - accuracy: 0.7132 - val_loss: 0.0415 - val_accuracy: 0.6968\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0415 - accuracy: 0.7025 - val_loss: 0.0604 - val_accuracy: 0.5501\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 8s 74ms/step - loss: 0.0372 - accuracy: 0.7396 - val_loss: 0.0420 - val_accuracy: 0.6846\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0409 - accuracy: 0.7277 - val_loss: 0.0203 - val_accuracy: 0.8655\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0330 - accuracy: 0.7808 - val_loss: 0.0642 - val_accuracy: 0.5281\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0375 - accuracy: 0.7436 - val_loss: 0.0367 - val_accuracy: 0.7286\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0362 - accuracy: 0.7488 - val_loss: 0.0371 - val_accuracy: 0.7457\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0335 - accuracy: 0.7615 - val_loss: 0.0341 - val_accuracy: 0.7653\n",
            "Epoch 54/300\n",
            "103/103 [==============================] - 8s 75ms/step - loss: 0.0345 - accuracy: 0.7634 - val_loss: 0.0323 - val_accuracy: 0.7946\n",
            "Epoch 55/300\n",
            "103/103 [==============================] - 8s 74ms/step - loss: 0.0319 - accuracy: 0.7844 - val_loss: 0.0271 - val_accuracy: 0.8264\n",
            "Epoch 56/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0334 - accuracy: 0.7797 - val_loss: 0.0282 - val_accuracy: 0.8117\n",
            "Epoch 57/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0317 - accuracy: 0.7850 - val_loss: 0.0391 - val_accuracy: 0.7262\n",
            "Epoch 58/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0322 - accuracy: 0.7822 - val_loss: 0.0268 - val_accuracy: 0.8362\n",
            "Epoch 59/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0317 - accuracy: 0.7810 - val_loss: 0.0459 - val_accuracy: 0.6724\n",
            "==================================================\n",
            "Result of inception_model, fold 4\n",
            "Epoch: 59\n",
            "Accuracy: 0.8655256628990173\n",
            "Time taken:  476.5214352607727\n",
            "==================================================\n",
            "Epoch 1/300\n",
            "103/103 [==============================] - 16s 85ms/step - loss: 0.0953 - accuracy: 0.1209 - val_loss: 0.0981 - val_accuracy: 0.1076\n",
            "Epoch 2/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0915 - accuracy: 0.1211 - val_loss: 0.0930 - val_accuracy: 0.1076\n",
            "Epoch 3/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0922 - accuracy: 0.1534 - val_loss: 0.0967 - val_accuracy: 0.1076\n",
            "Epoch 4/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0905 - accuracy: 0.1745 - val_loss: 0.1176 - val_accuracy: 0.1076\n",
            "Epoch 5/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0892 - accuracy: 0.1809 - val_loss: 0.0927 - val_accuracy: 0.1760\n",
            "Epoch 6/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0880 - accuracy: 0.2129 - val_loss: 0.1050 - val_accuracy: 0.2005\n",
            "Epoch 7/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0884 - accuracy: 0.2036 - val_loss: 0.1012 - val_accuracy: 0.1467\n",
            "Epoch 8/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0881 - accuracy: 0.2102 - val_loss: 0.0962 - val_accuracy: 0.1614\n",
            "Epoch 9/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0858 - accuracy: 0.2329 - val_loss: 0.1115 - val_accuracy: 0.1711\n",
            "Epoch 10/300\n",
            "103/103 [==============================] - 8s 74ms/step - loss: 0.0875 - accuracy: 0.2216 - val_loss: 0.0875 - val_accuracy: 0.2787\n",
            "Epoch 11/300\n",
            "103/103 [==============================] - 8s 74ms/step - loss: 0.0842 - accuracy: 0.2717 - val_loss: 0.0757 - val_accuracy: 0.3472\n",
            "Epoch 12/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0842 - accuracy: 0.2712 - val_loss: 0.0750 - val_accuracy: 0.3961\n",
            "Epoch 13/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0833 - accuracy: 0.2672 - val_loss: 0.0883 - val_accuracy: 0.3570\n",
            "Epoch 14/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0828 - accuracy: 0.3021 - val_loss: 0.1238 - val_accuracy: 0.2103\n",
            "Epoch 15/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0818 - accuracy: 0.2998 - val_loss: 0.0876 - val_accuracy: 0.2372\n",
            "Epoch 16/300\n",
            "103/103 [==============================] - 8s 74ms/step - loss: 0.0816 - accuracy: 0.3101 - val_loss: 0.1025 - val_accuracy: 0.2812\n",
            "Epoch 17/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0836 - accuracy: 0.2726 - val_loss: 0.0857 - val_accuracy: 0.2983\n",
            "Epoch 18/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0820 - accuracy: 0.2872 - val_loss: 0.0834 - val_accuracy: 0.2225\n",
            "Epoch 19/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0797 - accuracy: 0.3343 - val_loss: 0.1029 - val_accuracy: 0.1491\n",
            "Epoch 20/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0812 - accuracy: 0.2850 - val_loss: 0.1069 - val_accuracy: 0.3350\n",
            "Epoch 21/300\n",
            "103/103 [==============================] - 8s 74ms/step - loss: 0.0873 - accuracy: 0.2267 - val_loss: 0.0941 - val_accuracy: 0.1687\n",
            "Epoch 22/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0808 - accuracy: 0.3212 - val_loss: 0.0756 - val_accuracy: 0.4059\n",
            "Epoch 23/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0763 - accuracy: 0.3786 - val_loss: 0.1113 - val_accuracy: 0.2152\n",
            "Epoch 24/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0777 - accuracy: 0.3569 - val_loss: 0.0785 - val_accuracy: 0.4059\n",
            "Epoch 25/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0727 - accuracy: 0.4284 - val_loss: 0.0880 - val_accuracy: 0.3667\n",
            "Epoch 26/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0740 - accuracy: 0.4361 - val_loss: 0.0681 - val_accuracy: 0.4890\n",
            "Epoch 27/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0718 - accuracy: 0.4453 - val_loss: 0.0723 - val_accuracy: 0.4743\n",
            "Epoch 28/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0658 - accuracy: 0.4970 - val_loss: 0.0659 - val_accuracy: 0.5134\n",
            "Epoch 29/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0651 - accuracy: 0.5080 - val_loss: 0.0612 - val_accuracy: 0.5648\n",
            "Epoch 30/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0673 - accuracy: 0.4866 - val_loss: 0.0893 - val_accuracy: 0.3814\n",
            "Epoch 31/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0621 - accuracy: 0.5327 - val_loss: 0.0537 - val_accuracy: 0.6015\n",
            "Epoch 32/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0655 - accuracy: 0.5021 - val_loss: 0.0558 - val_accuracy: 0.6015\n",
            "Epoch 33/300\n",
            "103/103 [==============================] - 7s 70ms/step - loss: 0.0581 - accuracy: 0.5629 - val_loss: 0.0904 - val_accuracy: 0.3472\n",
            "Epoch 34/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0594 - accuracy: 0.5764 - val_loss: 0.0565 - val_accuracy: 0.6064\n",
            "Epoch 35/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0544 - accuracy: 0.5989 - val_loss: 0.0597 - val_accuracy: 0.5599\n",
            "Epoch 36/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0548 - accuracy: 0.6003 - val_loss: 0.0832 - val_accuracy: 0.4205\n",
            "Epoch 37/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0548 - accuracy: 0.6073 - val_loss: 0.0541 - val_accuracy: 0.6015\n",
            "Epoch 38/300\n",
            "103/103 [==============================] - 8s 73ms/step - loss: 0.0499 - accuracy: 0.6358 - val_loss: 0.0609 - val_accuracy: 0.5550\n",
            "Epoch 39/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0491 - accuracy: 0.6631 - val_loss: 0.0578 - val_accuracy: 0.5721\n",
            "Epoch 40/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0486 - accuracy: 0.6583 - val_loss: 0.0547 - val_accuracy: 0.5917\n",
            "Epoch 41/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0503 - accuracy: 0.6291 - val_loss: 0.0347 - val_accuracy: 0.7604\n",
            "Epoch 42/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0454 - accuracy: 0.6917 - val_loss: 0.0324 - val_accuracy: 0.7775\n",
            "Epoch 43/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0451 - accuracy: 0.6847 - val_loss: 0.0481 - val_accuracy: 0.6381\n",
            "Epoch 44/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0413 - accuracy: 0.7126 - val_loss: 0.0616 - val_accuracy: 0.5379\n",
            "Epoch 45/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0423 - accuracy: 0.7104 - val_loss: 0.0337 - val_accuracy: 0.7677\n",
            "Epoch 46/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0422 - accuracy: 0.7066 - val_loss: 0.0275 - val_accuracy: 0.8142\n",
            "Epoch 47/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0414 - accuracy: 0.7041 - val_loss: 0.0398 - val_accuracy: 0.7066\n",
            "Epoch 48/300\n",
            "103/103 [==============================] - 7s 72ms/step - loss: 0.0382 - accuracy: 0.7369 - val_loss: 0.0379 - val_accuracy: 0.7555\n",
            "Epoch 49/300\n",
            "103/103 [==============================] - 8s 76ms/step - loss: 0.0393 - accuracy: 0.7341 - val_loss: 0.0325 - val_accuracy: 0.7628\n",
            "Epoch 50/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0367 - accuracy: 0.7440 - val_loss: 0.0392 - val_accuracy: 0.7237\n",
            "Epoch 51/300\n",
            "103/103 [==============================] - 8s 74ms/step - loss: 0.0408 - accuracy: 0.7143 - val_loss: 0.0406 - val_accuracy: 0.7164\n",
            "Epoch 52/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0371 - accuracy: 0.7416 - val_loss: 0.0349 - val_accuracy: 0.7531\n",
            "Epoch 53/300\n",
            "103/103 [==============================] - 7s 71ms/step - loss: 0.0348 - accuracy: 0.7554 - val_loss: 0.0381 - val_accuracy: 0.7384\n",
            "Epoch 54/300\n",
            "103/103 [==============================] - 8s 75ms/step - loss: 0.0396 - accuracy: 0.7271 - val_loss: 0.0369 - val_accuracy: 0.7482\n",
            "Epoch 55/300\n",
            "103/103 [==============================] - 8s 74ms/step - loss: 0.0365 - accuracy: 0.7567 - val_loss: 0.0336 - val_accuracy: 0.7726\n",
            "Epoch 56/300\n",
            "103/103 [==============================] - 7s 73ms/step - loss: 0.0367 - accuracy: 0.7542 - val_loss: 0.0351 - val_accuracy: 0.7482\n",
            "==================================================\n",
            "Result of inception_model, fold 5\n",
            "Epoch: 56\n",
            "Accuracy: 0.8141809105873108\n",
            "Time taken:  451.6344335079193\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpK3JvfC5mEP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20daab7b-d889-4f87-8b36-491921baead7"
      },
      "source": [
        "# 작성자 gpu에 맞춰져 있음\n",
        "def get_gpu_memory():\n",
        "    total = 6144\n",
        "    _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
        "\n",
        "    ACCEPTABLE_AVAILABLE_MEMORY = 1024\n",
        "    COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
        "    memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
        "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
        "    print(f\"Free: {memory_free_values[0]}MB / {total}MB    {(total - memory_free_values[0]) / total * 100:.2f}% used \")\n",
        "\n",
        "\n",
        "def reset_keras():\n",
        "    sess = tf.compat.v1.keras.backend.get_session()\n",
        "    tf.compat.v1.keras.backend.clear_session()\n",
        "    sess.close()\n",
        "    sess = tf.compat.v1.keras.backend.get_session()\n",
        "\n",
        "    # use the same config as you used to create the session\n",
        "    config = tf.compat.v1.ConfigProto()\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
        "    config.gpu_options.visible_device_list = \"0\"\n",
        "    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
        "\n",
        "get_gpu_memory()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Free: 6341MB / 6144MB    -3.21% used \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpoJS9xU5zdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae478b0-5958-40d3-e2f4-01169581cc94"
      },
      "source": [
        "# 예측결과들을 담는다\n",
        "model_files = glob.glob(f'{MODEL_PATH}/*.h5')\n",
        "pred_list = []\n",
        "for i, model_file in enumerate(model_files):\n",
        "    model = keras.models.load_model(model_file,\n",
        "                                    custom_objects={'RandomRollLayer':RandomRollLayer})\n",
        "\n",
        "    print(i+1, model.name)\n",
        "    y_pred = model.predict([test_letters, test_pixels])\n",
        "    pred_list.append(y_pred)\n",
        "    model = None\n",
        "    gc.collect()\n",
        "    reset_keras()\n",
        "    get_gpu_memory()\n",
        "    print('='*50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 inception_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "2 vggnet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "3 inception_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "4 xception_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "5 densenet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "6 thin_resnet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "7 xception_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "8 vggnet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "9 resnet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "10 thin_resnet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "11 inception_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "12 resnet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "13 thin_resnet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "14 densenet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "15 thin_resnet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "16 vggnet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "17 densenet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "18 xception_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "19 xception_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "20 vggnet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "21 resnet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "22 resnet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "23 densenet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "24 thin_resnet_model\n",
            "Free: 6341MB / 6144MB    -3.21% used \n",
            "==================================================\n",
            "25 resnet_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwrYLGFQ53LW"
      },
      "source": [
        "# ensemble 과정 -> 여러 모델의 각 fold로 예측한 값의 root sum 사용\n",
        "ensemble_data = np.zeros(pred_list[0].shape)\n",
        "\n",
        "for pred in pred_list:\n",
        "    ensemble_data += pred ** 0.5\n",
        "    \n",
        "y_pred = np.argmax(ensemble_data, axis=1)\n",
        "\n",
        "submission_csv['digit'] = y_pred\n",
        "submission_csv.to_csv('CNN.csv', index=False)\n",
        "submission_csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiucyC-3HCu1",
        "outputId": "43955fc3-696a-4d67-cf64-d5ca6297b5b2"
      },
      "source": [
        "# 6개의 모델이 총 5개의 KFold에서 진행했으므로 30개\n",
        "len(pred_list)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YohfI3WHNOy",
        "outputId": "9e6d88a1-af46-459a-963f-fcb2c6ed5f47"
      },
      "source": [
        "pred_list[0]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.8088363e-02, 2.4314924e-01, 6.8227634e-02, ..., 2.4163483e-01,\n",
              "        5.2773241e-02, 7.8090534e-02],\n",
              "       [3.4034818e-02, 9.0969443e-02, 4.3149073e-02, ..., 2.6356050e-01,\n",
              "        6.3408054e-02, 2.2304362e-01],\n",
              "       [8.0581054e-02, 7.4622482e-02, 1.6514961e-01, ..., 4.7883619e-02,\n",
              "        5.1407184e-02, 3.1675544e-02],\n",
              "       ...,\n",
              "       [1.4727840e-07, 5.5142520e-17, 5.8866099e-05, ..., 1.6033250e-10,\n",
              "        6.1500170e-03, 8.6383667e-13],\n",
              "       [2.4030760e-02, 3.9125761e-01, 3.7166055e-02, ..., 3.3838847e-01,\n",
              "        2.6859017e-02, 4.6448644e-02],\n",
              "       [9.7432338e-02, 5.1816806e-02, 1.7883073e-01, ..., 3.6571123e-02,\n",
              "        5.8977082e-02, 3.0695712e-02]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGwNktdJHycH",
        "outputId": "b58506a2-6dac-4e36-b4b7-11f9514ca8de"
      },
      "source": [
        "# test 데이터 20480개\n",
        "pred_list[0].shape"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20480, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoJ-OkEfKa0m",
        "outputId": "a5c58d0c-7d76-4e55-d0f1-807b11b15eee"
      },
      "source": [
        "np.argmax(ensemble_data, axis=1)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 9, 1, ..., 6, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7KOvJ5FK8PK",
        "outputId": "5866b343-d67a-4ba8-9b74-ab20a606d5a4"
      },
      "source": [
        "ensemble_data.shape"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20480, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL6koBgGKcnN",
        "outputId": "c3c67cf2-8f61-4a2f-d1b8-c0d48b47cda2"
      },
      "source": [
        "np.argmax(ensemble_data, axis=1).shape"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20480,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vXvwKgo55iK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a918a51d-cac7-44a3-d54a-463e6944cd68"
      },
      "source": [
        "df = pd.DataFrame(ensemble_data)\n",
        "df['max'] = df.iloc[:, 0:10].max(axis=1)\n",
        "df['pred'] = df.iloc[:, 0:10].idxmax(axis=1)\n",
        "\n",
        "good_df = df[df['max'] > 20]\n",
        "\n",
        "test_digits_int = good_df['pred'].to_numpy()\n",
        "test_digits = tf.keras.utils.to_categorical(test_digits_int)\n",
        "\n",
        "new_digits = np.concatenate((train_digits, test_digits))\n",
        "new_letters = np.concatenate((train_letters, test_letters[good_df.index]))\n",
        "new_pixels = np.concatenate((train_pixels, test_pixels[good_df.index]))\n",
        "new_digits.shape, new_letters.shape, new_pixels.shape"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9536, 10), (9536, 26), (9536, 28, 28, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "9LHrvTbfK0Wg",
        "outputId": "0ce6ace9-353e-44a6-dad3-a19dfe74d32e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>max</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.916596</td>\n",
              "      <td>6.771963</td>\n",
              "      <td>8.231836</td>\n",
              "      <td>5.722177</td>\n",
              "      <td>6.518749</td>\n",
              "      <td>7.717994</td>\n",
              "      <td>13.784646</td>\n",
              "      <td>4.890967</td>\n",
              "      <td>7.160683</td>\n",
              "      <td>4.478457</td>\n",
              "      <td>13.784646</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.740641</td>\n",
              "      <td>5.437949</td>\n",
              "      <td>6.054413</td>\n",
              "      <td>8.252919</td>\n",
              "      <td>6.977593</td>\n",
              "      <td>9.009335</td>\n",
              "      <td>3.653211</td>\n",
              "      <td>9.240905</td>\n",
              "      <td>7.492497</td>\n",
              "      <td>12.667240</td>\n",
              "      <td>12.667240</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.243333</td>\n",
              "      <td>9.573554</td>\n",
              "      <td>9.505839</td>\n",
              "      <td>8.979655</td>\n",
              "      <td>6.573221</td>\n",
              "      <td>8.260269</td>\n",
              "      <td>8.904134</td>\n",
              "      <td>5.853799</td>\n",
              "      <td>8.477946</td>\n",
              "      <td>4.576027</td>\n",
              "      <td>9.573554</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.396715</td>\n",
              "      <td>4.049004</td>\n",
              "      <td>6.306641</td>\n",
              "      <td>4.777739</td>\n",
              "      <td>5.620322</td>\n",
              "      <td>6.655015</td>\n",
              "      <td>7.135661</td>\n",
              "      <td>4.145907</td>\n",
              "      <td>6.238315</td>\n",
              "      <td>4.873711</td>\n",
              "      <td>16.396715</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.766671</td>\n",
              "      <td>4.924097</td>\n",
              "      <td>7.965075</td>\n",
              "      <td>16.065784</td>\n",
              "      <td>4.422710</td>\n",
              "      <td>7.119639</td>\n",
              "      <td>5.713257</td>\n",
              "      <td>4.454555</td>\n",
              "      <td>4.820877</td>\n",
              "      <td>4.041815</td>\n",
              "      <td>16.065784</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2  ...          9        max  pred\n",
              "0   6.916596  6.771963  8.231836  ...   4.478457  13.784646     6\n",
              "1   8.740641  5.437949  6.054413  ...  12.667240  12.667240     9\n",
              "2   8.243333  9.573554  9.505839  ...   4.576027   9.573554     1\n",
              "3  16.396715  4.049004  6.306641  ...   4.873711  16.396715     0\n",
              "4   6.766671  4.924097  7.965075  ...   4.041815  16.065784     3\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFW5RF-zLBk9",
        "outputId": "04de89d8-d688-4451-968a-aaca713f28e8"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20480, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsQ3vFc2LIlp",
        "outputId": "6c25c590-7b37-4b83-95e1-086005ae5e8e"
      },
      "source": [
        "df[df['max'] > 20].shape"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2496, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnvmr_bm58J6"
      },
      "source": [
        "train_digits = new_digits\n",
        "train_letters = new_letters\n",
        "train_pixels = new_pixels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEuUD89U590p"
      },
      "source": [
        "test_digits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaWtWLkJ_JU1",
        "outputId": "f8bcbf0e-e5b6-4dff-b214-5a48b0d7fca2"
      },
      "source": [
        "test_digits.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2496, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-ua7jcV5_bw"
      },
      "source": [
        "y_pred = np.argmax(model.predict([test_letters, test_pixels]), axis=1)\n",
        "y_pred.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNEZJSqM6Bfz"
      },
      "source": [
        "submission_csv['digit'] = y_pred\n",
        "submission_csv.to_csv('CNN.csv', index=False)\n",
        "submission_csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ri3mMZlFNU7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}